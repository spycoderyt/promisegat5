{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import timeit\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Hyperparameters ###\n",
    "\n",
    "radius         = 1\n",
    "dim        = 20\n",
    "layer_gnn      = 2\n",
    "lr             = 1e-3\n",
    "lr_decay       = 0.5\n",
    "decay_interval = 10\n",
    "iteration      = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Check if GPU is available ###\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "### Define k-folds ###\n",
    "num_kfolds = 5\n",
    "kfold      = KFold(n_splits=num_kfolds, shuffle=True, random_state=1)  # Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "\n",
    "def file_ind(index):\n",
    "    st_ind, in_ind = divmod(index,10)\n",
    "    return 10*st_ind, in_ind\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def calculate_performace(test_num, pred_y,  labels):\n",
    "    tp =0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1        \n",
    "                \n",
    "                \n",
    "    if (tp+fn) == 0:\n",
    "        q9 = float(tn-fp)/(tn+fp + 1e-06)\n",
    "    if (tn+fp) == 0:\n",
    "        q9 = float(tp-fn)/(tp+fn + 1e-06)\n",
    "    if  (tp+fn) != 0 and (tn+fp) !=0:\n",
    "        q9 = 1- float(np.sqrt(2))*np.sqrt(float(fn*fn)/((tp+fn)*(tp+fn))+float(fp*fp)/((tn+fp)*(tn+fp)))\n",
    "        \n",
    "    Q9 = (float)(1+q9)/2\n",
    "    accuracy = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp + 1e-06)\n",
    "    sensitivity = float(tp)/ (tp + fn + 1e-06)\n",
    "    recall = float(tp)/ (tp + fn + 1e-06)\n",
    "    specificity = float(tn)/(tn + fp + 1e-06)\n",
    "    ppv = float(tp)/(tp + fp + 1e-06)\n",
    "    npv = float(tn)/(tn + fn + 1e-06)\n",
    "    F1_score = float(2*tp)/(2*tp + fp + fn + 1e-06)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    \n",
    "    return tp,fp,tn,fn,accuracy, precision, sensitivity, recall, specificity, MCC, F1_score, Q9, ppv, npv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPI, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(n_fingerprint, dim)\n",
    "        self.W_gnn             = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W1_attention      = nn.Linear(dim, dim)\n",
    "        self.W2_attention      = nn.Linear(dim, dim)\n",
    "        self.w                 = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "        self.W_out             = nn.Linear(2*dim, 2)\n",
    "        \n",
    "    def gnn(self, xs1, A1, xs2, A2):\n",
    "        for i in range(layer_gnn):\n",
    "            hs1 = torch.relu(self.W_gnn[i](xs1))            \n",
    "            hs2 = torch.relu(self.W_gnn[i](xs2))\n",
    "            \n",
    "            xs1 = torch.matmul(A1, hs1)\n",
    "            xs2 = torch.matmul(A2, hs2)\n",
    "        \n",
    "        return xs1, xs2\n",
    "    \n",
    "    def mutual_attention(self, h1, h2):\n",
    "        x1 = self.W1_attention(h1)\n",
    "        x2 = self.W2_attention(h2)\n",
    "        \n",
    "        m1 = x1.size()[0]\n",
    "        m2 = x2.size()[0]\n",
    "        \n",
    "        c1 = x1.repeat(1,m2).view(m1, m2, dim)\n",
    "        c2 = x2.repeat(m1,1).view(m1, m2, dim)\n",
    "\n",
    "        d = torch.tanh(c1 + c2)\n",
    "        alpha = torch.matmul(d,self.w).view(m1,m2)\n",
    "        \n",
    "        b1 = torch.mean(alpha,1)\n",
    "        p1 = torch.softmax(b1,0)\n",
    "        s1 = torch.matmul(torch.t(x1),p1).view(-1,1)\n",
    "        \n",
    "        b2 = torch.mean(alpha,0)\n",
    "        p2 = torch.softmax(b2,0)\n",
    "        s2 = torch.matmul(torch.t(x2),p2).view(-1,1)\n",
    "        \n",
    "        return torch.cat((s1,s2),0).view(1,-1), p1, p2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fingerprints1, adjacency1, fingerprints2, adjacency2 = inputs\n",
    "        \n",
    "        \"\"\"Protein vector with GNN.\"\"\"\n",
    "        x_fingerprints1        = self.embed_fingerprint(fingerprints1)\n",
    "        x_fingerprints2        = self.embed_fingerprint(fingerprints2)\n",
    "        \n",
    "        x_protein1, x_protein2 = self.gnn(x_fingerprints1, adjacency1, x_fingerprints2, adjacency2)\n",
    "        \n",
    "        \"\"\"Protein vector with mutual-attention.\"\"\"\n",
    "        y, p1, p2     = self.mutual_attention(x_protein1, x_protein2)\n",
    "        z_interaction = self.W_out(y)\n",
    "\n",
    "        return z_interaction, p1, p2\n",
    "    \n",
    "    def __call__(self, data, train=True):\n",
    "        \n",
    "        inputs, t_interaction = data[:-1], data[-1]\n",
    "        z_interaction, p1, p2 = self.forward(inputs)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_interaction, t_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            z = F.softmax(z_interaction, 1).to('cpu').data[0].numpy()\n",
    "            t = int(t_interaction.to('cpu').data[0].numpy())\n",
    "            return z, t, p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPIMPredict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPIMPredict, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(nmod_fingerprint, dim)\n",
    "        self.W_gnn             = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W1_attention      = nn.Linear(dim, dim)\n",
    "        self.W2_attention      = nn.Linear(dim, dim)\n",
    "        self.w                 = nn.Parameter(torch.zeros(dim)) #attention between prots\n",
    "        self.w2                 = nn.Parameter(torch.zeros(dim)) #attention ppi + mod        \n",
    "        self.W_out             = nn.Linear(2*dim, 2)\n",
    "        \n",
    "    def gnn(self, xs1, A1, xs2, A2):\n",
    "        for i in range(layer_gnn):\n",
    "            hs1 = torch.relu(self.W_gnn[i](xs1))            \n",
    "            hs2 = torch.relu(self.W_gnn[i](xs2))\n",
    "            \n",
    "            xs1 = torch.matmul(A1, hs1)\n",
    "            xs2 = torch.matmul(A2, hs2)\n",
    "        \n",
    "        return xs1, xs2\n",
    "    \n",
    "    def mutual_attention(self, h1, h2):\n",
    "        x1 = self.W1_attention(h1)\n",
    "        x2 = self.W2_attention(h2)\n",
    "        \n",
    "        m1 = x1.size()[0]\n",
    "        m2 = x2.size()[0]\n",
    "        \n",
    "        c1 = x1.repeat(1,m2).view(m1, m2, dim)\n",
    "        c2 = x2.repeat(m1,1).view(m1, m2, dim)\n",
    "\n",
    "        d = torch.tanh(c1 + c2)\n",
    "        alpha = torch.matmul(d,self.w).view(m1,m2)\n",
    "        \n",
    "        b1 = torch.mean(alpha,1)\n",
    "        p1 = torch.softmax(b1,0)\n",
    "        s1 = torch.matmul(torch.t(x1),p1).view(-1,1)\n",
    "        \n",
    "        b2 = torch.mean(alpha,0)\n",
    "        p2 = torch.softmax(b2,0)\n",
    "        s2 = torch.matmul(torch.t(x2),p2).view(-1,1)\n",
    "        \n",
    "        return torch.cat((s1,s2),0).view(1,-1), p1, p2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fingerprints1, adjacency1, fingerprints2, adjacency2 = inputs\n",
    "        \n",
    "        \"\"\"Protein vector with GNN.\"\"\"\n",
    "        x_fingerprints1        = self.embed_fingerprint(fingerprints1)\n",
    "        x_fingerprints2        = self.embed_fingerprint(fingerprints2)\n",
    "        \n",
    "        x_protein1, x_protein2 = self.gnn(x_fingerprints1, adjacency1, x_fingerprints2, adjacency2)\n",
    "        \n",
    "        \"\"\"Protein vector with mutual-attention.\"\"\"\n",
    "        y, p1, p2     = self.mutual_attention(x_protein1, x_protein2)\n",
    "        z_interaction = self.W_out(y)\n",
    "\n",
    "        return z_interaction, p1, p2\n",
    "    \n",
    "    def __call__(self, data, train=True):\n",
    "        \n",
    "        inputs, t_interaction = data[:-1], data[-1]\n",
    "        z_interaction, p1, p2 = self.forward(inputs)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_interaction, t_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            z = F.softmax(z_interaction, 1).to('cpu').data[0].numpy()\n",
    "            t = int(t_interaction.to('cpu').data[0].numpy())\n",
    "            return z, t, p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        \n",
    "        # sampling  = random.choices(dataset, k=800)\n",
    "        \n",
    "        loss_total = 0\n",
    "        for data in dataset:\n",
    "            \n",
    "            interaction = torch.LongTensor([data[2]])\n",
    "            \n",
    "            comb = (protein1.to(device), adjacency1.to(device), protein2.to(device), adjacency2.to(device), interaction.to(device))\n",
    "            \n",
    "            loss = self.model(comb)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "prot_data = pd.read_csv(\"prot_data.csv\")\n",
    "mod_data = pd.read_csv(\"mod_data.csv\")\n",
    "train_data = pd.read_csv(\"interaction_data.csv\")\n",
    "examples = np.array(train_data.values.tolist())\n",
    "# setup folders\n",
    "prot_fp_folder = \"protein_fingerprints\"\n",
    "mod_fp_folder = \"mod_fingerprints\"\n",
    "prot_fp_dict = np.load(\"protein_fingerprints/prot_fingerprint_dict.pickle\",allow_pickle=True)\n",
    "mod_fp_dict = np.load(\"mod_fingerprints/mod_fingerprint_dict.pickle\",allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WAOUXZZNLNSGRH-UHFFFAOYSA-N', 'A8MRB1', 'H2EHT1', '0',\n",
       "       'S100B_p53'], dtype='<U113')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train,test in kfold.split(examples):\n",
    "    dataset_train = examples[train] # mod, prot1, prot2, int, int_family\n",
    "    dataset_test = examples[test]\n",
    "\n",
    "    prot_model = \n",
    "    model = PPIMPredict().to(device)\n",
    "    trainer = Trainer(model)\n",
    "    tester = Tester(model)\n",
    "\n",
    "    torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out struct2graph\n",
    "n_fingerprint = len(prot_fp_dict)\n",
    "nmod_fingerprint = len(mod_fp_dict)\n",
    "model = PPI().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promisegat4",
   "language": "python",
   "name": "promisegat4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
