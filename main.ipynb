{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import timeit\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Check if GPU is available ###\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "### Define k-folds ###\n",
    "num_kfolds = 5\n",
    "kfold      = KFold(n_splits=num_kfolds, shuffle=True, random_state=1)  # Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "\n",
    "def file_ind(index):\n",
    "    st_ind, in_ind = divmod(index,10)\n",
    "    return 10*st_ind, in_ind\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def calculate_performace(test_num, pred_y,  labels):\n",
    "    tp =0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1        \n",
    "                \n",
    "                \n",
    "    if (tp+fn) == 0:\n",
    "        q9 = float(tn-fp)/(tn+fp + 1e-06)\n",
    "    if (tn+fp) == 0:\n",
    "        q9 = float(tp-fn)/(tp+fn + 1e-06)\n",
    "    if  (tp+fn) != 0 and (tn+fp) !=0:\n",
    "        q9 = 1- float(np.sqrt(2))*np.sqrt(float(fn*fn)/((tp+fn)*(tp+fn))+float(fp*fp)/((tn+fp)*(tn+fp)))\n",
    "        \n",
    "    Q9 = (float)(1+q9)/2\n",
    "    accuracy = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp + 1e-06)\n",
    "    sensitivity = float(tp)/ (tp + fn + 1e-06)\n",
    "    recall = float(tp)/ (tp + fn + 1e-06)\n",
    "    specificity = float(tn)/(tn + fp + 1e-06)\n",
    "    ppv = float(tp)/(tp + fp + 1e-06)\n",
    "    npv = float(tn)/(tn + fn + 1e-06)\n",
    "    F1_score = float(2*tp)/(2*tp + fp + fn + 1e-06)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    \n",
    "    return tp,fp,tn,fn,accuracy, precision, sensitivity, recall, specificity, MCC, F1_score, Q9, ppv, npv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPI, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(n_fingerprint, dim)\n",
    "        self.W_gnn             = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W1_attention      = nn.Linear(dim, dim)\n",
    "        self.W2_attention      = nn.Linear(dim, dim)\n",
    "        self.w                 = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "        self.W_out             = nn.Linear(2*dim, 2)\n",
    "        \n",
    "    def gnn(self, xs1, A1, xs2, A2):\n",
    "        for i in range(layer_gnn):\n",
    "            hs1 = torch.relu(self.W_gnn[i](xs1))            \n",
    "            hs2 = torch.relu(self.W_gnn[i](xs2))\n",
    "            \n",
    "            xs1 = torch.matmul(A1, hs1)\n",
    "            xs2 = torch.matmul(A2, hs2)\n",
    "        \n",
    "        return xs1, xs2\n",
    "    \n",
    "    def mutual_attention(self, h1, h2):\n",
    "        x1 = self.W1_attention(h1)\n",
    "        x2 = self.W2_attention(h2)\n",
    "        \n",
    "        m1 = x1.size()[0]\n",
    "        m2 = x2.size()[0]\n",
    "        \n",
    "        c1 = x1.repeat(1,m2).view(m1, m2, dim)\n",
    "        c2 = x2.repeat(m1,1).view(m1, m2, dim)\n",
    "\n",
    "        d = torch.tanh(c1 + c2)\n",
    "        alpha = torch.matmul(d,self.w).view(m1,m2)\n",
    "        \n",
    "        b1 = torch.mean(alpha,1)\n",
    "        p1 = torch.softmax(b1,0)\n",
    "        s1 = torch.matmul(torch.t(x1),p1).view(-1,1)\n",
    "        \n",
    "        b2 = torch.mean(alpha,0)\n",
    "        p2 = torch.softmax(b2,0)\n",
    "        s2 = torch.matmul(torch.t(x2),p2).view(-1,1)\n",
    "        \n",
    "        return torch.cat((s1,s2),0).view(1,-1), p1, p2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fingerprints1, adjacency1, fingerprints2, adjacency2 = inputs\n",
    "        \n",
    "        \"\"\"Protein vector with GNN.\"\"\"\n",
    "        x_fingerprints1        = self.embed_fingerprint(fingerprints1)\n",
    "        x_fingerprints2        = self.embed_fingerprint(fingerprints2)\n",
    "        \n",
    "        x_protein1, x_protein2 = self.gnn(x_fingerprints1, adjacency1, x_fingerprints2, adjacency2)\n",
    "        \n",
    "        \"\"\"Protein vector with mutual-attention.\"\"\"\n",
    "        y, p1, p2     = self.mutual_attention(x_protein1, x_protein2)\n",
    "        z_interaction = self.W_out(y)\n",
    "\n",
    "        return z_interaction, p1, p2, y\n",
    "    \n",
    "    def __call__(self, data, train=True):\n",
    "        \n",
    "        inputs, t_interaction = data[:-1], data[-1]\n",
    "        z_interaction, p1, p2, y = self.forward(inputs)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_interaction, t_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            z = F.softmax(z_interaction, 1).to('cpu').data[0].numpy()\n",
    "            t = int(t_interaction.to('cpu').data[0].numpy())\n",
    "            return z, t, p1, p2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPIMPredict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPIMPredict, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(nmod_fingerprint, dim)\n",
    "        self.W_gnn             = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W1_attention      = nn.Linear(dim, dim)\n",
    "        self.W2_attention      = nn.Linear(2*dim, dim)\n",
    "        self.w                 = nn.Parameter(torch.zeros(dim)) #attention between prots\n",
    "        self.w2                 = nn.Parameter(torch.zeros(dim)) #attention ppi + mod        \n",
    "        self.W_out             = nn.Linear(2*dim, 2)\n",
    "        \n",
    "    def gnn(self, xs1, A1):\n",
    "        for i in range(layer_gnn):\n",
    "            hs1 = torch.relu(self.W_gnn[i](xs1))            \n",
    "            \n",
    "            xs1 = torch.matmul(A1, hs1)\n",
    "        \n",
    "        return xs1\n",
    "    \n",
    "    def mutual_attention(self, h1, h2):\n",
    "        x1 = self.W1_attention(h1)\n",
    "        x2 = self.W2_attention(h2)\n",
    "        \n",
    "        m1 = x1.size()[0]\n",
    "        m2 = x2.size()[0]\n",
    "        \n",
    "        c1 = x1.repeat(1,m2).view(m1, m2, dim)\n",
    "        c2 = x2.repeat(m1,1).view(m1, m2, dim)\n",
    "\n",
    "        d = torch.tanh(c1 + c2)\n",
    "        alpha = torch.matmul(d,self.w).view(m1,m2)\n",
    "        \n",
    "        b1 = torch.mean(alpha,1)\n",
    "        p1 = torch.softmax(b1,0)\n",
    "        s1 = torch.matmul(torch.t(x1),p1).view(-1,1)\n",
    "        \n",
    "        b2 = torch.mean(alpha,0)\n",
    "        p2 = torch.softmax(b2,0)\n",
    "        s2 = torch.matmul(torch.t(x2),p2).view(-1,1)\n",
    "        \n",
    "        return torch.cat((s1,s2),0).view(1,-1), p1, p2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fingerprints1, adjacency1, prot_embed = inputs\n",
    "        \n",
    "        \"\"\"Protein vector with GNN.\"\"\"\n",
    "        x_fingerprints1        = self.embed_fingerprint(fingerprints1)\n",
    "        # print(x_fingerprints1.shape)\n",
    "        # print(adjacency1.shape)\n",
    "        x_protein1 = self.gnn(x_fingerprints1, adjacency1)\n",
    "        \n",
    "        \"\"\"Protein vector with mutual-attention.\"\"\"\n",
    "        y, p1, p2     = self.mutual_attention(x_protein1, prot_embed)\n",
    "        z_interaction = self.W_out(y)\n",
    "\n",
    "        return z_interaction, p1, p2\n",
    "    \n",
    "    def __call__(self, data, train=True):\n",
    "        \n",
    "        inputs, t_interaction = data[:-1], data[-1]\n",
    "        z_interaction, p1, p2 = self.forward(inputs)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_interaction, t_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            z = F.softmax(z_interaction, 1).to('cpu').data[0].numpy()\n",
    "            t = int(t_interaction.to('cpu').data[0].numpy())\n",
    "            return z, t, p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class Trainer(object):\n",
    "    def __init__(self, model,prot_model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.prot_model = prot_model\n",
    "\n",
    "    def train(self, dataset):\n",
    "        \n",
    "        loss_total = 0\n",
    "        for data in tqdm(dataset):\n",
    "            try:\n",
    "                mod,p1,p2,interaction,family = data\n",
    "                A1 = np.load(\n",
    "                        prot_data[prot_data['protein'] == p1].iloc[0]['adj_path'],\n",
    "                        allow_pickle=True\n",
    "                    )\n",
    "                A2 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p2].iloc[0]['adj_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "\n",
    "                P1 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p1].iloc[0]['fp_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                P2 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p2].iloc[0]['fp_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                mod_fp = np.load(\n",
    "                    mod_data[mod_data['inchikey'] == mod].iloc[0]['fp_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                mod_adj = np.load(\n",
    "                    mod_data[mod_data['inchikey'] == mod].iloc[0]['adj_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            protein1 = torch.LongTensor(P1.astype(np.float16))\n",
    "            protein2 = torch.LongTensor(P2.astype(np.float16))\n",
    "            adjacency1 = torch.FloatTensor(A1.astype(np.float16))\n",
    "            adjacency2 = torch.FloatTensor(A2.astype(np.float16))\n",
    "            mod_fp = torch.LongTensor(mod_fp.astype(np.float16))\n",
    "            mod_adj = torch.FloatTensor(mod_adj.astype(np.float16))\n",
    "            interaction = torch.LongTensor([interaction.astype(int)])\n",
    "            # print(\"hi\")\n",
    "\n",
    "            comb = (protein1.to(device), adjacency1.to(device), protein2.to(device), adjacency2.to(device), interaction.to(device))\n",
    "            _,_,_,_,prot_embed = self.prot_model(comb,train=False)\n",
    "\n",
    "            # print(mod_fp.shape)\n",
    "            # print(mod_adj.shape)\n",
    "            \n",
    "            comb = (mod_fp.to(device),mod_adj.to(device),prot_embed.to(device),interaction.to(device))\n",
    "            loss = self.model(comb)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import wandb\n",
    "\n",
    "class Tester(object):\n",
    "    def __init__(self, model, prot_model, wandb_project_name=None):\n",
    "        self.model = model\n",
    "        self.prot_model = prot_model\n",
    "\n",
    "        # Initialize wandb if a project name is provided\n",
    "        if wandb_project_name:\n",
    "            wandb.init(project=wandb_project_name)\n",
    "            self.use_wandb = True\n",
    "        else:\n",
    "            self.use_wandb = False\n",
    "\n",
    "    def test(self, dataset, epoch=None):\n",
    "        sampling = dataset\n",
    "\n",
    "        z_list, t_list = [], []\n",
    "        for data in sampling:\n",
    "            try:\n",
    "                mod, p1, p2, interaction, _ = data\n",
    "                A1 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p1].iloc[0]['adj_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                A2 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p2].iloc[0]['adj_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                P1 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p1].iloc[0]['fp_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                P2 = np.load(\n",
    "                    prot_data[prot_data['protein'] == p2].iloc[0]['fp_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                mod_fp = np.load(\n",
    "                    mod_data[mod_data['inchikey'] == mod].iloc[0]['fp_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "                mod_adj = np.load(\n",
    "                    mod_data[mod_data['inchikey'] == mod].iloc[0]['adj_path'],\n",
    "                    allow_pickle=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"failed for {mod}, {p1}, and {p2}: {e}\")\n",
    "                continue\n",
    "\n",
    "            protein1 = torch.LongTensor(P1.astype(np.float16))\n",
    "            protein2 = torch.LongTensor(P2.astype(np.float16))\n",
    "            adjacency1 = torch.FloatTensor(A1.astype(np.float16))\n",
    "            adjacency2 = torch.FloatTensor(A2.astype(np.float16))\n",
    "            mod_fp = torch.LongTensor(mod_fp.astype(np.float16))\n",
    "            mod_adj = torch.FloatTensor(mod_adj.astype(np.float16))\n",
    "            interaction = torch.LongTensor([interaction.astype(int)])\n",
    "\n",
    "            comb = (protein1.to(device), adjacency1.to(device), protein2.to(device), adjacency2.to(device), interaction.to(device))\n",
    "            _, _, _, _, prot_embed = self.prot_model(comb, train=False)\n",
    "\n",
    "            comb = (mod_fp.to(device), mod_adj.to(device), prot_embed.to(device), interaction.to(device))\n",
    "            z, _, _, _ = self.model(comb, train=False)\n",
    "            z_list.append(z)\n",
    "            t_list.append(interaction)\n",
    "\n",
    "        score_list, label_list = [], []\n",
    "        for z in z_list:\n",
    "            score_list.append(z[1].item())\n",
    "            label_list.append(torch.argmax(z).item())\n",
    "\n",
    "        labels = np.array(label_list)\n",
    "        y_true = np.array([t.item() for t in t_list])\n",
    "        y_pred = np.array(score_list)\n",
    "\n",
    "        (\n",
    "            tp,\n",
    "            fp,\n",
    "            tn,\n",
    "            fn,\n",
    "            accuracy,\n",
    "            precision,\n",
    "            sensitivity,\n",
    "            recall,\n",
    "            specificity,\n",
    "            MCC,\n",
    "            F1_score,\n",
    "            Q9,\n",
    "            ppv,\n",
    "            npv,\n",
    "        ) = calculate_performance(len(sampling), labels, y_true)\n",
    "        roc_auc_val = roc_auc_score(y_true, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "\n",
    "        # Log results to wandb\n",
    "        if self.use_wandb:\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"Sensitivity\": sensitivity,\n",
    "                \"Specificity\": specificity,\n",
    "                \"MCC\": MCC,\n",
    "                \"F1 Score\": F1_score,\n",
    "                \"ROC AUC\": roc_auc_val,\n",
    "                \"AUC\": auc_val,\n",
    "                \"TP\": tp,\n",
    "                \"FP\": fp,\n",
    "                \"TN\": tn,\n",
    "                \"FN\": fn,\n",
    "                \"PPV\": ppv,\n",
    "                \"NPV\": npv,\n",
    "            })\n",
    "\n",
    "        return (\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            sensitivity,\n",
    "            specificity,\n",
    "            MCC,\n",
    "            F1_score,\n",
    "            roc_auc_val,\n",
    "            auc_val,\n",
    "            Q9,\n",
    "            ppv,\n",
    "            npv,\n",
    "            tp,\n",
    "            fp,\n",
    "            tn,\n",
    "            fn,\n",
    "        )\n",
    "\n",
    "    def result(\n",
    "        self,\n",
    "        epoch,\n",
    "        time,\n",
    "        loss,\n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        sensitivity,\n",
    "        specificity,\n",
    "        MCC,\n",
    "        F1_score,\n",
    "        roc_auc_val,\n",
    "        auc_val,\n",
    "        Q9,\n",
    "        ppv,\n",
    "        npv,\n",
    "        tp,\n",
    "        fp,\n",
    "        tn,\n",
    "        fn,\n",
    "        file_name,\n",
    "    ):\n",
    "        with open(file_name, \"a\") as f:\n",
    "            result = map(\n",
    "                str,\n",
    "                [\n",
    "                    epoch,\n",
    "                    time,\n",
    "                    loss,\n",
    "                    accuracy,\n",
    "                    precision,\n",
    "                    recall,\n",
    "                    sensitivity,\n",
    "                    specificity,\n",
    "                    MCC,\n",
    "                    F1_score,\n",
    "                    roc_auc_val,\n",
    "                    auc_val,\n",
    "                    Q9,\n",
    "                    ppv,\n",
    "                    npv,\n",
    "                    tp,\n",
    "                    fp,\n",
    "                    tn,\n",
    "                    fn,\n",
    "                ],\n",
    "            )\n",
    "            f.write(\"\\t\".join(result) + \"\\n\")\n",
    "\n",
    "    def save_model(self, model, file_name):\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "\n",
    "# Note: Make sure to define calculate_performance() and other missing components if not already done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "prot_data = pd.read_csv(\"prot_data.csv\")\n",
    "mod_data = pd.read_csv(\"mod_data.csv\")\n",
    "train_data = pd.read_csv(\"interaction_data.csv\")\n",
    "examples = np.array(train_data.values.tolist())\n",
    "# setup folders\n",
    "prot_fp_folder = \"protein_fingerprints\"\n",
    "mod_fp_folder = \"mod_fingerprints\"\n",
    "prot_fp_dict = np.load(\"protein_fingerprints/prot_fingerprint_dict.pickle\",allow_pickle=True)\n",
    "mod_fp_dict = np.load(\"mod_fingerprints/mod_fingerprint_dict.pickle\",allow_pickle=True)\n",
    "\n",
    "\n",
    "n_fingerprint = len(prot_fp_dict) + 100\n",
    "nmod_fingerprint = len(mod_fp_dict) + 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Hyperparameters ###\n",
    "\n",
    "radius         = 1\n",
    "dim        = 20\n",
    "layer_gnn      = 2\n",
    "lr             = 1e-3\n",
    "lr_decay       = 0.5\n",
    "decay_interval = 1\n",
    "iteration      = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f729244f95ee4de797553c33c8fba926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011161762966852014, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize wandb run\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpromisegat4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with your actual entity name\u001b[39;00m\n\u001b[1;32m      8\u001b[0m fold_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m kfold\u001b[38;5;241m.\u001b[39msplit(examples):\n",
      "File \u001b[0;32m~/promisegat4/venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:1191\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1191\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m~/promisegat4/venv/lib/python3.12/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/promisegat4/venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:1177\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m   1176\u001b[0m     wi\u001b[38;5;241m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/promisegat4/venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:781\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    779\u001b[0m         backend\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m--> 781\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_result\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import timeit\n",
    "import torch\n",
    "\n",
    "# Initialize wandb run\n",
    "wandb.init(project=\"promisegat4\")  # Replace with your actual entity name\n",
    "\n",
    "fold_count = 1\n",
    "\n",
    "for train, test in kfold.split(examples):\n",
    "    dataset_train = examples[train]  # mod, prot1, prot2, int, int_family\n",
    "    dataset_test = examples[test]\n",
    "\n",
    "    prot_model = PPI().to(device)\n",
    "    prot_model.load_state_dict(torch.load(\"output/model/one/model_fold_1\"))\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    model = PPIMPredict().to(device)\n",
    "    trainer = Trainer(model, prot_model)\n",
    "    file_model = \"ppim/model/\" + \"model_fold_\" + str(fold_count)\n",
    "    file_result = \"ppim/result/\" + \"results_fold_\" + str(fold_count) + \".txt\"\n",
    "\n",
    "    for epoch in range(iteration):\n",
    "        loss = trainer.train(dataset_train)\n",
    "        print(f\"finished with loss {loss}\")\n",
    "\n",
    "        # Log training loss and GPU usage to wandb\n",
    "        gpu_usage = torch.cuda.memory_allocated(device=device) / 1024 ** 3  # in GB\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": loss, \"gpu_usage_gb\": gpu_usage, \"fold\": fold_count})\n",
    "\n",
    "        tester = Tester(model, prot_model)\n",
    "        (\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            sensitivity,\n",
    "            specificity,\n",
    "            MCC,\n",
    "            F1_score,\n",
    "            roc_auc_val,\n",
    "            auc_val,\n",
    "            Q9,\n",
    "            ppv,\n",
    "            npv,\n",
    "            tp,\n",
    "            fp,\n",
    "            tn,\n",
    "            fn,\n",
    "        ) = tester.test(dataset_test, epoch=epoch)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        time = end - start\n",
    "\n",
    "        # Log results to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"time\": time,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"sensitivity\": sensitivity,\n",
    "            \"specificity\": specificity,\n",
    "            \"MCC\": MCC,\n",
    "            \"F1_score\": F1_score,\n",
    "            \"ROC_AUC\": roc_auc_val,\n",
    "            \"AUC\": auc_val,\n",
    "            \"Q9\": Q9,\n",
    "            \"PPV\": ppv,\n",
    "            \"NPV\": npv,\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"TN\": tn,\n",
    "            \"FN\": fn,\n",
    "            \"fold\": fold_count\n",
    "        })\n",
    "\n",
    "        tester.result(\n",
    "            epoch,\n",
    "            time,\n",
    "            loss,\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            sensitivity,\n",
    "            specificity,\n",
    "            MCC,\n",
    "            F1_score,\n",
    "            roc_auc_val,\n",
    "            auc_val,\n",
    "            Q9,\n",
    "            ppv,\n",
    "            npv,\n",
    "            tp,\n",
    "            fp,\n",
    "            tn,\n",
    "            fn,\n",
    "            file_result,\n",
    "        )\n",
    "        tester.save_model(model, file_model)\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        print(\"Precision: \" + str(precision))\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        print(\"Sensitivity: \" + str(sensitivity))\n",
    "        print(\"Specificity: \" + str(specificity))\n",
    "        print(\"MCC: \" + str(MCC))\n",
    "        print(\"F1-score: \" + str(F1_score))\n",
    "        print(\"ROC-AUC: \" + str(roc_auc_val))\n",
    "        print(\"AUC: \" + str(auc_val))\n",
    "        print(\"Q9: \" + str(Q9))\n",
    "        print(\"PPV: \" + str(ppv))\n",
    "        print(\"NPV: \" + str(npv))\n",
    "        print(\"TP: \" + str(tp))\n",
    "        print(\"FP: \" + str(fp))\n",
    "        print(\"TN: \" + str(tn))\n",
    "        print(\"FN: \" + str(fn))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        torch.manual_seed(1234)\n",
    "    \n",
    "    fold_count += 1\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmod_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d3/q38hjcyx7tj50fgmqb4mcm8r0000gn/T/ipykernel_79017/140094238.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prot_model.load_state_dict(torch.load(\"output/model/one/model_fold_1\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out struct2grap\n",
    "prot_model = PPI().to(device)\n",
    "prot_model.load_state_dict(torch.load(\"output/model/one/model_fold_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp1 = torch.LongTensor(np.load(\"protein_fingerprints/fingerprint/E9PNT5.npy\",allow_pickle=True).astype(np.float16))\n",
    "fp2 = torch.LongTensor(np.load(\"protein_fingerprints/fingerprint/P33681.npy\",allow_pickle=True).astype(np.float16))\n",
    "a1 = torch.FloatTensor(np.load(\"protein_fingerprints/adj/E9PNT5.npy\",allow_pickle=True).astype(np.float16))\n",
    "a2 = torch.FloatTensor(np.load(\"protein_fingerprints/adj/P33681.npy\",allow_pickle=True).astype(np.float16))\n",
    "val = torch.LongTensor(1)\n",
    "comb = (fp1,a1,fp2,a2,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, t, p1, p2, y= prot_model(comb,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promisegat4",
   "language": "python",
   "name": "promisegat4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
