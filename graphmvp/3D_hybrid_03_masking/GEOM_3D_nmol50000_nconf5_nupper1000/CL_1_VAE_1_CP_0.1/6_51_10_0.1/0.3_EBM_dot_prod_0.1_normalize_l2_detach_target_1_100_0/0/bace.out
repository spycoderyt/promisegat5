13698849_0
--dataset=bace --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6681099086519006
train: 0.724475	val: 0.590842	test: 0.680751

Epoch: 2
Loss: 0.6172106117988514
train: 0.768476	val: 0.613919	test: 0.699357

Epoch: 3
Loss: 0.5705283801649429
train: 0.792412	val: 0.687179	test: 0.701269

Epoch: 4
Loss: 0.5221957504886616
train: 0.816387	val: 0.684249	test: 0.753782

Epoch: 5
Loss: 0.5254444550021429
train: 0.848607	val: 0.634432	test: 0.776908

Epoch: 6
Loss: 0.4929740969433305
train: 0.853088	val: 0.604396	test: 0.789776

Epoch: 7
Loss: 0.4853481171755787
train: 0.866638	val: 0.634432	test: 0.804382

Epoch: 8
Loss: 0.4879872563365614
train: 0.880080	val: 0.653480	test: 0.830117

Epoch: 9
Loss: 0.44871680163070726
train: 0.882586	val: 0.606227	test: 0.809424

Epoch: 10
Loss: 0.4609586117466275
train: 0.883907	val: 0.570696	test: 0.792906

Epoch: 11
Loss: 0.4444712366090718
train: 0.894244	val: 0.615385	test: 0.810120

Epoch: 12
Loss: 0.44216708067153476
train: 0.895442	val: 0.621978	test: 0.817249

Epoch: 13
Loss: 0.4383993347587814
train: 0.901358	val: 0.631502	test: 0.815163

Epoch: 14
Loss: 0.4434111021832631
train: 0.902440	val: 0.672161	test: 0.829595

Epoch: 15
Loss: 0.4238671526713317
train: 0.901681	val: 0.660806	test: 0.837072

Epoch: 16
Loss: 0.4339823892625308
train: 0.904737	val: 0.672527	test: 0.842288

Epoch: 17
Loss: 0.42911561943355964
train: 0.907143	val: 0.706227	test: 0.833073

Epoch: 18
Loss: 0.41656157147509887
train: 0.909141	val: 0.693040	test: 0.818988

Epoch: 19
Loss: 0.39973299294829767
train: 0.911053	val: 0.700000	test: 0.826987

Epoch: 20
Loss: 0.39730231596114624
train: 0.911941	val: 0.665201	test: 0.840028

Epoch: 21
Loss: 0.40794417478412814
train: 0.914446	val: 0.682418	test: 0.836029

Epoch: 22
Loss: 0.42145827083417037
train: 0.917129	val: 0.695604	test: 0.829247

Epoch: 23
Loss: 0.3961414985072663
train: 0.916941	val: 0.694872	test: 0.829421

Epoch: 24
Loss: 0.39528590175925066
train: 0.916267	val: 0.695604	test: 0.840376

Epoch: 25
Loss: 0.39358229315221854
train: 0.919150	val: 0.717949	test: 0.846635

Epoch: 26
Loss: 0.4072250160378224
train: 0.920345	val: 0.702198	test: 0.828378

Epoch: 27
Loss: 0.406122992351881
train: 0.922754	val: 0.700000	test: 0.827856

Epoch: 28
Loss: 0.3883185021377774
train: 0.924598	val: 0.690110	test: 0.838289

Epoch: 29
Loss: 0.3926404233536234
train: 0.926173	val: 0.678022	test: 0.826639

Epoch: 30
Loss: 0.3824585122991464
train: 0.926156	val: 0.696337	test: 0.819857

Epoch: 31
Loss: 0.3818623833534792
train: 0.927115	val: 0.680952	test: 0.816380

Epoch: 32
Loss: 0.37958306202622355
train: 0.928222	val: 0.660440	test: 0.826987

Epoch: 33
Loss: 0.37134606216630955
train: 0.931256	val: 0.680220	test: 0.843679

Epoch: 34
Loss: 0.3721343368237428
train: 0.933599	val: 0.673260	test: 0.841071

Epoch: 35
Loss: 0.36792022232290245
train: 0.933225	val: 0.669597	test: 0.828899

Epoch: 36
Loss: 0.36608135802739816
train: 0.933730	val: 0.683150	test: 0.818119

Epoch: 37
Loss: 0.3716093690933882
train: 0.924098	val: 0.685348	test: 0.783342

Epoch: 38
Loss: 0.34713831871153
train: 0.931005	val: 0.698168	test: 0.804208

Epoch: 39
Loss: 0.3487153202298871
train: 0.938014	val: 0.694872	test: 0.827682

Epoch: 40
Loss: 0.358693145597839
train: 0.939247	val: 0.680586	test: 0.823161

Epoch: 41
Loss: 0.36166615318146855
train: 0.941159	val: 0.681685	test: 0.822813

Epoch: 42
Loss: 0.34030966812369057
train: 0.941732	val: 0.682418	test: 0.824552

Epoch: 43
Loss: 0.3507206385148338
train: 0.941915	val: 0.685714	test: 0.819162

Epoch: 44
Loss: 0.35147732071701066
train: 0.942643	val: 0.703663	test: 0.833073

Epoch: 45
Loss: 0.34659176717801554
train: 0.941261	val: 0.675092	test: 0.801252

Epoch: 46
Loss: 0.3459161287156187
train: 0.939686	val: 0.669231	test: 0.789428

Epoch: 47
Loss: 0.3446991481627185
train: 0.945665	val: 0.672527	test: 0.836550

Epoch: 48
Loss: 0.3459597872927572
train: 0.945522	val: 0.639194	test: 0.838811

Epoch: 49
Loss: 0.325081447765527
train: 0.943756	val: 0.645055	test: 0.838289

Epoch: 50
Loss: 0.34512729861186764
train: 0.942477	val: 0.683883	test: 0.816901

Epoch: 51
Loss: 0.32531228689661773
train: 0.943499	val: 0.701465	test: 0.804556

Epoch: 52
Loss: 0.32614920840462663
train: 0.948262	val: 0.674725	test: 0.814467

Epoch: 53
Loss: 0.331022873322747
train: 0.953761	val: 0.700000	test: 0.861242

Epoch: 54
Loss: 0.3225470146899993
train: 0.950868	val: 0.682051	test: 0.827682

Epoch: 55
Loss: 0.33894646157787744
train: 0.951835	val: 0.701465	test: 0.820379

Epoch: 56
Loss: 0.333417023966947
train: 0.950631	val: 0.697802	test: 0.798122

Epoch: 57
Loss: 0.328741741937965
train: 0.951176	val: 0.678022	test: 0.805425

Epoch: 58
Loss: 0.352742631581031
train: 0.953753	val: 0.679487	test: 0.809598

Epoch: 59
Loss: 0.297449976668842
train: 0.953057	val: 0.674359	test: 0.803512

Epoch: 60
Loss: 0.32890172304204573
train: 0.955428	val: 0.671795	test: 0.823509

Epoch: 61
Loss: 0.3197949194636437
train: 0.955225	val: 0.692308	test: 0.849244

Epoch: 62
Loss: 0.31577268562719585
train: 0.954321	val: 0.671429	test: 0.828378

Epoch: 63
Loss: 0.3085386786951104
train: 0.957146	val: 0.683883	test: 0.797253

Epoch: 64
Loss: 0.3195263853631067
train: 0.959007	val: 0.700000	test: 0.813250

Epoch: 65
Loss: 0.3025594218397132
train: 0.958593	val: 0.707326	test: 0.809772

Epoch: 66
Loss: 0.30626328364731564
train: 0.959001	val: 0.700366	test: 0.812728

Epoch: 67
Loss: 0.3140832402691725
train: 0.958459	val: 0.695971	test: 0.818814

Epoch: 68
Loss: 0.3124783262673687
train: 0.961450	val: 0.677289	test: 0.825769

Epoch: 69
Loss: 0.32258941570439315
train: 0.963662	val: 0.693773	test: 0.845244

Epoch: 70
Loss: 0.29724118489051926
train: 0.962377	val: 0.687912	test: 0.823857

Epoch: 71
Loss: 0.3112344316498861
train: 0.961270	val: 0.691575	test: 0.821944

Epoch: 72
Loss: 0.307556352911897
train: 0.957072	val: 0.697802	test: 0.820205

Epoch: 73
Loss: 0.3133095256725157
train: 0.960111	val: 0.681685	test: 0.818640

Epoch: 74
Loss: 0.30204161401729196
train: 0.961718	val: 0.671429	test: 0.805425

Epoch: 75
Loss: 0.3130187600551597
train: 0.963428	val: 0.676557	test: 0.838811

Epoch: 76
Loss: 0.2973716133093417
train: 0.963408	val: 0.701465	test: 0.830464

Epoch: 77
Loss: 0.28838852922149816
train: 0.959849	val: 0.691209	test: 0.791688

Epoch: 78
Loss: 0.28324207584453553
train: 0.963131	val: 0.678755	test: 0.791341

Epoch: 79
Loss: 0.3040506062695232
train: 0.967086	val: 0.680586	test: 0.830986

Epoch: 80
Loss: 0.2756313598629586
train: 0.962800	val: 0.639927	test: 0.825769

Epoch: 81
Loss: 0.29197258070184295
train: 0.964364	val: 0.665568	test: 0.824726

Epoch: 82
Loss: 0.29214436364683916
train: 0.965554	val: 0.705128	test: 0.838289

Epoch: 83
Loss: 0.30284300750199394
train: 0.967212	val: 0.678022	test: 0.828725

Epoch: 84
Loss: 0.28719083628731934
train: 0.964381	val: 0.697070	test: 0.814467

Epoch: 85
Loss: 0.2891953080895458
train: 0.958134	val: 0.679487	test: 0.811859

Epoch: 86
Loss: 0.3038950592884541
train: 0.968265	val: 0.692674	test: 0.821075

Epoch: 87
Loss: 0.27302307632056333
train: 0.966453	val: 0.693773	test: 0.797600

Epoch: 88
Loss: 0.28941956553303466
train: 0.965925	val: 0.690476	test: 0.788906

Epoch: 89
Loss: 0.2911014485645466
train: 0.969817	val: 0.679853	test: 0.814119

Epoch: 90
Loss: 0.26899759104180804
train: 0.972229	val: 0.680220	test: 0.822466

Epoch: 91
Loss: 0.26686680437760224
train: 0.969615	val: 0.683516	test: 0.822466

Epoch: 92
Loss: 0.2868221921154422
train: 0.968442	val: 0.684982	test: 0.833942

Epoch: 93
Loss: 0.2819609213874116
train: 0.969150	val: 0.672894	test: 0.827334

Epoch: 94
Loss: 0.2661355385956124
train: 0.966724	val: 0.699267	test: 0.798991

Epoch: 95
Loss: 0.25592617490389
train: 0.958984	val: 0.671429	test: 0.759172

Epoch: 96
Loss: 0.2670731640731938
train: 0.959575	val: 0.627106	test: 0.760737

Epoch: 97
Loss: 0.2691417551309417
train: 0.974461	val: 0.678388	test: 0.790645

Epoch: 98
Loss: 0.2561173958111763
train: 0.962805	val: 0.680220	test: 0.812380

Epoch: 99
Loss: 0.2578460226977056
train: 0.967314	val: 0.662637	test: 0.816728

Epoch: 100
Loss: 0.2629500980727619
train: 0.977152	val: 0.676923	test: 0.824900

best train: 0.919150	val: 0.717949	test: 0.846635
end
