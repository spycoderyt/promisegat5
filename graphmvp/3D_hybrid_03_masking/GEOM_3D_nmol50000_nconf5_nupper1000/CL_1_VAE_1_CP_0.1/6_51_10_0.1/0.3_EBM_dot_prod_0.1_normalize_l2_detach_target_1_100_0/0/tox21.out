13698849_0
--dataset=tox21 --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.48153109199386507
train: 0.722119	val: 0.662193	test: 0.636252

Epoch: 2
Loss: 0.28355518894988996
train: 0.764154	val: 0.703272	test: 0.658281

Epoch: 3
Loss: 0.2211153408803038
train: 0.794655	val: 0.740494	test: 0.689931

Epoch: 4
Loss: 0.2039456043040288
train: 0.807553	val: 0.754342	test: 0.705782

Epoch: 5
Loss: 0.1968897359794568
train: 0.821769	val: 0.757369	test: 0.711933

Epoch: 6
Loss: 0.19436615586079214
train: 0.828980	val: 0.751290	test: 0.706120

Epoch: 7
Loss: 0.19157132738540786
train: 0.835561	val: 0.754085	test: 0.716111

Epoch: 8
Loss: 0.18909873040580966
train: 0.849555	val: 0.767381	test: 0.723272

Epoch: 9
Loss: 0.18514368586232507
train: 0.850976	val: 0.765752	test: 0.719412

Epoch: 10
Loss: 0.18366084617722428
train: 0.854899	val: 0.778116	test: 0.733481

Epoch: 11
Loss: 0.18340954399336
train: 0.862235	val: 0.777605	test: 0.728542

Epoch: 12
Loss: 0.1810862690081194
train: 0.862279	val: 0.770749	test: 0.735511

Epoch: 13
Loss: 0.1776690031026258
train: 0.857416	val: 0.779887	test: 0.727083

Epoch: 14
Loss: 0.17770912942134312
train: 0.875109	val: 0.779728	test: 0.729535

Epoch: 15
Loss: 0.17521329524018064
train: 0.875160	val: 0.780809	test: 0.738908

Epoch: 16
Loss: 0.17562472807395527
train: 0.877260	val: 0.782284	test: 0.733138

Epoch: 17
Loss: 0.17243095254764346
train: 0.873156	val: 0.763539	test: 0.726207

Epoch: 18
Loss: 0.17029648773048897
train: 0.883015	val: 0.786386	test: 0.742349

Epoch: 19
Loss: 0.16936984992335108
train: 0.883797	val: 0.783635	test: 0.741723

Epoch: 20
Loss: 0.1697226303293614
train: 0.887181	val: 0.781562	test: 0.734289

Epoch: 21
Loss: 0.16825291044204158
train: 0.889980	val: 0.786457	test: 0.748083

Epoch: 22
Loss: 0.1671760651053832
train: 0.883958	val: 0.786355	test: 0.740711

Epoch: 23
Loss: 0.16668922448075146
train: 0.892122	val: 0.779405	test: 0.733280

Epoch: 24
Loss: 0.16554774234160646
train: 0.891838	val: 0.783125	test: 0.743031

Epoch: 25
Loss: 0.16308419261389354
train: 0.896283	val: 0.785722	test: 0.740505

Epoch: 26
Loss: 0.16290982456158043
train: 0.899783	val: 0.787293	test: 0.746267

Epoch: 27
Loss: 0.16240028815772195
train: 0.897123	val: 0.777200	test: 0.739941

Epoch: 28
Loss: 0.16096516799329488
train: 0.903668	val: 0.788791	test: 0.741332

Epoch: 29
Loss: 0.16002859223219632
train: 0.906005	val: 0.787083	test: 0.751501

Epoch: 30
Loss: 0.15967315354243858
train: 0.907027	val: 0.777871	test: 0.743511

Epoch: 31
Loss: 0.15858386985964187
train: 0.909627	val: 0.778487	test: 0.743814

Epoch: 32
Loss: 0.15835935760248881
train: 0.911045	val: 0.780177	test: 0.735167

Epoch: 33
Loss: 0.15684323649464063
train: 0.910205	val: 0.786679	test: 0.734387

Epoch: 34
Loss: 0.1550070448733151
train: 0.911769	val: 0.773876	test: 0.735138

Epoch: 35
Loss: 0.15539853888661523
train: 0.915544	val: 0.777787	test: 0.737560

Epoch: 36
Loss: 0.15521718571259518
train: 0.917138	val: 0.784349	test: 0.743842

Epoch: 37
Loss: 0.15343489682398523
train: 0.917450	val: 0.781614	test: 0.747571

Epoch: 38
Loss: 0.15414852601967693
train: 0.915257	val: 0.788450	test: 0.752173

Epoch: 39
Loss: 0.151317244084523
train: 0.918545	val: 0.778010	test: 0.737560

Epoch: 40
Loss: 0.1520315545737657
train: 0.922503	val: 0.789712	test: 0.744053

Epoch: 41
Loss: 0.15145112848480896
train: 0.919779	val: 0.784624	test: 0.740222

Epoch: 42
Loss: 0.149194746279227
train: 0.922845	val: 0.786069	test: 0.725698

Epoch: 43
Loss: 0.14785932282567343
train: 0.925960	val: 0.799173	test: 0.742162

Epoch: 44
Loss: 0.1469261075284638
train: 0.926655	val: 0.778602	test: 0.734779

Epoch: 45
Loss: 0.14686999469289969
train: 0.927413	val: 0.794490	test: 0.746822

Epoch: 46
Loss: 0.14886901569860805
train: 0.930202	val: 0.791415	test: 0.746061

Epoch: 47
Loss: 0.14690001417581058
train: 0.931674	val: 0.785586	test: 0.737728

Epoch: 48
Loss: 0.14603228786048578
train: 0.932641	val: 0.784913	test: 0.734099

Epoch: 49
Loss: 0.1448506794885701
train: 0.931904	val: 0.789641	test: 0.745689

Epoch: 50
Loss: 0.14509892358270235
train: 0.933406	val: 0.792218	test: 0.753643

Epoch: 51
Loss: 0.14314130069324624
train: 0.936867	val: 0.791892	test: 0.744629

Epoch: 52
Loss: 0.14264754764558973
train: 0.938773	val: 0.789094	test: 0.742955

Epoch: 53
Loss: 0.142083173186713
train: 0.935272	val: 0.790614	test: 0.745246

Epoch: 54
Loss: 0.14011886702692933
train: 0.936985	val: 0.781717	test: 0.746938

Epoch: 55
Loss: 0.13842213104775186
train: 0.940502	val: 0.792416	test: 0.751687

Epoch: 56
Loss: 0.1414585839062469
train: 0.938836	val: 0.775330	test: 0.745597

Epoch: 57
Loss: 0.14019239502431877
train: 0.941808	val: 0.790587	test: 0.743192

Epoch: 58
Loss: 0.13836919759892619
train: 0.942469	val: 0.792282	test: 0.739564

Epoch: 59
Loss: 0.13930825029056879
train: 0.943071	val: 0.784451	test: 0.739557

Epoch: 60
Loss: 0.13724462085460742
train: 0.946106	val: 0.798265	test: 0.748297

Epoch: 61
Loss: 0.13688610288291408
train: 0.945977	val: 0.785882	test: 0.739270

Epoch: 62
Loss: 0.13595678241085543
train: 0.946274	val: 0.789326	test: 0.743747

Epoch: 63
Loss: 0.1364340884463206
train: 0.949218	val: 0.790154	test: 0.750064

Epoch: 64
Loss: 0.13572417766306613
train: 0.947400	val: 0.785531	test: 0.751718

Epoch: 65
Loss: 0.13408988392831284
train: 0.949673	val: 0.781398	test: 0.737259

Epoch: 66
Loss: 0.1312471744631315
train: 0.950822	val: 0.794184	test: 0.749805

Epoch: 67
Loss: 0.13323943902370222
train: 0.954037	val: 0.786332	test: 0.744355

Epoch: 68
Loss: 0.13240559093788223
train: 0.952718	val: 0.781068	test: 0.733463

Epoch: 69
Loss: 0.1314634921822298
train: 0.953763	val: 0.784046	test: 0.740819

Epoch: 70
Loss: 0.13012774549207518
train: 0.957079	val: 0.792236	test: 0.745974

Epoch: 71
Loss: 0.130459964063109
train: 0.955112	val: 0.787364	test: 0.744106

Epoch: 72
Loss: 0.12862579342204547
train: 0.955677	val: 0.781707	test: 0.740438

Epoch: 73
Loss: 0.12992416776996168
train: 0.955101	val: 0.780056	test: 0.750513

Epoch: 74
Loss: 0.12955277973111742
train: 0.956898	val: 0.790025	test: 0.746330

Epoch: 75
Loss: 0.12777463534055936
train: 0.958210	val: 0.788297	test: 0.750360

Epoch: 76
Loss: 0.12724976628861248
train: 0.957296	val: 0.782827	test: 0.754947

Epoch: 77
Loss: 0.12655830649581684
train: 0.960282	val: 0.785524	test: 0.739537

Epoch: 78
Loss: 0.12499667924987355
train: 0.961477	val: 0.780212	test: 0.733778

Epoch: 79
Loss: 0.12519609924094907
train: 0.961889	val: 0.792766	test: 0.745917

Epoch: 80
Loss: 0.12648230273669958
train: 0.963241	val: 0.796366	test: 0.748556

Epoch: 81
Loss: 0.12348661944583825
train: 0.963644	val: 0.789560	test: 0.742755

Epoch: 82
Loss: 0.12499098693538471
train: 0.963920	val: 0.789373	test: 0.746813

Epoch: 83
Loss: 0.12399864494442714
train: 0.962616	val: 0.781545	test: 0.738811

Epoch: 84
Loss: 0.12314768156053829
train: 0.965203	val: 0.794887	test: 0.748053

Epoch: 85
Loss: 0.12105041812380725
train: 0.965144	val: 0.774696	test: 0.731743

Epoch: 86
Loss: 0.1207379853381294
train: 0.967835	val: 0.786953	test: 0.741528

Epoch: 87
Loss: 0.12155059962067352
train: 0.966345	val: 0.784262	test: 0.735947

Epoch: 88
Loss: 0.11968503703302778
train: 0.968225	val: 0.783352	test: 0.742921

Epoch: 89
Loss: 0.12031319009128075
train: 0.966254	val: 0.779431	test: 0.733908

Epoch: 90
Loss: 0.11803524447733148
train: 0.967884	val: 0.777434	test: 0.732306

Epoch: 91
Loss: 0.11810200484321406
train: 0.967269	val: 0.770199	test: 0.746107

Epoch: 92
Loss: 0.11878915718704464
train: 0.970619	val: 0.788175	test: 0.738012

Epoch: 93
Loss: 0.11535531191053168
train: 0.970809	val: 0.785817	test: 0.743035

Epoch: 94
Loss: 0.11699539342232779
train: 0.969319	val: 0.787721	test: 0.750239

Epoch: 95
Loss: 0.11587625638799828
train: 0.971165	val: 0.785902	test: 0.737040

Epoch: 96
Loss: 0.11588046833592966
train: 0.969003	val: 0.764071	test: 0.732965

Epoch: 97
Loss: 0.11566989303452317
train: 0.972762	val: 0.769192	test: 0.727699

Epoch: 98
Loss: 0.11409233563593522
train: 0.970647	val: 0.782033	test: 0.749698

Epoch: 99
Loss: 0.114450969093597
train: 0.973399	val: 0.778966	test: 0.740525

Epoch: 100
Loss: 0.11437422653147518
train: 0.973597	val: 0.780936	test: 0.739204

best train: 0.925960	val: 0.799173	test: 0.742162
end
