13698849_0
--dataset=sider --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6691816263316561
train: 0.524623	val: 0.507270	test: 0.486922

Epoch: 2
Loss: 0.6160499422075151
train: 0.557095	val: 0.506116	test: 0.520879

Epoch: 3
Loss: 0.5823667418383325
train: 0.573793	val: 0.512532	test: 0.533347

Epoch: 4
Loss: 0.5587235502067547
train: 0.589453	val: 0.524843	test: 0.547393

Epoch: 5
Loss: 0.5412714889346052
train: 0.619241	val: 0.545293	test: 0.567956

Epoch: 6
Loss: 0.527566472533705
train: 0.641507	val: 0.571018	test: 0.594302

Epoch: 7
Loss: 0.5168094370807457
train: 0.652907	val: 0.578905	test: 0.592138

Epoch: 8
Loss: 0.5126484388360317
train: 0.659114	val: 0.571757	test: 0.590644

Epoch: 9
Loss: 0.5066750445566341
train: 0.667386	val: 0.584994	test: 0.601376

Epoch: 10
Loss: 0.5010740528127602
train: 0.674519	val: 0.589588	test: 0.607473

Epoch: 11
Loss: 0.4927827700581829
train: 0.683696	val: 0.585820	test: 0.611261

Epoch: 12
Loss: 0.4924368572553295
train: 0.690491	val: 0.593291	test: 0.613665

Epoch: 13
Loss: 0.4900610365743073
train: 0.695160	val: 0.599651	test: 0.615912

Epoch: 14
Loss: 0.48859936638676055
train: 0.701657	val: 0.603216	test: 0.623119

Epoch: 15
Loss: 0.48575844706838406
train: 0.708869	val: 0.595228	test: 0.625719

Epoch: 16
Loss: 0.4829844173543593
train: 0.713891	val: 0.596892	test: 0.621466

Epoch: 17
Loss: 0.4818376839842899
train: 0.716508	val: 0.607537	test: 0.624722

Epoch: 18
Loss: 0.475678032598095
train: 0.718409	val: 0.608053	test: 0.633868

Epoch: 19
Loss: 0.4780058355903476
train: 0.724953	val: 0.609988	test: 0.626993

Epoch: 20
Loss: 0.47940968958422464
train: 0.730321	val: 0.606047	test: 0.623793

Epoch: 21
Loss: 0.47419203811868016
train: 0.736273	val: 0.592674	test: 0.629791

Epoch: 22
Loss: 0.4737951622451293
train: 0.740365	val: 0.598797	test: 0.633002

Epoch: 23
Loss: 0.4685532755558369
train: 0.741770	val: 0.604140	test: 0.633750

Epoch: 24
Loss: 0.47099984919454263
train: 0.744169	val: 0.601378	test: 0.629025

Epoch: 25
Loss: 0.46813393580715273
train: 0.742265	val: 0.603666	test: 0.629288

Epoch: 26
Loss: 0.46252511563933385
train: 0.748399	val: 0.613318	test: 0.628211

Epoch: 27
Loss: 0.4693004817831013
train: 0.754710	val: 0.617498	test: 0.626253

Epoch: 28
Loss: 0.46489605612653573
train: 0.752725	val: 0.612398	test: 0.621851

Epoch: 29
Loss: 0.4632219592350175
train: 0.759901	val: 0.618682	test: 0.628297

Epoch: 30
Loss: 0.4579074707405885
train: 0.758515	val: 0.611652	test: 0.621482

Epoch: 31
Loss: 0.46154604008582406
train: 0.761745	val: 0.604442	test: 0.626509

Epoch: 32
Loss: 0.45814597171966537
train: 0.764605	val: 0.608813	test: 0.623578

Epoch: 33
Loss: 0.4573274439118401
train: 0.770021	val: 0.621043	test: 0.631614

Epoch: 34
Loss: 0.45914942591837316
train: 0.762919	val: 0.619910	test: 0.626998

Epoch: 35
Loss: 0.45987315730050937
train: 0.773875	val: 0.610204	test: 0.621753

Epoch: 36
Loss: 0.4573599985368717
train: 0.779406	val: 0.605372	test: 0.636562

Epoch: 37
Loss: 0.4534612086038921
train: 0.774441	val: 0.606138	test: 0.632183

Epoch: 38
Loss: 0.4578547500238561
train: 0.779361	val: 0.621235	test: 0.633279

Epoch: 39
Loss: 0.4519416046647386
train: 0.776745	val: 0.612838	test: 0.621768

Epoch: 40
Loss: 0.45300315157060833
train: 0.781336	val: 0.613452	test: 0.630175

Epoch: 41
Loss: 0.4478733966545866
train: 0.789272	val: 0.616485	test: 0.623162

Epoch: 42
Loss: 0.44729898579363
train: 0.789234	val: 0.613927	test: 0.623362

Epoch: 43
Loss: 0.4508191792632309
train: 0.792036	val: 0.618419	test: 0.629682

Epoch: 44
Loss: 0.4443216359834256
train: 0.794056	val: 0.622400	test: 0.628455

Epoch: 45
Loss: 0.4456171429598121
train: 0.798670	val: 0.621892	test: 0.629367

Epoch: 46
Loss: 0.44377171043391295
train: 0.797088	val: 0.620838	test: 0.626951

Epoch: 47
Loss: 0.4432392308596923
train: 0.798889	val: 0.622441	test: 0.618599

Epoch: 48
Loss: 0.43776895407307376
train: 0.801360	val: 0.619576	test: 0.634607

Epoch: 49
Loss: 0.43771559942634913
train: 0.805856	val: 0.624909	test: 0.627072

Epoch: 50
Loss: 0.4369523485917849
train: 0.806393	val: 0.622822	test: 0.628097

Epoch: 51
Loss: 0.4378871946768631
train: 0.807422	val: 0.600389	test: 0.621588

Epoch: 52
Loss: 0.4373729358878019
train: 0.812188	val: 0.611206	test: 0.628937

Epoch: 53
Loss: 0.4359151730003695
train: 0.811497	val: 0.619587	test: 0.630213

Epoch: 54
Loss: 0.4336516416562034
train: 0.815800	val: 0.623697	test: 0.620758

Epoch: 55
Loss: 0.4340906133777006
train: 0.819971	val: 0.615855	test: 0.611163

Epoch: 56
Loss: 0.4301854170276724
train: 0.820272	val: 0.609452	test: 0.617825

Epoch: 57
Loss: 0.4340129726830364
train: 0.821919	val: 0.613391	test: 0.619445

Epoch: 58
Loss: 0.43198761118851736
train: 0.824526	val: 0.620608	test: 0.616787

Epoch: 59
Loss: 0.4223255163575663
train: 0.825992	val: 0.617594	test: 0.618373

Epoch: 60
Loss: 0.42851630249549844
train: 0.824940	val: 0.614494	test: 0.616846

Epoch: 61
Loss: 0.42973544012156867
train: 0.830408	val: 0.625062	test: 0.610929

Epoch: 62
Loss: 0.42628421914160475
train: 0.827374	val: 0.620089	test: 0.605110

Epoch: 63
Loss: 0.4232227153553609
train: 0.823473	val: 0.614336	test: 0.602662

Epoch: 64
Loss: 0.4180982142106965
train: 0.832347	val: 0.617536	test: 0.604976

Epoch: 65
Loss: 0.4205323765168164
train: 0.835558	val: 0.614791	test: 0.593988

Epoch: 66
Loss: 0.4216261696305727
train: 0.837972	val: 0.621134	test: 0.606523

Epoch: 67
Loss: 0.42087510169694375
train: 0.835588	val: 0.621087	test: 0.611825

Epoch: 68
Loss: 0.42172553761229226
train: 0.833421	val: 0.613525	test: 0.594220

Epoch: 69
Loss: 0.4244228542136345
train: 0.840057	val: 0.619846	test: 0.600831

Epoch: 70
Loss: 0.4156270248092011
train: 0.841739	val: 0.622884	test: 0.612064

Epoch: 71
Loss: 0.41634846885188626
train: 0.843151	val: 0.631158	test: 0.613971

Epoch: 72
Loss: 0.41550899187537793
train: 0.843884	val: 0.619327	test: 0.599437

Epoch: 73
Loss: 0.4164114976549474
train: 0.849250	val: 0.621068	test: 0.608139

Epoch: 74
Loss: 0.40836232038641496
train: 0.848263	val: 0.624642	test: 0.615962

Epoch: 75
Loss: 0.4044220314965896
train: 0.847567	val: 0.624902	test: 0.606061

Epoch: 76
Loss: 0.41264493269035774
train: 0.853739	val: 0.623684	test: 0.604698

Epoch: 77
Loss: 0.407980317308111
train: 0.859150	val: 0.623712	test: 0.595884

Epoch: 78
Loss: 0.4021937202963989
train: 0.858720	val: 0.616429	test: 0.605996

Epoch: 79
Loss: 0.40898465174719084
train: 0.860068	val: 0.623973	test: 0.602840

Epoch: 80
Loss: 0.40303580989317195
train: 0.863344	val: 0.628618	test: 0.602433

Epoch: 81
Loss: 0.40505178863591923
train: 0.861439	val: 0.616953	test: 0.598061

Epoch: 82
Loss: 0.40169499921967605
train: 0.862472	val: 0.627817	test: 0.601224

Epoch: 83
Loss: 0.4006303198393925
train: 0.862065	val: 0.616601	test: 0.607681

Epoch: 84
Loss: 0.3942332771199897
train: 0.866334	val: 0.626530	test: 0.597888

Epoch: 85
Loss: 0.402462772833431
train: 0.863459	val: 0.606600	test: 0.603656

Epoch: 86
Loss: 0.4008702320087397
train: 0.868130	val: 0.615379	test: 0.606554

Epoch: 87
Loss: 0.39519021441891855
train: 0.868377	val: 0.614796	test: 0.598891

Epoch: 88
Loss: 0.40181579335224776
train: 0.872223	val: 0.617177	test: 0.609962

Epoch: 89
Loss: 0.4007347208584898
train: 0.863401	val: 0.618271	test: 0.598357

Epoch: 90
Loss: 0.3954772374934161
train: 0.867293	val: 0.624196	test: 0.610339

Epoch: 91
Loss: 0.38887443720824566
train: 0.872409	val: 0.617671	test: 0.596206

Epoch: 92
Loss: 0.39170796850321943
train: 0.874133	val: 0.620086	test: 0.592072

Epoch: 93
Loss: 0.3868031247878392
train: 0.872506	val: 0.621288	test: 0.604097

Epoch: 94
Loss: 0.3856091269508205
train: 0.875017	val: 0.620805	test: 0.599574

Epoch: 95
Loss: 0.3926389258277519
train: 0.878766	val: 0.617941	test: 0.584501

Epoch: 96
Loss: 0.3887027186803147
train: 0.877933	val: 0.602826	test: 0.584031

Epoch: 97
Loss: 0.39398936125208645
train: 0.877932	val: 0.606222	test: 0.607114

Epoch: 98
Loss: 0.3839139915454401
train: 0.878921	val: 0.634211	test: 0.599060

Epoch: 99
Loss: 0.3826173894291527
train: 0.881844	val: 0.620844	test: 0.600370

Epoch: 100
Loss: 0.38010937506891485
train: 0.883274	val: 0.620796	test: 0.590355

best train: 0.878921	val: 0.634211	test: 0.599060
end
