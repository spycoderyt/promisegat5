9242925_2
--dataset=clintox --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='clintox', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6582544877930331
train: 0.647586	val: 0.631474	test: 0.513094

Epoch: 2
Loss: 0.5505482482142431
train: 0.656412	val: 0.679526	test: 0.512853

Epoch: 3
Loss: 0.477974769861106
train: 0.711268	val: 0.745018	test: 0.580908

Epoch: 4
Loss: 0.4168854187445106
train: 0.740229	val: 0.719681	test: 0.601209

Epoch: 5
Loss: 0.36928807178389633
train: 0.783548	val: 0.701763	test: 0.637950

Epoch: 6
Loss: 0.3329324118278868
train: 0.795262	val: 0.680011	test: 0.614813

Epoch: 7
Loss: 0.3013864699584795
train: 0.821667	val: 0.696506	test: 0.639810

Epoch: 8
Loss: 0.27949357513051687
train: 0.827022	val: 0.708531	test: 0.648917

Epoch: 9
Loss: 0.2615391730028914
train: 0.833227	val: 0.723491	test: 0.664246

Epoch: 10
Loss: 0.24585604529419616
train: 0.837846	val: 0.721730	test: 0.693987

Epoch: 11
Loss: 0.23920906104335563
train: 0.857665	val: 0.728748	test: 0.704574

Epoch: 12
Loss: 0.23735304550843694
train: 0.860585	val: 0.767420	test: 0.694601

Epoch: 13
Loss: 0.22817915218148946
train: 0.875785	val: 0.760764	test: 0.733349

Epoch: 14
Loss: 0.21673589492054385
train: 0.874989	val: 0.723603	test: 0.728127

Epoch: 15
Loss: 0.21348849783286675
train: 0.869564	val: 0.696956	test: 0.748912

Epoch: 16
Loss: 0.20136807440980475
train: 0.888141	val: 0.762862	test: 0.733155

Epoch: 17
Loss: 0.2104739670460701
train: 0.894448	val: 0.782854	test: 0.741268

Epoch: 18
Loss: 0.20947103690167684
train: 0.882558	val: 0.749815	test: 0.720812

Epoch: 19
Loss: 0.19797576668220634
train: 0.894618	val: 0.765498	test: 0.725059

Epoch: 20
Loss: 0.20275988038850765
train: 0.905987	val: 0.726688	test: 0.745263

Epoch: 21
Loss: 0.19298052857289455
train: 0.875948	val: 0.725251	test: 0.739511

Epoch: 22
Loss: 0.18793598241905105
train: 0.893331	val: 0.790121	test: 0.735549

Epoch: 23
Loss: 0.18671538282244898
train: 0.917766	val: 0.800347	test: 0.738130

Epoch: 24
Loss: 0.18882902939818258
train: 0.916780	val: 0.748715	test: 0.762380

Epoch: 25
Loss: 0.17618325280114133
train: 0.921389	val: 0.758593	test: 0.748518

Epoch: 26
Loss: 0.18903900453236283
train: 0.917875	val: 0.768246	test: 0.747136

Epoch: 27
Loss: 0.18969866933376867
train: 0.922154	val: 0.750440	test: 0.774094

Epoch: 28
Loss: 0.1830374293096191
train: 0.934562	val: 0.777973	test: 0.762387

Epoch: 29
Loss: 0.17726214873713098
train: 0.934578	val: 0.767058	test: 0.787010

Epoch: 30
Loss: 0.18246424987685164
train: 0.931739	val: 0.780658	test: 0.768211

Epoch: 31
Loss: 0.1770253296886574
train: 0.936256	val: 0.758368	test: 0.785486

Epoch: 32
Loss: 0.1711018845106743
train: 0.934848	val: 0.766159	test: 0.795430

Epoch: 33
Loss: 0.1789283372365566
train: 0.936059	val: 0.761151	test: 0.783217

Epoch: 34
Loss: 0.17354737094810124
train: 0.949381	val: 0.782467	test: 0.768239

Epoch: 35
Loss: 0.16138755331681337
train: 0.945398	val: 0.771493	test: 0.758841

Epoch: 36
Loss: 0.16569603815955714
train: 0.950464	val: 0.752425	test: 0.762451

Epoch: 37
Loss: 0.1784821063271969
train: 0.956264	val: 0.747418	test: 0.743961

Epoch: 38
Loss: 0.15876724589711297
train: 0.950505	val: 0.752049	test: 0.761599

Epoch: 39
Loss: 0.16137791884502445
train: 0.957916	val: 0.754759	test: 0.783772

Epoch: 40
Loss: 0.16074744556180925
train: 0.957539	val: 0.765972	test: 0.786996

Epoch: 41
Loss: 0.1633666607946261
train: 0.951601	val: 0.733969	test: 0.774774

Epoch: 42
Loss: 0.16459633364807197
train: 0.951687	val: 0.778711	test: 0.747671

Epoch: 43
Loss: 0.16258704734506338
train: 0.964991	val: 0.814235	test: 0.740600

Epoch: 44
Loss: 0.15640223137167392
train: 0.963801	val: 0.822588	test: 0.777026

Epoch: 45
Loss: 0.15952639938603536
train: 0.956341	val: 0.796190	test: 0.782073

Epoch: 46
Loss: 0.1683285958620937
train: 0.963530	val: 0.796004	test: 0.761256

Epoch: 47
Loss: 0.14731353441782877
train: 0.962941	val: 0.792458	test: 0.742424

Epoch: 48
Loss: 0.15410665050363487
train: 0.965532	val: 0.809002	test: 0.769518

Epoch: 49
Loss: 0.1531270293294012
train: 0.963022	val: 0.782717	test: 0.783085

Epoch: 50
Loss: 0.16177440059092218
train: 0.965748	val: 0.820216	test: 0.793753

Epoch: 51
Loss: 0.1484792972827681
train: 0.954363	val: 0.795006	test: 0.806445

Epoch: 52
Loss: 0.1447428367243648
train: 0.970859	val: 0.824211	test: 0.842203

Epoch: 53
Loss: 0.17529603061970894
train: 0.966973	val: 0.833327	test: 0.815064

Epoch: 54
Loss: 0.14389142296572427
train: 0.958082	val: 0.835449	test: 0.803509

Epoch: 55
Loss: 0.1509919110576789
train: 0.949561	val: 0.816670	test: 0.773554

Epoch: 56
Loss: 0.1542010605927308
train: 0.951120	val: 0.771430	test: 0.784779

Epoch: 57
Loss: 0.14864461605614124
train: 0.970780	val: 0.764187	test: 0.791093

Epoch: 58
Loss: 0.15366685813055167
train: 0.972615	val: 0.767757	test: 0.807082

Epoch: 59
Loss: 0.1488192338768513
train: 0.969558	val: 0.766696	test: 0.829593

Epoch: 60
Loss: 0.1376045240819243
train: 0.970912	val: 0.797314	test: 0.820204

Epoch: 61
Loss: 0.15080181810706383
train: 0.973760	val: 0.819653	test: 0.794078

Epoch: 62
Loss: 0.13979535539708537
train: 0.970773	val: 0.806455	test: 0.748819

Epoch: 63
Loss: 0.14846930596837016
train: 0.974684	val: 0.795716	test: 0.781822

Epoch: 64
Loss: 0.1404625279953713
train: 0.971826	val: 0.801672	test: 0.781710

Epoch: 65
Loss: 0.13344198440830463
train: 0.973525	val: 0.792757	test: 0.792516

Epoch: 66
Loss: 0.14067877755929623
train: 0.973583	val: 0.803158	test: 0.804346

Epoch: 67
Loss: 0.1351947740882176
train: 0.971898	val: 0.824686	test: 0.817636

Epoch: 68
Loss: 0.1296336551819561
train: 0.974827	val: 0.825297	test: 0.799460

Epoch: 69
Loss: 0.1314072042345374
train: 0.976693	val: 0.803432	test: 0.810416

Epoch: 70
Loss: 0.1348857749652576
train: 0.977467	val: 0.782267	test: 0.832040

Epoch: 71
Loss: 0.14584119219143052
train: 0.975324	val: 0.811824	test: 0.796588

Epoch: 72
Loss: 0.13253185517833427
train: 0.976558	val: 0.810288	test: 0.770805

Epoch: 73
Loss: 0.14678609759166675
train: 0.975632	val: 0.796889	test: 0.775558

Epoch: 74
Loss: 0.13020160475686987
train: 0.975680	val: 0.782942	test: 0.784034

Epoch: 75
Loss: 0.12496006530522301
train: 0.979289	val: 0.792507	test: 0.798400

Epoch: 76
Loss: 0.13207587336949117
train: 0.977683	val: 0.767283	test: 0.811865

Epoch: 77
Loss: 0.1348777709035944
train: 0.977395	val: 0.771416	test: 0.822539

Epoch: 78
Loss: 0.13573893351597238
train: 0.975606	val: 0.814695	test: 0.836311

Epoch: 79
Loss: 0.11997312375865646
train: 0.979084	val: 0.842692	test: 0.843145

Epoch: 80
Loss: 0.13327546960828993
train: 0.977766	val: 0.814959	test: 0.835562

Epoch: 81
Loss: 0.12962364821280126
train: 0.980433	val: 0.794967	test: 0.818960

Epoch: 82
Loss: 0.11773643362034165
train: 0.978492	val: 0.772178	test: 0.792490

Epoch: 83
Loss: 0.12739104628780526
train: 0.980535	val: 0.796552	test: 0.804507

Epoch: 84
Loss: 0.1291678704580685
train: 0.978185	val: 0.824960	test: 0.810928

Epoch: 85
Loss: 0.12244383105616356
train: 0.979503	val: 0.811125	test: 0.814239

Epoch: 86
Loss: 0.12652690547667783
train: 0.978925	val: 0.773914	test: 0.820098

Epoch: 87
Loss: 0.1297236852594436
train: 0.980454	val: 0.771528	test: 0.832977

Epoch: 88
Loss: 0.1227621824360197
train: 0.978586	val: 0.775949	test: 0.821103

Epoch: 89
Loss: 0.1350594307822381
train: 0.981972	val: 0.837097	test: 0.797712

Epoch: 90
Loss: 0.12263801809703814
train: 0.980005	val: 0.802459	test: 0.770786

Epoch: 91
Loss: 0.12714178071579108
train: 0.979266	val: 0.785314	test: 0.800473

Epoch: 92
Loss: 0.10860801504556598
train: 0.982887	val: 0.793505	test: 0.840347

Epoch: 93
Loss: 0.11948387288313544
train: 0.982438	val: 0.799237	test: 0.857023

Epoch: 94
Loss: 0.11662672918469943
train: 0.981203	val: 0.811037	test: 0.847767

Epoch: 95
Loss: 0.12377904128978442
train: 0.982469	val: 0.846350	test: 0.846462

Epoch: 96
Loss: 0.12986451936100923
train: 0.983289	val: 0.865080	test: 0.851489

Epoch: 97
Loss: 0.12222071696536738
train: 0.982910	val: 0.866704	test: 0.852976

Epoch: 98
Loss: 0.11983836055210076
train: 0.983490	val: 0.863457	test: 0.829466

Epoch: 99
Loss: 0.11764128348006957
train: 0.982345	val: 0.874645	test: 0.844881

Epoch: 100
Loss: 0.11926130435707782
train: 0.979777	val: 0.854829	test: 0.840565

best train: 0.982345	val: 0.874645	test: 0.844881
end
