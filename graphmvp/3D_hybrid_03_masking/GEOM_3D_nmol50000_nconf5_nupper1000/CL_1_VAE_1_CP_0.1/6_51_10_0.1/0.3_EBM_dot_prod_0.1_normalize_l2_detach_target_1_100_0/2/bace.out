9242925_2
--dataset=bace --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6580818945885561
train: 0.703102	val: 0.506960	test: 0.597809

Epoch: 2
Loss: 0.607356048588638
train: 0.759737	val: 0.541026	test: 0.664058

Epoch: 3
Loss: 0.5748843756362143
train: 0.816478	val: 0.614652	test: 0.696053

Epoch: 4
Loss: 0.5395473680977124
train: 0.848096	val: 0.636996	test: 0.740393

Epoch: 5
Loss: 0.5059357530889763
train: 0.854158	val: 0.672894	test: 0.750130

Epoch: 6
Loss: 0.5031251015871793
train: 0.846672	val: 0.670696	test: 0.762650

Epoch: 7
Loss: 0.5018096199656965
train: 0.876296	val: 0.634432	test: 0.786124

Epoch: 8
Loss: 0.4727571250298169
train: 0.878465	val: 0.666667	test: 0.782646

Epoch: 9
Loss: 0.4573762803122712
train: 0.876162	val: 0.658974	test: 0.760389

Epoch: 10
Loss: 0.45019005596630646
train: 0.887534	val: 0.656410	test: 0.762128

Epoch: 11
Loss: 0.45577803399621375
train: 0.873125	val: 0.582051	test: 0.772214

Epoch: 12
Loss: 0.44551141931954474
train: 0.891495	val: 0.634432	test: 0.815336

Epoch: 13
Loss: 0.43258352435623443
train: 0.899492	val: 0.659707	test: 0.815163

Epoch: 14
Loss: 0.43162414197297466
train: 0.904201	val: 0.642125	test: 0.779690

Epoch: 15
Loss: 0.42536212039920357
train: 0.906592	val: 0.652747	test: 0.776213

Epoch: 16
Loss: 0.4237034899076863
train: 0.909181	val: 0.671429	test: 0.805077

Epoch: 17
Loss: 0.4262471442260802
train: 0.903878	val: 0.659341	test: 0.813250

Epoch: 18
Loss: 0.4089179163959701
train: 0.902486	val: 0.653846	test: 0.786472

Epoch: 19
Loss: 0.4125548726586434
train: 0.909852	val: 0.637363	test: 0.805773

Epoch: 20
Loss: 0.3961833159001501
train: 0.917491	val: 0.662637	test: 0.806990

Epoch: 21
Loss: 0.391819259237403
train: 0.920106	val: 0.688278	test: 0.807338

Epoch: 22
Loss: 0.4153357750011577
train: 0.918028	val: 0.648718	test: 0.796383

Epoch: 23
Loss: 0.39025990160043234
train: 0.922009	val: 0.673993	test: 0.789428

Epoch: 24
Loss: 0.387698174177744
train: 0.922432	val: 0.701099	test: 0.790471

Epoch: 25
Loss: 0.39599213110634895
train: 0.924897	val: 0.667399	test: 0.792732

Epoch: 26
Loss: 0.3828751528844534
train: 0.924055	val: 0.660073	test: 0.813076

Epoch: 27
Loss: 0.3849646917549251
train: 0.929298	val: 0.692674	test: 0.820205

Epoch: 28
Loss: 0.38019174568859376
train: 0.929763	val: 0.711355	test: 0.794123

Epoch: 29
Loss: 0.37179734347875065
train: 0.930183	val: 0.698168	test: 0.785950

Epoch: 30
Loss: 0.3773842533619768
train: 0.933151	val: 0.699267	test: 0.815163

Epoch: 31
Loss: 0.37938174312370304
train: 0.933325	val: 0.667033	test: 0.804034

Epoch: 32
Loss: 0.3807181315227125
train: 0.927269	val: 0.651282	test: 0.782473

Epoch: 33
Loss: 0.36817268309441015
train: 0.931490	val: 0.678388	test: 0.789776

Epoch: 34
Loss: 0.3638610726124479
train: 0.934452	val: 0.684249	test: 0.797774

Epoch: 35
Loss: 0.3644219225650246
train: 0.935788	val: 0.664103	test: 0.785603

Epoch: 36
Loss: 0.363450502963727
train: 0.935708	val: 0.682418	test: 0.811163

Epoch: 37
Loss: 0.35477719663309576
train: 0.938459	val: 0.656044	test: 0.801600

Epoch: 38
Loss: 0.35347007114272966
train: 0.940654	val: 0.653480	test: 0.812728

Epoch: 39
Loss: 0.34679064252734093
train: 0.942374	val: 0.656410	test: 0.803860

Epoch: 40
Loss: 0.3593851495295469
train: 0.943542	val: 0.669963	test: 0.793427

Epoch: 41
Loss: 0.34996815958210725
train: 0.941815	val: 0.665568	test: 0.805425

Epoch: 42
Loss: 0.3540533023668834
train: 0.942523	val: 0.659341	test: 0.818119

Epoch: 43
Loss: 0.3491765999189796
train: 0.946221	val: 0.671795	test: 0.822640

Epoch: 44
Loss: 0.3415636212340133
train: 0.943713	val: 0.675824	test: 0.797427

Epoch: 45
Loss: 0.32885271140889893
train: 0.944044	val: 0.674359	test: 0.794123

Epoch: 46
Loss: 0.34682811182545115
train: 0.947611	val: 0.691575	test: 0.797774

Epoch: 47
Loss: 0.34655682338564575
train: 0.947851	val: 0.673260	test: 0.780212

Epoch: 48
Loss: 0.34191008064806916
train: 0.947611	val: 0.666667	test: 0.765258

Epoch: 49
Loss: 0.3468143315693278
train: 0.951207	val: 0.686081	test: 0.807338

Epoch: 50
Loss: 0.3210148718306252
train: 0.951470	val: 0.697070	test: 0.804556

Epoch: 51
Loss: 0.326348482880912
train: 0.952857	val: 0.672527	test: 0.804034

Epoch: 52
Loss: 0.3426896057468679
train: 0.951784	val: 0.670696	test: 0.790993

Epoch: 53
Loss: 0.3331325533872045
train: 0.949880	val: 0.645055	test: 0.798296

Epoch: 54
Loss: 0.3348082765412001
train: 0.952320	val: 0.658974	test: 0.788037

Epoch: 55
Loss: 0.3305605439722358
train: 0.950936	val: 0.667033	test: 0.800209

Epoch: 56
Loss: 0.3245653216459966
train: 0.954649	val: 0.667033	test: 0.788906

Epoch: 57
Loss: 0.3128594544930102
train: 0.952175	val: 0.650916	test: 0.773778

Epoch: 58
Loss: 0.3321793000181641
train: 0.956005	val: 0.659341	test: 0.774822

Epoch: 59
Loss: 0.31419752120754907
train: 0.956998	val: 0.650916	test: 0.802295

Epoch: 60
Loss: 0.3164628831059511
train: 0.956641	val: 0.661172	test: 0.808033

Epoch: 61
Loss: 0.3143824889346562
train: 0.950959	val: 0.674725	test: 0.773431

Epoch: 62
Loss: 0.3143745613978637
train: 0.957015	val: 0.686447	test: 0.796731

Epoch: 63
Loss: 0.32086921778014427
train: 0.956595	val: 0.646520	test: 0.781255

Epoch: 64
Loss: 0.31113468692075996
train: 0.958242	val: 0.647253	test: 0.770301

Epoch: 65
Loss: 0.31420390411860916
train: 0.957985	val: 0.679121	test: 0.784907

Epoch: 66
Loss: 0.3194966278029012
train: 0.959518	val: 0.659341	test: 0.788037

Epoch: 67
Loss: 0.3122762213337894
train: 0.959552	val: 0.674359	test: 0.781429

Epoch: 68
Loss: 0.3096749877937189
train: 0.957180	val: 0.704762	test: 0.797079

Epoch: 69
Loss: 0.30452946478446663
train: 0.961424	val: 0.678022	test: 0.806121

Epoch: 70
Loss: 0.31340657212179746
train: 0.958741	val: 0.648352	test: 0.805947

Epoch: 71
Loss: 0.3109438496373119
train: 0.962874	val: 0.648718	test: 0.808729

Epoch: 72
Loss: 0.28328925364907087
train: 0.961301	val: 0.663004	test: 0.780908

Epoch: 73
Loss: 0.2943990420701027
train: 0.960591	val: 0.664835	test: 0.796557

Epoch: 74
Loss: 0.29852181592308924
train: 0.958131	val: 0.685348	test: 0.814119

Epoch: 75
Loss: 0.29147981511004845
train: 0.964409	val: 0.665568	test: 0.792036

Epoch: 76
Loss: 0.309117566839362
train: 0.958630	val: 0.664103	test: 0.758477

Epoch: 77
Loss: 0.2969637850140305
train: 0.953473	val: 0.638462	test: 0.738654

Epoch: 78
Loss: 0.29082109807098955
train: 0.963573	val: 0.661172	test: 0.779517

Epoch: 79
Loss: 0.29775422549217645
train: 0.965237	val: 0.661172	test: 0.798470

Epoch: 80
Loss: 0.28635010064584415
train: 0.965051	val: 0.654945	test: 0.810120

Epoch: 81
Loss: 0.286332858458687
train: 0.966687	val: 0.667033	test: 0.804730

Epoch: 82
Loss: 0.2992486171862298
train: 0.965682	val: 0.683150	test: 0.806468

Epoch: 83
Loss: 0.27035353104767973
train: 0.964623	val: 0.643223	test: 0.778299

Epoch: 84
Loss: 0.28183347248109325
train: 0.966361	val: 0.643956	test: 0.769431

Epoch: 85
Loss: 0.29773538358554913
train: 0.968850	val: 0.646886	test: 0.798470

Epoch: 86
Loss: 0.2922302689672872
train: 0.968447	val: 0.637363	test: 0.819510

Epoch: 87
Loss: 0.28032184676615246
train: 0.967922	val: 0.666667	test: 0.811511

Epoch: 88
Loss: 0.27995275716996004
train: 0.968014	val: 0.676190	test: 0.804208

Epoch: 89
Loss: 0.28195206013071195
train: 0.969381	val: 0.658974	test: 0.794471

Epoch: 90
Loss: 0.268649481868008
train: 0.970257	val: 0.639927	test: 0.791862

Epoch: 91
Loss: 0.2843603660597663
train: 0.971233	val: 0.635165	test: 0.796731

Epoch: 92
Loss: 0.2857644908151271
train: 0.971553	val: 0.656777	test: 0.801600

Epoch: 93
Loss: 0.2826172593757871
train: 0.971946	val: 0.654945	test: 0.791341

Epoch: 94
Loss: 0.2714673526761481
train: 0.970725	val: 0.659707	test: 0.797253

Epoch: 95
Loss: 0.2705501178023292
train: 0.970585	val: 0.650916	test: 0.811163

Epoch: 96
Loss: 0.2614468616099055
train: 0.974289	val: 0.639927	test: 0.815336

Epoch: 97
Loss: 0.2778267107384008
train: 0.972714	val: 0.650183	test: 0.801947

Epoch: 98
Loss: 0.27215481980916906
train: 0.971561	val: 0.649451	test: 0.819336

Epoch: 99
Loss: 0.2524268460675212
train: 0.972626	val: 0.653480	test: 0.812380

Epoch: 100
Loss: 0.2710760542753669
train: 0.971016	val: 0.644689	test: 0.791688

best train: 0.929763	val: 0.711355	test: 0.794123
end
