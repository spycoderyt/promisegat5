9242925_2
--dataset=hiv --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.22973259180216962
train: 0.754261	val: 0.717948	test: 0.739645

Epoch: 2
Loss: 0.14271244307809405
train: 0.767959	val: 0.721426	test: 0.752896

Epoch: 3
Loss: 0.13679601996198792
train: 0.781370	val: 0.747652	test: 0.780336

Epoch: 4
Loss: 0.13144129697013393
train: 0.806389	val: 0.766409	test: 0.757479

Epoch: 5
Loss: 0.1295830721456118
train: 0.812373	val: 0.773387	test: 0.762610

Epoch: 6
Loss: 0.12668778716227888
train: 0.822547	val: 0.803314	test: 0.768143

Epoch: 7
Loss: 0.1262150991944454
train: 0.830563	val: 0.747327	test: 0.746673

Epoch: 8
Loss: 0.12420798492140114
train: 0.831823	val: 0.710492	test: 0.731700

Epoch: 9
Loss: 0.12292357836696298
train: 0.841814	val: 0.752805	test: 0.727484

Epoch: 10
Loss: 0.12044070171122323
train: 0.843385	val: 0.773877	test: 0.756003

Epoch: 11
Loss: 0.11868957904672268
train: 0.838293	val: 0.772643	test: 0.751440

Epoch: 12
Loss: 0.1184451319789535
train: 0.852414	val: 0.745594	test: 0.759088

Epoch: 13
Loss: 0.11721839251045278
train: 0.858242	val: 0.763353	test: 0.743711

Epoch: 14
Loss: 0.11572906159780998
train: 0.867227	val: 0.777615	test: 0.745318

Epoch: 15
Loss: 0.11512299928814881
train: 0.867989	val: 0.754550	test: 0.722036

Epoch: 16
Loss: 0.11426692792797256
train: 0.869350	val: 0.753209	test: 0.739432

Epoch: 17
Loss: 0.1118021111857733
train: 0.877257	val: 0.793663	test: 0.755544

Epoch: 18
Loss: 0.11195431667041757
train: 0.872375	val: 0.782841	test: 0.742739

Epoch: 19
Loss: 0.11150788641737434
train: 0.877422	val: 0.771464	test: 0.749169

Epoch: 20
Loss: 0.11018003533650349
train: 0.874974	val: 0.773981	test: 0.727764

Epoch: 21
Loss: 0.10969516517371093
train: 0.881623	val: 0.771804	test: 0.726999

Epoch: 22
Loss: 0.10887693132790183
train: 0.893269	val: 0.758904	test: 0.736905

Epoch: 23
Loss: 0.10860903570750653
train: 0.889521	val: 0.771035	test: 0.721370

Epoch: 24
Loss: 0.10762609112574753
train: 0.889475	val: 0.743744	test: 0.743989

Epoch: 25
Loss: 0.10697760356358581
train: 0.886611	val: 0.755481	test: 0.745833

Epoch: 26
Loss: 0.1070860949140263
train: 0.901973	val: 0.788828	test: 0.746612

Epoch: 27
Loss: 0.10600720636135647
train: 0.903785	val: 0.789986	test: 0.759012

Epoch: 28
Loss: 0.10463588263438386
train: 0.903983	val: 0.775356	test: 0.759478

Epoch: 29
Loss: 0.10266060385571325
train: 0.904581	val: 0.748478	test: 0.745768

Epoch: 30
Loss: 0.10426354371375626
train: 0.909674	val: 0.764840	test: 0.755383

Epoch: 31
Loss: 0.10428818183493681
train: 0.904436	val: 0.792144	test: 0.766436

Epoch: 32
Loss: 0.10326031408343951
train: 0.907682	val: 0.787261	test: 0.757574

Epoch: 33
Loss: 0.1038525381499236
train: 0.917269	val: 0.781425	test: 0.760627

Epoch: 34
Loss: 0.10209555172777629
train: 0.914076	val: 0.762199	test: 0.758018

Epoch: 35
Loss: 0.10346389906156446
train: 0.916262	val: 0.781587	test: 0.760857

Epoch: 36
Loss: 0.10128567798752897
train: 0.916064	val: 0.754287	test: 0.750700

Epoch: 37
Loss: 0.10092866774685563
train: 0.919547	val: 0.774015	test: 0.746428

Epoch: 38
Loss: 0.10000118883942538
train: 0.926108	val: 0.773240	test: 0.753110

Epoch: 39
Loss: 0.09868490011231768
train: 0.919923	val: 0.785080	test: 0.760976

Epoch: 40
Loss: 0.09883873727916845
train: 0.924243	val: 0.793883	test: 0.736457

Epoch: 41
Loss: 0.09886400550771898
train: 0.924525	val: 0.763471	test: 0.766454

Epoch: 42
Loss: 0.09725051618716878
train: 0.926310	val: 0.768286	test: 0.746878

Epoch: 43
Loss: 0.09882083602828705
train: 0.930275	val: 0.789471	test: 0.724077

Epoch: 44
Loss: 0.09792792412557798
train: 0.923922	val: 0.768298	test: 0.745756

Epoch: 45
Loss: 0.09748826386455618
train: 0.934792	val: 0.774645	test: 0.755638

Epoch: 46
Loss: 0.09721575043814772
train: 0.926474	val: 0.774287	test: 0.733376

Epoch: 47
Loss: 0.09752524548024205
train: 0.928042	val: 0.760839	test: 0.753981

Epoch: 48
Loss: 0.0948473037160544
train: 0.934179	val: 0.756712	test: 0.759163

Epoch: 49
Loss: 0.09515867752490591
train: 0.937820	val: 0.758794	test: 0.753612

Epoch: 50
Loss: 0.09527448464946087
train: 0.931852	val: 0.742605	test: 0.744434

Epoch: 51
Loss: 0.09438911231960023
train: 0.943564	val: 0.762885	test: 0.761861

Epoch: 52
Loss: 0.09346621875133669
train: 0.932252	val: 0.729874	test: 0.736517

Epoch: 53
Loss: 0.09547231450127983
train: 0.940638	val: 0.770855	test: 0.759120

Epoch: 54
Loss: 0.09315809055895886
train: 0.937492	val: 0.741234	test: 0.719191

Epoch: 55
Loss: 0.09283006014055574
train: 0.932586	val: 0.770806	test: 0.744449

Epoch: 56
Loss: 0.0931764438666951
train: 0.942838	val: 0.755726	test: 0.758599

Epoch: 57
Loss: 0.09213375414488414
train: 0.945631	val: 0.771783	test: 0.747508

Epoch: 58
Loss: 0.09160889053753275
train: 0.950580	val: 0.778136	test: 0.755098

Epoch: 59
Loss: 0.09040687302872671
train: 0.950326	val: 0.759988	test: 0.756878

Epoch: 60
Loss: 0.09103285756930798
train: 0.942677	val: 0.778491	test: 0.741882

Epoch: 61
Loss: 0.09160615324786235
train: 0.947412	val: 0.787086	test: 0.747691

Epoch: 62
Loss: 0.0920305785130773
train: 0.949165	val: 0.779988	test: 0.730663

Epoch: 63
Loss: 0.0921839620179239
train: 0.949238	val: 0.767141	test: 0.754818

Epoch: 64
Loss: 0.09143774724013001
train: 0.955013	val: 0.767523	test: 0.763937

Epoch: 65
Loss: 0.08895890617596866
train: 0.945636	val: 0.763053	test: 0.756208

Epoch: 66
Loss: 0.09070567081291621
train: 0.946659	val: 0.740924	test: 0.720647

Epoch: 67
Loss: 0.08879975087799458
train: 0.949172	val: 0.776461	test: 0.744201

Epoch: 68
Loss: 0.08930311586640007
train: 0.953545	val: 0.795990	test: 0.762875

Epoch: 69
Loss: 0.08675714625492312
train: 0.950758	val: 0.759278	test: 0.743004

Epoch: 70
Loss: 0.08812347433763693
train: 0.960673	val: 0.753846	test: 0.759893

Epoch: 71
Loss: 0.08789007225749344
train: 0.958590	val: 0.771219	test: 0.767357

Epoch: 72
Loss: 0.08741809169908175
train: 0.957149	val: 0.755086	test: 0.754796

Epoch: 73
Loss: 0.08633483017079614
train: 0.957783	val: 0.802197	test: 0.748344

Epoch: 74
Loss: 0.08603960272383467
train: 0.959116	val: 0.728080	test: 0.764308

Epoch: 75
Loss: 0.08579573195581676
train: 0.960685	val: 0.760891	test: 0.765948

Epoch: 76
Loss: 0.08665986994964388
train: 0.961685	val: 0.785442	test: 0.765608

Epoch: 77
Loss: 0.08604535067429224
train: 0.961412	val: 0.754617	test: 0.762367

Epoch: 78
Loss: 0.08639439801411812
train: 0.960359	val: 0.752015	test: 0.766531

Epoch: 79
Loss: 0.08439164139657451
train: 0.964061	val: 0.751026	test: 0.739794

Epoch: 80
Loss: 0.08394878506759058
train: 0.959248	val: 0.745508	test: 0.721258

Epoch: 81
Loss: 0.08376358583006219
train: 0.968044	val: 0.751430	test: 0.737901

Epoch: 82
Loss: 0.08361640572474523
train: 0.962385	val: 0.749587	test: 0.739379

Epoch: 83
Loss: 0.08384816112828683
train: 0.963322	val: 0.778491	test: 0.742954

Epoch: 84
Loss: 0.08257564135062055
train: 0.967568	val: 0.741757	test: 0.743758

Epoch: 85
Loss: 0.08260183877352312
train: 0.965650	val: 0.774021	test: 0.763877

Epoch: 86
Loss: 0.08331016911783751
train: 0.969230	val: 0.748944	test: 0.742639

Epoch: 87
Loss: 0.07991286636995983
train: 0.971535	val: 0.733760	test: 0.746772

Epoch: 88
Loss: 0.08044897472525402
train: 0.967582	val: 0.748500	test: 0.762286

Epoch: 89
Loss: 0.08115260510009864
train: 0.966621	val: 0.774048	test: 0.756834

Epoch: 90
Loss: 0.08143294806200732
train: 0.970671	val: 0.763711	test: 0.753991

Epoch: 91
Loss: 0.08057478413747302
train: 0.967144	val: 0.744706	test: 0.740588

Epoch: 92
Loss: 0.08190177884434759
train: 0.970241	val: 0.769798	test: 0.760399

Epoch: 93
Loss: 0.08083291032623338
train: 0.970415	val: 0.740003	test: 0.744242

Epoch: 94
Loss: 0.07954459411079663
train: 0.973771	val: 0.758589	test: 0.748388

Epoch: 95
Loss: 0.0794867657934704
train: 0.973023	val: 0.753233	test: 0.750772

Epoch: 96
Loss: 0.07899424380104306
train: 0.975978	val: 0.782631	test: 0.771760

Epoch: 97
Loss: 0.07827771003203386
train: 0.973582	val: 0.766785	test: 0.734443

Epoch: 98
Loss: 0.07908658321694266
train: 0.973363	val: 0.764492	test: 0.753016

Epoch: 99
Loss: 0.07807767794248958
train: 0.972875	val: 0.766825	test: 0.753746

Epoch: 100
Loss: 0.07768069386079116
train: 0.974486	val: 0.749394	test: 0.749671

best train: 0.822547	val: 0.803314	test: 0.768143
end
