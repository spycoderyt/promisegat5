11405800_1
--dataset=bace --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_1_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6537365309796924
train: 0.718955	val: 0.520147	test: 0.667884

Epoch: 2
Loss: 0.5924891903829785
train: 0.753556	val: 0.554945	test: 0.673100

Epoch: 3
Loss: 0.5606800176567436
train: 0.820768	val: 0.608791	test: 0.766997

Epoch: 4
Loss: 0.5237991043807408
train: 0.855636	val: 0.641758	test: 0.792558

Epoch: 5
Loss: 0.5157995164660694
train: 0.862223	val: 0.683516	test: 0.785776

Epoch: 6
Loss: 0.4990677219937913
train: 0.866164	val: 0.670696	test: 0.783168

Epoch: 7
Loss: 0.48977601306598
train: 0.881119	val: 0.650916	test: 0.808555

Epoch: 8
Loss: 0.47472071013357714
train: 0.887517	val: 0.657143	test: 0.808381

Epoch: 9
Loss: 0.4444451216247204
train: 0.889241	val: 0.670330	test: 0.800904

Epoch: 10
Loss: 0.4670193777222093
train: 0.887414	val: 0.641026	test: 0.816032

Epoch: 11
Loss: 0.4486777323989176
train: 0.889195	val: 0.626374	test: 0.829073

Epoch: 12
Loss: 0.43198109544096663
train: 0.901013	val: 0.669231	test: 0.836898

Epoch: 13
Loss: 0.4351366348145306
train: 0.904703	val: 0.667766	test: 0.813076

Epoch: 14
Loss: 0.41955874398384296
train: 0.901299	val: 0.649451	test: 0.793079

Epoch: 15
Loss: 0.43589629370996574
train: 0.906190	val: 0.640659	test: 0.798644

Epoch: 16
Loss: 0.41752939360622215
train: 0.910928	val: 0.664469	test: 0.803686

Epoch: 17
Loss: 0.41215932734629745
train: 0.911104	val: 0.663004	test: 0.798296

Epoch: 18
Loss: 0.39520627535760633
train: 0.914746	val: 0.661538	test: 0.814293

Epoch: 19
Loss: 0.40475494503682674
train: 0.915999	val: 0.635165	test: 0.822640

Epoch: 20
Loss: 0.40774362910598405
train: 0.917654	val: 0.643590	test: 0.785603

Epoch: 21
Loss: 0.4053934129509346
train: 0.918119	val: 0.675824	test: 0.771518

Epoch: 22
Loss: 0.3971771952479143
train: 0.922372	val: 0.673260	test: 0.813250

Epoch: 23
Loss: 0.3965541562438134
train: 0.924458	val: 0.656777	test: 0.822640

Epoch: 24
Loss: 0.3893950138154631
train: 0.923664	val: 0.664835	test: 0.806468

Epoch: 25
Loss: 0.39134869751945145
train: 0.924666	val: 0.655311	test: 0.801947

Epoch: 26
Loss: 0.40665616004642136
train: 0.925385	val: 0.677656	test: 0.779343

Epoch: 27
Loss: 0.3903859897209701
train: 0.928011	val: 0.711355	test: 0.825074

Epoch: 28
Loss: 0.3951874413469703
train: 0.929980	val: 0.671795	test: 0.808207

Epoch: 29
Loss: 0.3808130914082439
train: 0.929715	val: 0.675092	test: 0.800383

Epoch: 30
Loss: 0.3771258805467005
train: 0.931099	val: 0.702930	test: 0.807686

Epoch: 31
Loss: 0.3856431398078223
train: 0.933092	val: 0.678755	test: 0.825596

Epoch: 32
Loss: 0.37061744147636694
train: 0.932854	val: 0.677656	test: 0.824900

Epoch: 33
Loss: 0.3794682492221356
train: 0.934195	val: 0.672527	test: 0.832203

Epoch: 34
Loss: 0.3599785597026672
train: 0.935431	val: 0.676557	test: 0.815163

Epoch: 35
Loss: 0.35720734476537314
train: 0.929258	val: 0.674725	test: 0.776908

Epoch: 36
Loss: 0.366068322324674
train: 0.929526	val: 0.691575	test: 0.780212

Epoch: 37
Loss: 0.35310258069646855
train: 0.930665	val: 0.675824	test: 0.768040

Epoch: 38
Loss: 0.3559082572289515
train: 0.938716	val: 0.670330	test: 0.791862

Epoch: 39
Loss: 0.3612634102419564
train: 0.940203	val: 0.654579	test: 0.827856

Epoch: 40
Loss: 0.3485085144249257
train: 0.939404	val: 0.664103	test: 0.802469

Epoch: 41
Loss: 0.35938183680575503
train: 0.939018	val: 0.669231	test: 0.785255

Epoch: 42
Loss: 0.3488689898229239
train: 0.940662	val: 0.672894	test: 0.810468

Epoch: 43
Loss: 0.3641150960270217
train: 0.941924	val: 0.662271	test: 0.811859

Epoch: 44
Loss: 0.3528930040466879
train: 0.939994	val: 0.673626	test: 0.810294

Epoch: 45
Loss: 0.34403685311068755
train: 0.943856	val: 0.664469	test: 0.805599

Epoch: 46
Loss: 0.3313590032626943
train: 0.944398	val: 0.654579	test: 0.795340

Epoch: 47
Loss: 0.3348932672721059
train: 0.945434	val: 0.646886	test: 0.792558

Epoch: 48
Loss: 0.3482993149748101
train: 0.946062	val: 0.650549	test: 0.805947

Epoch: 49
Loss: 0.34039512769809877
train: 0.945856	val: 0.650549	test: 0.788559

Epoch: 50
Loss: 0.346246608681286
train: 0.945188	val: 0.635165	test: 0.784559

Epoch: 51
Loss: 0.3360526552912729
train: 0.947743	val: 0.632601	test: 0.785950

Epoch: 52
Loss: 0.32007662490570066
train: 0.947891	val: 0.665934	test: 0.785429

Epoch: 53
Loss: 0.3369768540379634
train: 0.948736	val: 0.671429	test: 0.783168

Epoch: 54
Loss: 0.32538444752103785
train: 0.949503	val: 0.660440	test: 0.770127

Epoch: 55
Loss: 0.34137005382414526
train: 0.952788	val: 0.652747	test: 0.801600

Epoch: 56
Loss: 0.33032709604805166
train: 0.947397	val: 0.651648	test: 0.761607

Epoch: 57
Loss: 0.3296857380571794
train: 0.937951	val: 0.648352	test: 0.737089

Epoch: 58
Loss: 0.32441692110483034
train: 0.952494	val: 0.644322	test: 0.784385

Epoch: 59
Loss: 0.3149189111051559
train: 0.953921	val: 0.654579	test: 0.801774

Epoch: 60
Loss: 0.3298129923783304
train: 0.952032	val: 0.684249	test: 0.781255

Epoch: 61
Loss: 0.3227429151751026
train: 0.954920	val: 0.696703	test: 0.787689

Epoch: 62
Loss: 0.31211854201243994
train: 0.953793	val: 0.689744	test: 0.813250

Epoch: 63
Loss: 0.32252937036598106
train: 0.956755	val: 0.663736	test: 0.805599

Epoch: 64
Loss: 0.3178784078502654
train: 0.955445	val: 0.651282	test: 0.793949

Epoch: 65
Loss: 0.3310922334319895
train: 0.956829	val: 0.686447	test: 0.814815

Epoch: 66
Loss: 0.32250630036526107
train: 0.956955	val: 0.694139	test: 0.798122

Epoch: 67
Loss: 0.3058690839864162
train: 0.957306	val: 0.661172	test: 0.790123

Epoch: 68
Loss: 0.3065351593200708
train: 0.958818	val: 0.665568	test: 0.790297

Epoch: 69
Loss: 0.3023220806340109
train: 0.958436	val: 0.658242	test: 0.781777

Epoch: 70
Loss: 0.30775274201628205
train: 0.960260	val: 0.646520	test: 0.802295

Epoch: 71
Loss: 0.31204832507026464
train: 0.956775	val: 0.657875	test: 0.798470

Epoch: 72
Loss: 0.30282950582774115
train: 0.958995	val: 0.655311	test: 0.818814

Epoch: 73
Loss: 0.3105257986161672
train: 0.960180	val: 0.639560	test: 0.779343

Epoch: 74
Loss: 0.2953504440826046
train: 0.959826	val: 0.658242	test: 0.774300

Epoch: 75
Loss: 0.30624056403241784
train: 0.960990	val: 0.669231	test: 0.785950

Epoch: 76
Loss: 0.2951515289271435
train: 0.959994	val: 0.662637	test: 0.782994

Epoch: 77
Loss: 0.3070277746054135
train: 0.961030	val: 0.680220	test: 0.792906

Epoch: 78
Loss: 0.3042716651781592
train: 0.962951	val: 0.687546	test: 0.796731

Epoch: 79
Loss: 0.30002643490655995
train: 0.959906	val: 0.690110	test: 0.771344

Epoch: 80
Loss: 0.27886110909838774
train: 0.963881	val: 0.663370	test: 0.779343

Epoch: 81
Loss: 0.2879741987115537
train: 0.964366	val: 0.668498	test: 0.794992

Epoch: 82
Loss: 0.285464874561547
train: 0.963239	val: 0.645788	test: 0.796209

Epoch: 83
Loss: 0.3067859196329567
train: 0.964495	val: 0.659341	test: 0.787167

Epoch: 84
Loss: 0.2836667872109905
train: 0.965462	val: 0.654579	test: 0.772040

Epoch: 85
Loss: 0.28924505735597317
train: 0.962902	val: 0.651648	test: 0.760737

Epoch: 86
Loss: 0.29230844799430156
train: 0.964900	val: 0.674359	test: 0.766302

Epoch: 87
Loss: 0.281434435355459
train: 0.965699	val: 0.681685	test: 0.759346

Epoch: 88
Loss: 0.27088368810155433
train: 0.967409	val: 0.688645	test: 0.761085

Epoch: 89
Loss: 0.29143461961418654
train: 0.965220	val: 0.683516	test: 0.765606

Epoch: 90
Loss: 0.28717313812780454
train: 0.967132	val: 0.640659	test: 0.754304

Epoch: 91
Loss: 0.26976681573651157
train: 0.969130	val: 0.640659	test: 0.774474

Epoch: 92
Loss: 0.28270585318634234
train: 0.968171	val: 0.656044	test: 0.786472

Epoch: 93
Loss: 0.2719993839423448
train: 0.967158	val: 0.665934	test: 0.784733

Epoch: 94
Loss: 0.25609870872121976
train: 0.969018	val: 0.667033	test: 0.778126

Epoch: 95
Loss: 0.26703216490590975
train: 0.971858	val: 0.641758	test: 0.799165

Epoch: 96
Loss: 0.285479357097333
train: 0.970671	val: 0.639194	test: 0.794471

Epoch: 97
Loss: 0.2577091507992896
train: 0.971104	val: 0.664469	test: 0.780908

Epoch: 98
Loss: 0.2580923191186715
train: 0.967834	val: 0.680586	test: 0.757607

Epoch: 99
Loss: 0.2681809770738206
train: 0.972103	val: 0.658242	test: 0.758477

Epoch: 100
Loss: 0.2691049529633059
train: 0.972877	val: 0.649084	test: 0.792732

best train: 0.928011	val: 0.711355	test: 0.825074
end
