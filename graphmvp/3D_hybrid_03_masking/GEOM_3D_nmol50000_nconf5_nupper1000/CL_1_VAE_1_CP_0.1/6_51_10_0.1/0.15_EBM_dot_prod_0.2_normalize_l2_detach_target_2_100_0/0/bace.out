11501686_0
--dataset=bace --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6882818002134177
train: 0.686553	val: 0.586447	test: 0.602156

Epoch: 2
Loss: 0.6507447282030461
train: 0.737426	val: 0.625275	test: 0.680230

Epoch: 3
Loss: 0.6054154424770424
train: 0.779615	val: 0.636264	test: 0.687880

Epoch: 4
Loss: 0.5586008461573226
train: 0.812563	val: 0.683883	test: 0.718832

Epoch: 5
Loss: 0.5295756476558143
train: 0.811775	val: 0.703663	test: 0.728395

Epoch: 6
Loss: 0.5252772530537179
train: 0.861752	val: 0.648352	test: 0.758998

Epoch: 7
Loss: 0.4957062128856403
train: 0.863096	val: 0.671429	test: 0.771344

Epoch: 8
Loss: 0.49713471938715337
train: 0.873516	val: 0.682051	test: 0.777778

Epoch: 9
Loss: 0.4638541467636374
train: 0.880006	val: 0.677656	test: 0.772561

Epoch: 10
Loss: 0.44925404462263446
train: 0.879104	val: 0.668864	test: 0.775691

Epoch: 11
Loss: 0.4620171396644145
train: 0.890188	val: 0.637363	test: 0.780734

Epoch: 12
Loss: 0.4542655238761865
train: 0.894638	val: 0.663004	test: 0.770822

Epoch: 13
Loss: 0.4361361095134833
train: 0.897554	val: 0.658974	test: 0.781255

Epoch: 14
Loss: 0.44145144805873154
train: 0.902009	val: 0.663370	test: 0.791688

Epoch: 15
Loss: 0.4319795944083026
train: 0.903482	val: 0.669597	test: 0.795340

Epoch: 16
Loss: 0.42858143042988656
train: 0.903930	val: 0.672527	test: 0.800730

Epoch: 17
Loss: 0.4084549440329929
train: 0.909110	val: 0.686081	test: 0.816728

Epoch: 18
Loss: 0.41411054967290806
train: 0.911290	val: 0.668132	test: 0.799165

Epoch: 19
Loss: 0.4241672884711189
train: 0.914863	val: 0.679121	test: 0.788037

Epoch: 20
Loss: 0.3957029039439159
train: 0.916470	val: 0.676923	test: 0.815163

Epoch: 21
Loss: 0.40249349145724445
train: 0.916373	val: 0.674359	test: 0.838289

Epoch: 22
Loss: 0.4086144150874974
train: 0.920157	val: 0.695604	test: 0.837767

Epoch: 23
Loss: 0.4148205476022995
train: 0.918887	val: 0.677289	test: 0.809077

Epoch: 24
Loss: 0.39612016628208213
train: 0.923031	val: 0.702930	test: 0.816728

Epoch: 25
Loss: 0.39936455093544077
train: 0.924155	val: 0.690842	test: 0.822987

Epoch: 26
Loss: 0.394638408269606
train: 0.925696	val: 0.677289	test: 0.824900

Epoch: 27
Loss: 0.40942690265404574
train: 0.924755	val: 0.674725	test: 0.806468

Epoch: 28
Loss: 0.38841554039630755
train: 0.927175	val: 0.671062	test: 0.810989

Epoch: 29
Loss: 0.39813855539489273
train: 0.928659	val: 0.683516	test: 0.804903

Epoch: 30
Loss: 0.376753538234773
train: 0.929167	val: 0.693773	test: 0.811685

Epoch: 31
Loss: 0.38468529607815183
train: 0.929817	val: 0.702198	test: 0.816206

Epoch: 32
Loss: 0.3648087857359421
train: 0.931047	val: 0.687546	test: 0.818466

Epoch: 33
Loss: 0.3619659212962393
train: 0.929589	val: 0.685714	test: 0.811685

Epoch: 34
Loss: 0.3768690727111612
train: 0.933493	val: 0.681319	test: 0.811337

Epoch: 35
Loss: 0.3924614233058968
train: 0.935528	val: 0.676190	test: 0.805773

Epoch: 36
Loss: 0.36365763565234027
train: 0.936735	val: 0.656410	test: 0.819162

Epoch: 37
Loss: 0.3652964128307288
train: 0.936113	val: 0.668864	test: 0.805425

Epoch: 38
Loss: 0.3516867295933358
train: 0.934951	val: 0.678022	test: 0.820727

Epoch: 39
Loss: 0.3752462080864792
train: 0.936678	val: 0.693040	test: 0.824726

Epoch: 40
Loss: 0.3744708155577404
train: 0.938051	val: 0.689377	test: 0.811337

Epoch: 41
Loss: 0.35709144916996527
train: 0.940243	val: 0.674725	test: 0.813424

Epoch: 42
Loss: 0.3691040593970601
train: 0.940320	val: 0.671062	test: 0.817075

Epoch: 43
Loss: 0.35584951920483265
train: 0.941712	val: 0.683150	test: 0.802643

Epoch: 44
Loss: 0.36025064457501
train: 0.942945	val: 0.690110	test: 0.801947

Epoch: 45
Loss: 0.35130678686967154
train: 0.943995	val: 0.683883	test: 0.807860

Epoch: 46
Loss: 0.3447532348358564
train: 0.942354	val: 0.678388	test: 0.797253

Epoch: 47
Loss: 0.34758238691440546
train: 0.942934	val: 0.693407	test: 0.788211

Epoch: 48
Loss: 0.34744850158558677
train: 0.942392	val: 0.712454	test: 0.773431

Epoch: 49
Loss: 0.34461838128966943
train: 0.944212	val: 0.692674	test: 0.779517

Epoch: 50
Loss: 0.3427923667496512
train: 0.947848	val: 0.689011	test: 0.809251

Epoch: 51
Loss: 0.34515042951931485
train: 0.949207	val: 0.700366	test: 0.809424

Epoch: 52
Loss: 0.32549702687083737
train: 0.950779	val: 0.695971	test: 0.812033

Epoch: 53
Loss: 0.3189989913070065
train: 0.950000	val: 0.681319	test: 0.809077

Epoch: 54
Loss: 0.32937503655734074
train: 0.949949	val: 0.666667	test: 0.801600

Epoch: 55
Loss: 0.3278187540281957
train: 0.949732	val: 0.668864	test: 0.808381

Epoch: 56
Loss: 0.33598377873656177
train: 0.949127	val: 0.661538	test: 0.801774

Epoch: 57
Loss: 0.3246245362906006
train: 0.953045	val: 0.681319	test: 0.796905

Epoch: 58
Loss: 0.3202659712141398
train: 0.954289	val: 0.678388	test: 0.800209

Epoch: 59
Loss: 0.33356502788732967
train: 0.955123	val: 0.681319	test: 0.806295

Epoch: 60
Loss: 0.3127647676062282
train: 0.954632	val: 0.683883	test: 0.787689

Epoch: 61
Loss: 0.31526176535245404
train: 0.954806	val: 0.676557	test: 0.782646

Epoch: 62
Loss: 0.322141977075911
train: 0.956627	val: 0.653846	test: 0.801947

Epoch: 63
Loss: 0.30308945268377874
train: 0.958787	val: 0.654945	test: 0.813945

Epoch: 64
Loss: 0.3182162490356129
train: 0.959235	val: 0.690842	test: 0.809251

Epoch: 65
Loss: 0.3193057956849098
train: 0.958599	val: 0.695971	test: 0.800730

Epoch: 66
Loss: 0.29448248286711043
train: 0.958622	val: 0.700000	test: 0.806121

Epoch: 67
Loss: 0.3002009304075525
train: 0.960699	val: 0.692674	test: 0.803165

Epoch: 68
Loss: 0.31564118643222355
train: 0.960551	val: 0.676923	test: 0.787341

Epoch: 69
Loss: 0.2961493520400125
train: 0.960297	val: 0.671429	test: 0.798122

Epoch: 70
Loss: 0.3047915929605633
train: 0.961259	val: 0.653114	test: 0.805947

Epoch: 71
Loss: 0.3136846674266374
train: 0.959966	val: 0.679853	test: 0.807512

Epoch: 72
Loss: 0.30448165028949215
train: 0.961307	val: 0.690842	test: 0.806816

Epoch: 73
Loss: 0.3042999945979449
train: 0.964232	val: 0.684982	test: 0.801947

Epoch: 74
Loss: 0.29546924284637444
train: 0.960568	val: 0.716484	test: 0.788385

Epoch: 75
Loss: 0.29804069792856563
train: 0.960482	val: 0.721978	test: 0.800035

Epoch: 76
Loss: 0.3033834715423712
train: 0.962451	val: 0.671795	test: 0.821944

Epoch: 77
Loss: 0.29925282562421185
train: 0.961124	val: 0.661172	test: 0.819510

Epoch: 78
Loss: 0.2939541058632084
train: 0.964609	val: 0.672161	test: 0.805599

Epoch: 79
Loss: 0.29575879438846503
train: 0.966299	val: 0.681685	test: 0.806295

Epoch: 80
Loss: 0.28903026586859737
train: 0.965639	val: 0.666300	test: 0.807860

Epoch: 81
Loss: 0.2933507798860469
train: 0.960748	val: 0.666300	test: 0.809772

Epoch: 82
Loss: 0.2809121268209157
train: 0.963542	val: 0.686813	test: 0.819510

Epoch: 83
Loss: 0.2999816624822145
train: 0.966096	val: 0.681685	test: 0.820901

Epoch: 84
Loss: 0.28524592528267234
train: 0.964024	val: 0.684615	test: 0.805077

Epoch: 85
Loss: 0.2929604022971082
train: 0.963527	val: 0.663736	test: 0.784038

Epoch: 86
Loss: 0.29870484051045293
train: 0.966344	val: 0.677656	test: 0.790471

Epoch: 87
Loss: 0.29800248603186075
train: 0.969301	val: 0.644322	test: 0.807686

Epoch: 88
Loss: 0.2954401684749948
train: 0.964192	val: 0.645788	test: 0.800730

Epoch: 89
Loss: 0.2885158537109699
train: 0.966644	val: 0.653480	test: 0.801426

Epoch: 90
Loss: 0.2738978516768167
train: 0.971073	val: 0.672894	test: 0.794297

Epoch: 91
Loss: 0.26814995008399806
train: 0.970280	val: 0.673993	test: 0.810120

Epoch: 92
Loss: 0.2786491480069794
train: 0.969969	val: 0.694139	test: 0.797079

Epoch: 93
Loss: 0.27541066991310614
train: 0.971995	val: 0.688278	test: 0.802121

Epoch: 94
Loss: 0.2824269206853903
train: 0.972172	val: 0.679121	test: 0.815858

Epoch: 95
Loss: 0.28143849992519615
train: 0.971421	val: 0.674725	test: 0.804382

Epoch: 96
Loss: 0.2668200668826302
train: 0.973350	val: 0.665934	test: 0.807860

Epoch: 97
Loss: 0.2857329362821807
train: 0.972237	val: 0.672894	test: 0.819857

Epoch: 98
Loss: 0.2593837667185825
train: 0.972309	val: 0.687546	test: 0.811337

Epoch: 99
Loss: 0.2567349543610279
train: 0.972446	val: 0.684249	test: 0.798122

Epoch: 100
Loss: 0.27401222301007344
train: 0.974295	val: 0.683883	test: 0.810468

best train: 0.960482	val: 0.721978	test: 0.800035
end
