11501686_0
--dataset=tox21 --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5312718096085951
train: 0.711513	val: 0.632227	test: 0.593585

Epoch: 2
Loss: 0.3347125902747139
train: 0.753868	val: 0.694241	test: 0.650055

Epoch: 3
Loss: 0.24208193970109118
train: 0.776353	val: 0.713505	test: 0.678596

Epoch: 4
Loss: 0.21262261211931188
train: 0.802940	val: 0.736814	test: 0.695715

Epoch: 5
Loss: 0.2011972570647022
train: 0.816393	val: 0.754019	test: 0.704444

Epoch: 6
Loss: 0.19616150765504725
train: 0.830204	val: 0.748874	test: 0.706138

Epoch: 7
Loss: 0.19195404048741696
train: 0.836962	val: 0.753246	test: 0.711518

Epoch: 8
Loss: 0.18997725137547203
train: 0.842291	val: 0.752230	test: 0.722013

Epoch: 9
Loss: 0.18608108287906625
train: 0.851354	val: 0.760892	test: 0.727027

Epoch: 10
Loss: 0.18407312727218186
train: 0.853725	val: 0.765945	test: 0.736686

Epoch: 11
Loss: 0.18519291693861445
train: 0.852930	val: 0.764648	test: 0.739499

Epoch: 12
Loss: 0.18016713343477375
train: 0.863130	val: 0.768536	test: 0.738918

Epoch: 13
Loss: 0.176751314508572
train: 0.867248	val: 0.779195	test: 0.741054

Epoch: 14
Loss: 0.17887784435158627
train: 0.868234	val: 0.777177	test: 0.735458

Epoch: 15
Loss: 0.17403447068855404
train: 0.873850	val: 0.776957	test: 0.750993

Epoch: 16
Loss: 0.17511797352900124
train: 0.872351	val: 0.774206	test: 0.730899

Epoch: 17
Loss: 0.17237073731986563
train: 0.869663	val: 0.758325	test: 0.729156

Epoch: 18
Loss: 0.17202235103067817
train: 0.876717	val: 0.771842	test: 0.737713

Epoch: 19
Loss: 0.17012494762564756
train: 0.883411	val: 0.780000	test: 0.743713

Epoch: 20
Loss: 0.16839172447532577
train: 0.887257	val: 0.781750	test: 0.733182

Epoch: 21
Loss: 0.1680806870453417
train: 0.886354	val: 0.776807	test: 0.740931

Epoch: 22
Loss: 0.16707198552030333
train: 0.883611	val: 0.785891	test: 0.736509

Epoch: 23
Loss: 0.16665940729468925
train: 0.886410	val: 0.784422	test: 0.740481

Epoch: 24
Loss: 0.16672931644995878
train: 0.888100	val: 0.781911	test: 0.749901

Epoch: 25
Loss: 0.1652211429088843
train: 0.893128	val: 0.777159	test: 0.747052

Epoch: 26
Loss: 0.16404956714478666
train: 0.895660	val: 0.787765	test: 0.748798

Epoch: 27
Loss: 0.1624917637387919
train: 0.898756	val: 0.784365	test: 0.747309

Epoch: 28
Loss: 0.16234007105477632
train: 0.899083	val: 0.782181	test: 0.739371

Epoch: 29
Loss: 0.16230113614975927
train: 0.900533	val: 0.790606	test: 0.749224

Epoch: 30
Loss: 0.16057611656515935
train: 0.904410	val: 0.790685	test: 0.740148

Epoch: 31
Loss: 0.15902198585711094
train: 0.905276	val: 0.785608	test: 0.750344

Epoch: 32
Loss: 0.1589440176445503
train: 0.907021	val: 0.784155	test: 0.736017

Epoch: 33
Loss: 0.15813200280148984
train: 0.907695	val: 0.785251	test: 0.739601

Epoch: 34
Loss: 0.15806917767236997
train: 0.909721	val: 0.778716	test: 0.742036

Epoch: 35
Loss: 0.15748892687455407
train: 0.911362	val: 0.779510	test: 0.738007

Epoch: 36
Loss: 0.15773157732364823
train: 0.909599	val: 0.780243	test: 0.743239

Epoch: 37
Loss: 0.15618852522658144
train: 0.913090	val: 0.782306	test: 0.745070

Epoch: 38
Loss: 0.15538899921200122
train: 0.913849	val: 0.787565	test: 0.745329

Epoch: 39
Loss: 0.15358914951177455
train: 0.915816	val: 0.787413	test: 0.740588

Epoch: 40
Loss: 0.15534704021203052
train: 0.918747	val: 0.789757	test: 0.739552

Epoch: 41
Loss: 0.15331794013479055
train: 0.920456	val: 0.796547	test: 0.747057

Epoch: 42
Loss: 0.15194969365987457
train: 0.921231	val: 0.793763	test: 0.742837

Epoch: 43
Loss: 0.1509669869195078
train: 0.921436	val: 0.789841	test: 0.748699

Epoch: 44
Loss: 0.14937920215946218
train: 0.921063	val: 0.788631	test: 0.742186

Epoch: 45
Loss: 0.14824875357798864
train: 0.923842	val: 0.789516	test: 0.743543

Epoch: 46
Loss: 0.1488789335077104
train: 0.925345	val: 0.783958	test: 0.750588

Epoch: 47
Loss: 0.14897141317769466
train: 0.928254	val: 0.783344	test: 0.741273

Epoch: 48
Loss: 0.14618534201749764
train: 0.926178	val: 0.790181	test: 0.736938

Epoch: 49
Loss: 0.14641368304694224
train: 0.930210	val: 0.793110	test: 0.742134

Epoch: 50
Loss: 0.14608954742393002
train: 0.930104	val: 0.793420	test: 0.749329

Epoch: 51
Loss: 0.14508657003177242
train: 0.932087	val: 0.792473	test: 0.756458

Epoch: 52
Loss: 0.14333614795610405
train: 0.931699	val: 0.791804	test: 0.746373

Epoch: 53
Loss: 0.1436536759921281
train: 0.934050	val: 0.795639	test: 0.740372

Epoch: 54
Loss: 0.14266451267705366
train: 0.932217	val: 0.779369	test: 0.741021

Epoch: 55
Loss: 0.14324943466850243
train: 0.935354	val: 0.785660	test: 0.751327

Epoch: 56
Loss: 0.14309884099720732
train: 0.939186	val: 0.795803	test: 0.740919

Epoch: 57
Loss: 0.14018798018424583
train: 0.937508	val: 0.793168	test: 0.743627

Epoch: 58
Loss: 0.14113255912996736
train: 0.938211	val: 0.794145	test: 0.751777

Epoch: 59
Loss: 0.1404627636386151
train: 0.940638	val: 0.780362	test: 0.736318

Epoch: 60
Loss: 0.13907788607789143
train: 0.942245	val: 0.791389	test: 0.736460

Epoch: 61
Loss: 0.13876254463704416
train: 0.942081	val: 0.779308	test: 0.735254

Epoch: 62
Loss: 0.1404147104852005
train: 0.941016	val: 0.779624	test: 0.734351

Epoch: 63
Loss: 0.13851207980536662
train: 0.943390	val: 0.779277	test: 0.742150

Epoch: 64
Loss: 0.13803913416419622
train: 0.943325	val: 0.794453	test: 0.741137

Epoch: 65
Loss: 0.13466446218148242
train: 0.947362	val: 0.785603	test: 0.737601

Epoch: 66
Loss: 0.13473043765220452
train: 0.945350	val: 0.783492	test: 0.745460

Epoch: 67
Loss: 0.13373463350013418
train: 0.949232	val: 0.781087	test: 0.737824

Epoch: 68
Loss: 0.13391583809519889
train: 0.948192	val: 0.782244	test: 0.734332

Epoch: 69
Loss: 0.13556759389656922
train: 0.949225	val: 0.786649	test: 0.741227

Epoch: 70
Loss: 0.1337318970975392
train: 0.951509	val: 0.791255	test: 0.745849

Epoch: 71
Loss: 0.13351477489210495
train: 0.952195	val: 0.786723	test: 0.737290

Epoch: 72
Loss: 0.13243291352789663
train: 0.951442	val: 0.781147	test: 0.743426

Epoch: 73
Loss: 0.13184109831535074
train: 0.954391	val: 0.786911	test: 0.740956

Epoch: 74
Loss: 0.13065378826382212
train: 0.952836	val: 0.776521	test: 0.730702

Epoch: 75
Loss: 0.12984694391582752
train: 0.954925	val: 0.781614	test: 0.738282

Epoch: 76
Loss: 0.13006373517698522
train: 0.954732	val: 0.794558	test: 0.743919

Epoch: 77
Loss: 0.12950612411757523
train: 0.954903	val: 0.773546	test: 0.727994

Epoch: 78
Loss: 0.12887738005241234
train: 0.956971	val: 0.787493	test: 0.738672

Epoch: 79
Loss: 0.12808372548804325
train: 0.958392	val: 0.793651	test: 0.745411

Epoch: 80
Loss: 0.12778619065822924
train: 0.958582	val: 0.792738	test: 0.744304

Epoch: 81
Loss: 0.12537363578036
train: 0.959258	val: 0.784166	test: 0.739331

Epoch: 82
Loss: 0.1261063235278944
train: 0.959353	val: 0.782546	test: 0.737545

Epoch: 83
Loss: 0.12634087188572432
train: 0.960759	val: 0.780827	test: 0.741468

Epoch: 84
Loss: 0.12381505199949144
train: 0.961844	val: 0.794199	test: 0.739266

Epoch: 85
Loss: 0.1241111229886026
train: 0.961208	val: 0.786859	test: 0.731998

Epoch: 86
Loss: 0.1260922947392644
train: 0.963316	val: 0.794315	test: 0.744083

Epoch: 87
Loss: 0.12255557980705413
train: 0.963895	val: 0.779471	test: 0.738831

Epoch: 88
Loss: 0.1230912977076089
train: 0.963468	val: 0.789974	test: 0.736622

Epoch: 89
Loss: 0.12269974262262599
train: 0.964299	val: 0.775123	test: 0.736347

Epoch: 90
Loss: 0.12118721933131767
train: 0.964803	val: 0.777839	test: 0.740229

Epoch: 91
Loss: 0.12482745080834753
train: 0.963383	val: 0.770578	test: 0.741726

Epoch: 92
Loss: 0.12244125509411347
train: 0.961824	val: 0.779029	test: 0.723607

Epoch: 93
Loss: 0.11909431429776554
train: 0.967372	val: 0.769854	test: 0.731154

Epoch: 94
Loss: 0.11929809838857855
train: 0.967172	val: 0.769796	test: 0.746840

Epoch: 95
Loss: 0.11986618675914511
train: 0.967193	val: 0.777188	test: 0.736485

Epoch: 96
Loss: 0.11776879874663937
train: 0.968326	val: 0.784351	test: 0.739134

Epoch: 97
Loss: 0.11910882252910039
train: 0.968238	val: 0.785406	test: 0.740071

Epoch: 98
Loss: 0.11843286217428389
train: 0.968659	val: 0.782244	test: 0.737886

Epoch: 99
Loss: 0.11554154601589614
train: 0.969785	val: 0.780034	test: 0.734564

Epoch: 100
Loss: 0.11783387498330489
train: 0.969770	val: 0.788846	test: 0.743197

best train: 0.920456	val: 0.796547	test: 0.747057
end
