11501686_0
--dataset=hiv --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2740680812477641
train: 0.754600	val: 0.707892	test: 0.769445

Epoch: 2
Loss: 0.14040157149531549
train: 0.762941	val: 0.727470	test: 0.690140

Epoch: 3
Loss: 0.13464315323923443
train: 0.798191	val: 0.772214	test: 0.730827

Epoch: 4
Loss: 0.13027078681697973
train: 0.801074	val: 0.739776	test: 0.744897

Epoch: 5
Loss: 0.12860652572599485
train: 0.815579	val: 0.740989	test: 0.750188

Epoch: 6
Loss: 0.12656239542030573
train: 0.816878	val: 0.772303	test: 0.739222

Epoch: 7
Loss: 0.12708689952908872
train: 0.826138	val: 0.780503	test: 0.761040

Epoch: 8
Loss: 0.12400505835638737
train: 0.823987	val: 0.782285	test: 0.756339

Epoch: 9
Loss: 0.12376505380543927
train: 0.839837	val: 0.793204	test: 0.765355

Epoch: 10
Loss: 0.12167025555380188
train: 0.840024	val: 0.750897	test: 0.742301

Epoch: 11
Loss: 0.11994923722977013
train: 0.851563	val: 0.776437	test: 0.767836

Epoch: 12
Loss: 0.11814283228606566
train: 0.859910	val: 0.785956	test: 0.769818

Epoch: 13
Loss: 0.11696219495145713
train: 0.858013	val: 0.757750	test: 0.754725

Epoch: 14
Loss: 0.11541297925661008
train: 0.866312	val: 0.777057	test: 0.765521

Epoch: 15
Loss: 0.11456374473863251
train: 0.861504	val: 0.804977	test: 0.758143

Epoch: 16
Loss: 0.11402668731499707
train: 0.867659	val: 0.816658	test: 0.774134

Epoch: 17
Loss: 0.11368418035731312
train: 0.874385	val: 0.770790	test: 0.765951

Epoch: 18
Loss: 0.1127667229727728
train: 0.877516	val: 0.801716	test: 0.756550

Epoch: 19
Loss: 0.11137200432003916
train: 0.870871	val: 0.777374	test: 0.743871

Epoch: 20
Loss: 0.11102024125922222
train: 0.878134	val: 0.821251	test: 0.769702

Epoch: 21
Loss: 0.11122683002915042
train: 0.883896	val: 0.809515	test: 0.759638

Epoch: 22
Loss: 0.10989606303141745
train: 0.887626	val: 0.796688	test: 0.764704

Epoch: 23
Loss: 0.10867033741710169
train: 0.886392	val: 0.761908	test: 0.769339

Epoch: 24
Loss: 0.10975577516275087
train: 0.892587	val: 0.807635	test: 0.767375

Epoch: 25
Loss: 0.10845673418101101
train: 0.897332	val: 0.804567	test: 0.771452

Epoch: 26
Loss: 0.10561885872021994
train: 0.895121	val: 0.819545	test: 0.767321

Epoch: 27
Loss: 0.10786837444950002
train: 0.898244	val: 0.782983	test: 0.771672

Epoch: 28
Loss: 0.10547789072014815
train: 0.903911	val: 0.800699	test: 0.770185

Epoch: 29
Loss: 0.10637429147674655
train: 0.904446	val: 0.789288	test: 0.760438

Epoch: 30
Loss: 0.10624612346511456
train: 0.903208	val: 0.806049	test: 0.756658

Epoch: 31
Loss: 0.10340227386807543
train: 0.902606	val: 0.806878	test: 0.765729

Epoch: 32
Loss: 0.10493314840416194
train: 0.905973	val: 0.806710	test: 0.765364

Epoch: 33
Loss: 0.10263875533226445
train: 0.900589	val: 0.793348	test: 0.748533

Epoch: 34
Loss: 0.10325817573561091
train: 0.912942	val: 0.808219	test: 0.764980

Epoch: 35
Loss: 0.10086874055784963
train: 0.915375	val: 0.814928	test: 0.766413

Epoch: 36
Loss: 0.10424522373754987
train: 0.913266	val: 0.817267	test: 0.760903

Epoch: 37
Loss: 0.10094458489191728
train: 0.910916	val: 0.807837	test: 0.764688

Epoch: 38
Loss: 0.10206111980615126
train: 0.910766	val: 0.793568	test: 0.742687

Epoch: 39
Loss: 0.10218106253938068
train: 0.923466	val: 0.814028	test: 0.759130

Epoch: 40
Loss: 0.09996140601395913
train: 0.921619	val: 0.784496	test: 0.763292

Epoch: 41
Loss: 0.099677593218178
train: 0.919959	val: 0.814389	test: 0.747996

Epoch: 42
Loss: 0.09943002595342652
train: 0.924826	val: 0.818330	test: 0.752237

Epoch: 43
Loss: 0.09949146729151369
train: 0.922096	val: 0.818113	test: 0.760696

Epoch: 44
Loss: 0.09738509286253663
train: 0.932705	val: 0.800754	test: 0.757143

Epoch: 45
Loss: 0.09908782321162596
train: 0.929314	val: 0.802954	test: 0.746154

Epoch: 46
Loss: 0.09805942495599591
train: 0.921135	val: 0.806346	test: 0.763730

Epoch: 47
Loss: 0.09768767722189789
train: 0.930553	val: 0.800834	test: 0.751231

Epoch: 48
Loss: 0.09637397122804177
train: 0.930444	val: 0.785993	test: 0.742697

Epoch: 49
Loss: 0.09706124844749553
train: 0.926920	val: 0.804334	test: 0.740582

Epoch: 50
Loss: 0.09671798678138344
train: 0.936749	val: 0.788142	test: 0.752577

Epoch: 51
Loss: 0.0960384430357184
train: 0.935501	val: 0.793562	test: 0.738419

Epoch: 52
Loss: 0.09504038689702198
train: 0.935455	val: 0.797622	test: 0.757670

Epoch: 53
Loss: 0.09460544088114725
train: 0.940494	val: 0.790546	test: 0.744997

Epoch: 54
Loss: 0.0940077199293114
train: 0.939959	val: 0.770941	test: 0.755438

Epoch: 55
Loss: 0.09299749170585532
train: 0.941603	val: 0.776198	test: 0.753831

Epoch: 56
Loss: 0.09267139462809215
train: 0.942319	val: 0.812702	test: 0.743406

Epoch: 57
Loss: 0.09213677416439901
train: 0.943570	val: 0.797313	test: 0.742902

Epoch: 58
Loss: 0.0914004292211899
train: 0.945786	val: 0.786443	test: 0.732453

Epoch: 59
Loss: 0.09156975175035999
train: 0.945642	val: 0.795764	test: 0.737266

Epoch: 60
Loss: 0.09171914091082081
train: 0.943600	val: 0.785264	test: 0.749452

Epoch: 61
Loss: 0.09107340830835332
train: 0.950169	val: 0.789456	test: 0.728685

Epoch: 62
Loss: 0.09123587447882818
train: 0.930605	val: 0.782120	test: 0.729999

Epoch: 63
Loss: 0.09092336868843794
train: 0.949343	val: 0.796624	test: 0.753628

Epoch: 64
Loss: 0.09041799832334431
train: 0.951657	val: 0.806747	test: 0.740630

Epoch: 65
Loss: 0.08864466091163269
train: 0.951913	val: 0.776884	test: 0.738305

Epoch: 66
Loss: 0.08890900703243663
train: 0.944797	val: 0.782055	test: 0.722685

Epoch: 67
Loss: 0.0886767467437175
train: 0.948715	val: 0.800698	test: 0.735970

Epoch: 68
Loss: 0.08930504105706694
train: 0.956427	val: 0.782297	test: 0.734421

Epoch: 69
Loss: 0.08891269859125096
train: 0.957582	val: 0.798911	test: 0.739518

Epoch: 70
Loss: 0.08708001371730122
train: 0.951970	val: 0.773454	test: 0.741078

Epoch: 71
Loss: 0.08759947434199403
train: 0.956245	val: 0.780420	test: 0.739827

Epoch: 72
Loss: 0.08813952686448008
train: 0.957823	val: 0.802279	test: 0.740101

Epoch: 73
Loss: 0.08836609733759562
train: 0.956240	val: 0.791572	test: 0.741639

Epoch: 74
Loss: 0.08708696871500128
train: 0.957781	val: 0.791339	test: 0.736878

Epoch: 75
Loss: 0.08630898221069262
train: 0.958249	val: 0.780962	test: 0.750316

Epoch: 76
Loss: 0.08544491845174489
train: 0.960711	val: 0.787025	test: 0.742656

Epoch: 77
Loss: 0.08627000154210379
train: 0.959050	val: 0.796361	test: 0.743118

Epoch: 78
Loss: 0.08537618910963729
train: 0.961617	val: 0.811961	test: 0.745982

Epoch: 79
Loss: 0.08379260296346579
train: 0.956463	val: 0.773583	test: 0.748199

Epoch: 80
Loss: 0.08543934730493064
train: 0.962611	val: 0.792160	test: 0.739253

Epoch: 81
Loss: 0.08462482097982528
train: 0.962936	val: 0.779593	test: 0.745337

Epoch: 82
Loss: 0.08494031920743314
train: 0.964776	val: 0.776516	test: 0.737990

Epoch: 83
Loss: 0.08239611443732678
train: 0.966893	val: 0.782827	test: 0.748732

Epoch: 84
Loss: 0.0831533621967582
train: 0.966356	val: 0.776388	test: 0.746538

Epoch: 85
Loss: 0.08220221990841341
train: 0.965273	val: 0.788133	test: 0.753414

Epoch: 86
Loss: 0.08231553335364175
train: 0.960232	val: 0.806618	test: 0.741076

Epoch: 87
Loss: 0.0815189776903779
train: 0.968305	val: 0.810641	test: 0.743176

Epoch: 88
Loss: 0.08155887741469156
train: 0.969827	val: 0.779465	test: 0.747388

Epoch: 89
Loss: 0.08010342559812766
train: 0.969752	val: 0.805611	test: 0.735731

Epoch: 90
Loss: 0.08192256935452831
train: 0.964997	val: 0.804493	test: 0.762382

Epoch: 91
Loss: 0.08138405770004369
train: 0.969294	val: 0.783749	test: 0.735034

Epoch: 92
Loss: 0.07991780403411744
train: 0.971457	val: 0.800338	test: 0.748047

Epoch: 93
Loss: 0.0797003607560616
train: 0.971370	val: 0.776755	test: 0.741944

Epoch: 94
Loss: 0.07949292632532434
train: 0.972225	val: 0.791835	test: 0.736241

Epoch: 95
Loss: 0.0792304451224846
train: 0.974303	val: 0.789223	test: 0.745221

Epoch: 96
Loss: 0.07952169047965807
train: 0.974519	val: 0.793813	test: 0.735379

Epoch: 97
Loss: 0.07727977999222418
train: 0.972873	val: 0.795418	test: 0.744472

Epoch: 98
Loss: 0.07923463067840149
train: 0.973569	val: 0.768292	test: 0.747006

Epoch: 99
Loss: 0.07922332508480699
train: 0.970724	val: 0.759636	test: 0.742457

Epoch: 100
Loss: 0.07788988994314065
train: 0.973999	val: 0.783363	test: 0.747703

best train: 0.878134	val: 0.821251	test: 0.769702
end
