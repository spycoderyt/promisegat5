11501686_0
--dataset=bbbp --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6179010454258747
train: 0.687391	val: 0.822343	test: 0.569444

Epoch: 2
Loss: 0.5203231431512695
train: 0.793632	val: 0.897420	test: 0.610629

Epoch: 3
Loss: 0.4281622861005987
train: 0.853585	val: 0.911372	test: 0.616416

Epoch: 4
Loss: 0.3652353121928217
train: 0.883453	val: 0.905450	test: 0.640529

Epoch: 5
Loss: 0.3186314661338671
train: 0.900052	val: 0.911774	test: 0.656154

Epoch: 6
Loss: 0.28981907654455846
train: 0.914021	val: 0.911774	test: 0.681424

Epoch: 7
Loss: 0.2751950994753187
train: 0.917731	val: 0.925324	test: 0.678048

Epoch: 8
Loss: 0.26415718668097543
train: 0.923804	val: 0.925524	test: 0.684317

Epoch: 9
Loss: 0.26490259811572253
train: 0.928134	val: 0.926629	test: 0.688561

Epoch: 10
Loss: 0.26681414964245226
train: 0.927282	val: 0.909164	test: 0.695409

Epoch: 11
Loss: 0.24850522626044522
train: 0.938491	val: 0.916893	test: 0.699074

Epoch: 12
Loss: 0.24106658834303057
train: 0.939817	val: 0.920707	test: 0.694444

Epoch: 13
Loss: 0.2387304973104978
train: 0.941466	val: 0.917796	test: 0.686150

Epoch: 14
Loss: 0.2384760076409055
train: 0.946528	val: 0.924922	test: 0.697434

Epoch: 15
Loss: 0.2284582740958078
train: 0.951860	val: 0.925223	test: 0.707948

Epoch: 16
Loss: 0.2216111819158443
train: 0.953260	val: 0.912175	test: 0.720004

Epoch: 17
Loss: 0.2068484209076759
train: 0.951773	val: 0.919000	test: 0.714313

Epoch: 18
Loss: 0.21843764497861123
train: 0.955315	val: 0.907558	test: 0.709684

Epoch: 19
Loss: 0.2072914967960998
train: 0.949174	val: 0.908361	test: 0.682677

Epoch: 20
Loss: 0.21860754988248882
train: 0.957354	val: 0.913480	test: 0.715085

Epoch: 21
Loss: 0.20194077861048726
train: 0.958866	val: 0.920506	test: 0.711902

Epoch: 22
Loss: 0.21395564712019888
train: 0.962111	val: 0.922011	test: 0.724730

Epoch: 23
Loss: 0.19839074253922398
train: 0.962164	val: 0.911974	test: 0.735050

Epoch: 24
Loss: 0.1966645563259231
train: 0.957997	val: 0.914383	test: 0.693094

Epoch: 25
Loss: 0.21187492250492065
train: 0.965580	val: 0.913580	test: 0.719715

Epoch: 26
Loss: 0.1735404743775319
train: 0.965711	val: 0.925524	test: 0.723090

Epoch: 27
Loss: 0.1894638924533191
train: 0.970283	val: 0.935762	test: 0.728588

Epoch: 28
Loss: 0.2124587067320076
train: 0.971645	val: 0.929941	test: 0.722704

Epoch: 29
Loss: 0.18055574477166506
train: 0.969914	val: 0.909365	test: 0.713059

Epoch: 30
Loss: 0.18118339307395345
train: 0.972489	val: 0.919803	test: 0.702739

Epoch: 31
Loss: 0.1775355904926519
train: 0.970976	val: 0.925123	test: 0.706115

Epoch: 32
Loss: 0.18188457136912942
train: 0.974843	val: 0.925926	test: 0.717014

Epoch: 33
Loss: 0.17589159898245094
train: 0.975648	val: 0.934658	test: 0.727720

Epoch: 34
Loss: 0.17201529730160886
train: 0.978331	val: 0.924922	test: 0.722222

Epoch: 35
Loss: 0.1722653773028406
train: 0.975317	val: 0.920807	test: 0.729842

Epoch: 36
Loss: 0.17229054408151281
train: 0.979448	val: 0.919101	test: 0.727816

Epoch: 37
Loss: 0.16521588689658745
train: 0.980369	val: 0.921409	test: 0.726080

Epoch: 38
Loss: 0.1758834794176028
train: 0.980357	val: 0.929238	test: 0.720679

Epoch: 39
Loss: 0.1679578815191359
train: 0.979313	val: 0.907156	test: 0.728395

Epoch: 40
Loss: 0.16825097790943216
train: 0.980207	val: 0.927431	test: 0.711998

Epoch: 41
Loss: 0.1713519963333366
train: 0.981358	val: 0.926930	test: 0.706501

Epoch: 42
Loss: 0.15931482482869883
train: 0.982992	val: 0.916290	test: 0.711227

Epoch: 43
Loss: 0.16347362983719685
train: 0.984297	val: 0.913781	test: 0.724633

Epoch: 44
Loss: 0.15253812315035978
train: 0.983911	val: 0.919803	test: 0.702257

Epoch: 45
Loss: 0.14528932192885874
train: 0.983561	val: 0.906354	test: 0.704186

Epoch: 46
Loss: 0.15111075841873964
train: 0.984049	val: 0.915889	test: 0.710745

Epoch: 47
Loss: 0.15175640838419224
train: 0.983138	val: 0.926829	test: 0.714506

Epoch: 48
Loss: 0.15735518407469748
train: 0.983838	val: 0.919201	test: 0.717689

Epoch: 49
Loss: 0.1473294940627044
train: 0.986419	val: 0.916190	test: 0.725019

Epoch: 50
Loss: 0.1401139720523234
train: 0.987909	val: 0.909766	test: 0.726080

Epoch: 51
Loss: 0.14300557377164275
train: 0.989644	val: 0.915588	test: 0.725405

Epoch: 52
Loss: 0.13355327444019108
train: 0.986595	val: 0.912175	test: 0.718557

Epoch: 53
Loss: 0.15063890098431143
train: 0.989432	val: 0.913781	test: 0.728588

Epoch: 54
Loss: 0.14901908795829913
train: 0.988842	val: 0.911573	test: 0.708623

Epoch: 55
Loss: 0.14310866095487942
train: 0.990313	val: 0.909967	test: 0.701871

Epoch: 56
Loss: 0.14085265817980563
train: 0.990782	val: 0.913881	test: 0.708623

Epoch: 57
Loss: 0.1273126603599958
train: 0.991897	val: 0.908461	test: 0.724923

Epoch: 58
Loss: 0.13649071693313078
train: 0.991578	val: 0.911673	test: 0.724633

Epoch: 59
Loss: 0.1554115568485171
train: 0.989746	val: 0.921208	test: 0.709587

Epoch: 60
Loss: 0.12512214909967798
train: 0.990352	val: 0.903242	test: 0.706597

Epoch: 61
Loss: 0.13378581920601645
train: 0.991754	val: 0.905149	test: 0.705343

Epoch: 62
Loss: 0.1301311276529091
train: 0.990190	val: 0.919301	test: 0.707272

Epoch: 63
Loss: 0.12148214107566511
train: 0.991820	val: 0.913681	test: 0.705343

Epoch: 64
Loss: 0.12124548773437561
train: 0.992887	val: 0.913781	test: 0.719811

Epoch: 65
Loss: 0.13328122729145458
train: 0.993510	val: 0.910168	test: 0.720486

Epoch: 66
Loss: 0.1292305975208814
train: 0.991929	val: 0.914484	test: 0.712481

Epoch: 67
Loss: 0.1393795250189823
train: 0.993574	val: 0.913681	test: 0.717496

Epoch: 68
Loss: 0.11098129857048579
train: 0.994682	val: 0.905450	test: 0.719329

Epoch: 69
Loss: 0.12814468247188557
train: 0.992978	val: 0.898525	test: 0.719522

Epoch: 70
Loss: 0.11751535317668148
train: 0.993358	val: 0.910469	test: 0.711516

Epoch: 71
Loss: 0.12389019068984215
train: 0.994942	val: 0.908863	test: 0.702064

Epoch: 72
Loss: 0.1070952030084708
train: 0.995687	val: 0.889792	test: 0.702739

Epoch: 73
Loss: 0.12692558490266448
train: 0.994541	val: 0.887986	test: 0.711902

Epoch: 74
Loss: 0.12487977445724956
train: 0.993590	val: 0.907156	test: 0.725598

Epoch: 75
Loss: 0.12028593888679022
train: 0.995304	val: 0.903643	test: 0.720197

Epoch: 76
Loss: 0.12359180646724231
train: 0.994242	val: 0.911774	test: 0.711902

Epoch: 77
Loss: 0.11339688846035252
train: 0.995771	val: 0.909264	test: 0.711709

Epoch: 78
Loss: 0.11364900462391316
train: 0.996185	val: 0.896015	test: 0.722126

Epoch: 79
Loss: 0.1217795431232563
train: 0.994215	val: 0.912577	test: 0.720390

Epoch: 80
Loss: 0.11484937491911111
train: 0.995642	val: 0.894911	test: 0.711998

Epoch: 81
Loss: 0.10241889244316846
train: 0.995450	val: 0.896116	test: 0.703414

Epoch: 82
Loss: 0.09910538661230588
train: 0.995941	val: 0.894610	test: 0.694444

Epoch: 83
Loss: 0.10407914453718928
train: 0.995179	val: 0.893606	test: 0.697531

Epoch: 84
Loss: 0.11076111191126815
train: 0.996397	val: 0.906755	test: 0.727913

Epoch: 85
Loss: 0.10416087507710392
train: 0.996190	val: 0.906454	test: 0.737558

Epoch: 86
Loss: 0.10739259752349192
train: 0.995619	val: 0.907257	test: 0.711613

Epoch: 87
Loss: 0.11032807949661148
train: 0.994586	val: 0.902138	test: 0.701389

Epoch: 88
Loss: 0.10402132708311893
train: 0.995812	val: 0.901235	test: 0.726177

Epoch: 89
Loss: 0.10783436510885579
train: 0.996047	val: 0.891599	test: 0.725791

Epoch: 90
Loss: 0.10938707349262913
train: 0.996758	val: 0.900632	test: 0.699267

Epoch: 91
Loss: 0.10061390017806494
train: 0.997357	val: 0.893807	test: 0.706983

Epoch: 92
Loss: 0.0950803885330875
train: 0.996852	val: 0.894911	test: 0.704379

Epoch: 93
Loss: 0.11899259452601528
train: 0.997028	val: 0.907558	test: 0.715567

Epoch: 94
Loss: 0.09903706341349545
train: 0.997234	val: 0.902138	test: 0.721740

Epoch: 95
Loss: 0.10859026175686813
train: 0.997348	val: 0.890194	test: 0.703029

Epoch: 96
Loss: 0.09244617569602857
train: 0.997959	val: 0.880458	test: 0.702257

Epoch: 97
Loss: 0.08956941351897717
train: 0.997798	val: 0.876644	test: 0.700039

Epoch: 98
Loss: 0.10208445231010514
train: 0.997457	val: 0.893004	test: 0.719811

Epoch: 99
Loss: 0.08736393582112015
train: 0.995305	val: 0.905149	test: 0.722801

Epoch: 100
Loss: 0.10030111766185189
train: 0.996838	val: 0.899629	test: 0.719329

best train: 0.970283	val: 0.935762	test: 0.728588
end
