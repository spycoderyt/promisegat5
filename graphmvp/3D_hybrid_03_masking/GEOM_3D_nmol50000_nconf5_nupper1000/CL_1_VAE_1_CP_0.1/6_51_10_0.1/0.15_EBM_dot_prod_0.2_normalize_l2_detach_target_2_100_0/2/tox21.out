14371622_2
--dataset=tox21 --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', PCQM4M_sample_size=200000, SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5332281476062054
train: 0.705501	val: 0.622963	test: 0.593512

Epoch: 2
Loss: 0.3358128012585408
train: 0.754165	val: 0.710011	test: 0.664617

Epoch: 3
Loss: 0.24202759296125428
train: 0.775465	val: 0.702543	test: 0.666207

Epoch: 4
Loss: 0.21036707118699577
train: 0.802943	val: 0.751579	test: 0.714298

Epoch: 5
Loss: 0.20028439771764986
train: 0.824821	val: 0.762723	test: 0.717103

Epoch: 6
Loss: 0.19535357325722746
train: 0.835618	val: 0.757490	test: 0.720726

Epoch: 7
Loss: 0.19034661841955233
train: 0.831819	val: 0.745095	test: 0.708421

Epoch: 8
Loss: 0.18776803267038084
train: 0.847546	val: 0.763242	test: 0.721123

Epoch: 9
Loss: 0.18581470699408376
train: 0.853417	val: 0.772083	test: 0.731131

Epoch: 10
Loss: 0.18352844444707508
train: 0.860233	val: 0.766842	test: 0.733841

Epoch: 11
Loss: 0.18196660809617488
train: 0.861535	val: 0.767292	test: 0.729899

Epoch: 12
Loss: 0.18114440643589041
train: 0.866457	val: 0.764177	test: 0.735264

Epoch: 13
Loss: 0.1780652809682925
train: 0.866905	val: 0.759172	test: 0.722165

Epoch: 14
Loss: 0.17580409036963207
train: 0.870745	val: 0.769531	test: 0.742275

Epoch: 15
Loss: 0.17461086234505216
train: 0.874145	val: 0.775514	test: 0.736714

Epoch: 16
Loss: 0.1746444863954282
train: 0.875383	val: 0.771726	test: 0.732437

Epoch: 17
Loss: 0.17288864764549827
train: 0.875757	val: 0.767561	test: 0.735321

Epoch: 18
Loss: 0.1706168419089292
train: 0.882435	val: 0.775285	test: 0.741082

Epoch: 19
Loss: 0.1696998174430164
train: 0.882589	val: 0.785062	test: 0.742253

Epoch: 20
Loss: 0.16961261779922432
train: 0.886920	val: 0.780017	test: 0.739970

Epoch: 21
Loss: 0.16906958915171943
train: 0.888206	val: 0.781849	test: 0.745241

Epoch: 22
Loss: 0.1680656569084298
train: 0.890769	val: 0.783401	test: 0.742758

Epoch: 23
Loss: 0.16453992061215345
train: 0.893905	val: 0.774056	test: 0.744081

Epoch: 24
Loss: 0.16426155247525562
train: 0.897738	val: 0.782100	test: 0.742272

Epoch: 25
Loss: 0.16330601939680509
train: 0.899027	val: 0.782596	test: 0.743948

Epoch: 26
Loss: 0.16306753371589433
train: 0.901041	val: 0.781493	test: 0.743818

Epoch: 27
Loss: 0.16348282052903898
train: 0.899346	val: 0.784338	test: 0.751080

Epoch: 28
Loss: 0.16080184724846983
train: 0.900635	val: 0.782762	test: 0.743326

Epoch: 29
Loss: 0.1612863049095547
train: 0.900429	val: 0.789917	test: 0.747112

Epoch: 30
Loss: 0.16010686361365706
train: 0.903985	val: 0.779237	test: 0.739276

Epoch: 31
Loss: 0.15915765562412001
train: 0.906904	val: 0.778184	test: 0.741311

Epoch: 32
Loss: 0.15774540357324113
train: 0.909567	val: 0.779541	test: 0.746252

Epoch: 33
Loss: 0.1580391359459805
train: 0.906591	val: 0.783638	test: 0.742554

Epoch: 34
Loss: 0.1575274884670057
train: 0.912353	val: 0.790400	test: 0.743603

Epoch: 35
Loss: 0.15451604431340688
train: 0.914195	val: 0.784806	test: 0.743435

Epoch: 36
Loss: 0.15514971751495643
train: 0.914377	val: 0.789095	test: 0.747376

Epoch: 37
Loss: 0.155136144928781
train: 0.917470	val: 0.791934	test: 0.753509

Epoch: 38
Loss: 0.15322656337600107
train: 0.919730	val: 0.790359	test: 0.745271

Epoch: 39
Loss: 0.15384964594733752
train: 0.915989	val: 0.786634	test: 0.745360

Epoch: 40
Loss: 0.15449629989338262
train: 0.920858	val: 0.787606	test: 0.746612

Epoch: 41
Loss: 0.1508725758109007
train: 0.922427	val: 0.794429	test: 0.745359

Epoch: 42
Loss: 0.15050875542640776
train: 0.924666	val: 0.782794	test: 0.744158

Epoch: 43
Loss: 0.14957675773033294
train: 0.926945	val: 0.794260	test: 0.743228

Epoch: 44
Loss: 0.14837915446423042
train: 0.928199	val: 0.790640	test: 0.754163

Epoch: 45
Loss: 0.14731781222398393
train: 0.926887	val: 0.790339	test: 0.741593

Epoch: 46
Loss: 0.14737663274308702
train: 0.928493	val: 0.798087	test: 0.749012

Epoch: 47
Loss: 0.1454310699158148
train: 0.930842	val: 0.791675	test: 0.746755

Epoch: 48
Loss: 0.1476719227637514
train: 0.932338	val: 0.795333	test: 0.752374

Epoch: 49
Loss: 0.14416254941930234
train: 0.934746	val: 0.797076	test: 0.743810

Epoch: 50
Loss: 0.14459645541287416
train: 0.934217	val: 0.802203	test: 0.745202

Epoch: 51
Loss: 0.14345527459273946
train: 0.933514	val: 0.794753	test: 0.733823

Epoch: 52
Loss: 0.14263406560333636
train: 0.938060	val: 0.792722	test: 0.734229

Epoch: 53
Loss: 0.144220808030456
train: 0.936734	val: 0.790244	test: 0.740523

Epoch: 54
Loss: 0.14127351867507687
train: 0.937363	val: 0.786741	test: 0.735931

Epoch: 55
Loss: 0.14164928359802129
train: 0.940659	val: 0.787231	test: 0.744369

Epoch: 56
Loss: 0.14073246614071644
train: 0.941010	val: 0.793291	test: 0.747412

Epoch: 57
Loss: 0.13996524259879767
train: 0.941826	val: 0.794400	test: 0.743612

Epoch: 58
Loss: 0.14101333580486877
train: 0.940438	val: 0.783105	test: 0.741220

Epoch: 59
Loss: 0.13692254621327649
train: 0.944735	val: 0.788021	test: 0.734665

Epoch: 60
Loss: 0.13766179045083748
train: 0.943921	val: 0.792644	test: 0.743181

Epoch: 61
Loss: 0.13861581707050696
train: 0.945578	val: 0.789641	test: 0.734735

Epoch: 62
Loss: 0.13599936492850662
train: 0.946593	val: 0.797997	test: 0.749942

Epoch: 63
Loss: 0.1353190753533377
train: 0.947537	val: 0.793228	test: 0.745430

Epoch: 64
Loss: 0.1351884731096251
train: 0.948474	val: 0.794421	test: 0.738887

Epoch: 65
Loss: 0.13462089945482467
train: 0.949101	val: 0.794093	test: 0.741548

Epoch: 66
Loss: 0.1363123155568581
train: 0.949063	val: 0.793337	test: 0.740880

Epoch: 67
Loss: 0.13128924296929612
train: 0.952236	val: 0.795836	test: 0.736977

Epoch: 68
Loss: 0.13335496995874213
train: 0.949378	val: 0.798054	test: 0.746028

Epoch: 69
Loss: 0.13109783731806568
train: 0.954325	val: 0.796140	test: 0.740922

Epoch: 70
Loss: 0.13162723022131353
train: 0.953508	val: 0.792631	test: 0.745799

Epoch: 71
Loss: 0.1315772123011353
train: 0.956128	val: 0.796413	test: 0.732951

Epoch: 72
Loss: 0.12929859931045987
train: 0.953134	val: 0.806198	test: 0.741712

Epoch: 73
Loss: 0.1330199778199291
train: 0.957171	val: 0.796174	test: 0.731565

Epoch: 74
Loss: 0.1305901976529972
train: 0.955545	val: 0.788430	test: 0.733520

Epoch: 75
Loss: 0.12888213957111078
train: 0.956518	val: 0.793255	test: 0.741642

Epoch: 76
Loss: 0.12641577350004218
train: 0.956458	val: 0.786040	test: 0.735966

Epoch: 77
Loss: 0.12757996305615527
train: 0.958650	val: 0.791182	test: 0.734339

Epoch: 78
Loss: 0.12686561820822015
train: 0.958895	val: 0.790488	test: 0.725884

Epoch: 79
Loss: 0.12634187434498798
train: 0.960880	val: 0.786526	test: 0.736943

Epoch: 80
Loss: 0.12602934830954993
train: 0.961697	val: 0.792070	test: 0.739334

Epoch: 81
Loss: 0.12597865272827247
train: 0.960263	val: 0.785486	test: 0.739461

Epoch: 82
Loss: 0.12486218759322468
train: 0.962693	val: 0.796035	test: 0.741557

Epoch: 83
Loss: 0.12664046296967008
train: 0.962451	val: 0.788843	test: 0.737326

Epoch: 84
Loss: 0.12482541469622208
train: 0.963331	val: 0.788244	test: 0.745231

Epoch: 85
Loss: 0.12159971827085642
train: 0.965051	val: 0.790401	test: 0.736848

Epoch: 86
Loss: 0.12080830026389978
train: 0.966010	val: 0.793713	test: 0.732499

Epoch: 87
Loss: 0.12120091340912836
train: 0.967312	val: 0.793006	test: 0.729146

Epoch: 88
Loss: 0.1206053007096939
train: 0.962097	val: 0.777413	test: 0.733983

Epoch: 89
Loss: 0.11972258736762993
train: 0.967119	val: 0.777844	test: 0.729858

Epoch: 90
Loss: 0.11777677100824575
train: 0.968294	val: 0.791670	test: 0.729605

Epoch: 91
Loss: 0.11999986451933797
train: 0.965072	val: 0.767855	test: 0.725972

Epoch: 92
Loss: 0.11735643176920646
train: 0.968168	val: 0.786871	test: 0.730321

Epoch: 93
Loss: 0.11815567175885805
train: 0.969332	val: 0.784275	test: 0.730579

Epoch: 94
Loss: 0.11857631811308945
train: 0.969496	val: 0.790605	test: 0.735497

Epoch: 95
Loss: 0.11740724481759612
train: 0.971105	val: 0.784276	test: 0.733201

Epoch: 96
Loss: 0.11454373147673338
train: 0.971738	val: 0.788470	test: 0.734195

Epoch: 97
Loss: 0.11625348990713338
train: 0.969742	val: 0.792490	test: 0.730195

Epoch: 98
Loss: 0.11552148768047629
train: 0.971565	val: 0.789483	test: 0.733537

Epoch: 99
Loss: 0.11460176236062443
train: 0.971553	val: 0.787700	test: 0.735076

Epoch: 100
Loss: 0.11672967264846845
train: 0.972926	val: 0.786360	test: 0.728558

best train: 0.953134	val: 0.806198	test: 0.741712
end
