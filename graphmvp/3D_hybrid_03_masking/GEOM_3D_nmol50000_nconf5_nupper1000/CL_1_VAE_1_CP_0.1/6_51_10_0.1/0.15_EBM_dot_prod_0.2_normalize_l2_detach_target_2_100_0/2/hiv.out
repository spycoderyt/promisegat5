9242913_2
--dataset=hiv --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25374419712966806
train: 0.749324	val: 0.713416	test: 0.741548

Epoch: 2
Loss: 0.14013229881366313
train: 0.773455	val: 0.748432	test: 0.764976

Epoch: 3
Loss: 0.1341605201782467
train: 0.774783	val: 0.741019	test: 0.733433

Epoch: 4
Loss: 0.1311820418081231
train: 0.802287	val: 0.761834	test: 0.759474

Epoch: 5
Loss: 0.1290215412327599
train: 0.812730	val: 0.793819	test: 0.768248

Epoch: 6
Loss: 0.12548765435923742
train: 0.824307	val: 0.773139	test: 0.759204

Epoch: 7
Loss: 0.1241597365435439
train: 0.822564	val: 0.787567	test: 0.761029

Epoch: 8
Loss: 0.12460436212171871
train: 0.835361	val: 0.773433	test: 0.766662

Epoch: 9
Loss: 0.12132552192432612
train: 0.831262	val: 0.777202	test: 0.777421

Epoch: 10
Loss: 0.12073209053296578
train: 0.848193	val: 0.803075	test: 0.778165

Epoch: 11
Loss: 0.11871523620776629
train: 0.838994	val: 0.776124	test: 0.776286

Epoch: 12
Loss: 0.11909200068667439
train: 0.846540	val: 0.780824	test: 0.783787

Epoch: 13
Loss: 0.11674882587654954
train: 0.855392	val: 0.783252	test: 0.761297

Epoch: 14
Loss: 0.11707790973725705
train: 0.847722	val: 0.802598	test: 0.778864

Epoch: 15
Loss: 0.1163985558273551
train: 0.864084	val: 0.775365	test: 0.778995

Epoch: 16
Loss: 0.11520556488834924
train: 0.866374	val: 0.787750	test: 0.766732

Epoch: 17
Loss: 0.11392611080833807
train: 0.877372	val: 0.807482	test: 0.784924

Epoch: 18
Loss: 0.11256956321969362
train: 0.869828	val: 0.800075	test: 0.792528

Epoch: 19
Loss: 0.11265264820563205
train: 0.878766	val: 0.800182	test: 0.779497

Epoch: 20
Loss: 0.11202532385156304
train: 0.877752	val: 0.791155	test: 0.799218

Epoch: 21
Loss: 0.11039706189396863
train: 0.877186	val: 0.780573	test: 0.767110

Epoch: 22
Loss: 0.11058872755522293
train: 0.879112	val: 0.792870	test: 0.758866

Epoch: 23
Loss: 0.10961670543263508
train: 0.886370	val: 0.798418	test: 0.779540

Epoch: 24
Loss: 0.10964339436764776
train: 0.889170	val: 0.793850	test: 0.777630

Epoch: 25
Loss: 0.1081748947006588
train: 0.890825	val: 0.803008	test: 0.792478

Epoch: 26
Loss: 0.1084050370453316
train: 0.888922	val: 0.790316	test: 0.768727

Epoch: 27
Loss: 0.10627373383981276
train: 0.893165	val: 0.804986	test: 0.782553

Epoch: 28
Loss: 0.10732882497963803
train: 0.893292	val: 0.787071	test: 0.785722

Epoch: 29
Loss: 0.1049817748694979
train: 0.900154	val: 0.776299	test: 0.774353

Epoch: 30
Loss: 0.10517919482546068
train: 0.903424	val: 0.789278	test: 0.783771

Epoch: 31
Loss: 0.10563181740150562
train: 0.906311	val: 0.793387	test: 0.787120

Epoch: 32
Loss: 0.10495601812090834
train: 0.900619	val: 0.781673	test: 0.772085

Epoch: 33
Loss: 0.10404509377083235
train: 0.908885	val: 0.804876	test: 0.792916

Epoch: 34
Loss: 0.1043609642999484
train: 0.901260	val: 0.787095	test: 0.764082

Epoch: 35
Loss: 0.10445402634003813
train: 0.912278	val: 0.787187	test: 0.775851

Epoch: 36
Loss: 0.10177421940304994
train: 0.910835	val: 0.772974	test: 0.777406

Epoch: 37
Loss: 0.10214979092283177
train: 0.910145	val: 0.772168	test: 0.769186

Epoch: 38
Loss: 0.1019696130850378
train: 0.918826	val: 0.782453	test: 0.780405

Epoch: 39
Loss: 0.10026213245281407
train: 0.919463	val: 0.778614	test: 0.794629

Epoch: 40
Loss: 0.10065571696040379
train: 0.917154	val: 0.795816	test: 0.772543

Epoch: 41
Loss: 0.09947819983686006
train: 0.918777	val: 0.768065	test: 0.781686

Epoch: 42
Loss: 0.0993858309290941
train: 0.922664	val: 0.779327	test: 0.777049

Epoch: 43
Loss: 0.09908985482405244
train: 0.912176	val: 0.791511	test: 0.772763

Epoch: 44
Loss: 0.09901504510618954
train: 0.922462	val: 0.759761	test: 0.798451

Epoch: 45
Loss: 0.09976150215599305
train: 0.925351	val: 0.778032	test: 0.790187

Epoch: 46
Loss: 0.09760545947755449
train: 0.923782	val: 0.787766	test: 0.777278

Epoch: 47
Loss: 0.09646333493387431
train: 0.927768	val: 0.796235	test: 0.766392

Epoch: 48
Loss: 0.09784752244241184
train: 0.927178	val: 0.781345	test: 0.776187

Epoch: 49
Loss: 0.09650748008609795
train: 0.930496	val: 0.788403	test: 0.792256

Epoch: 50
Loss: 0.09645473952008483
train: 0.928812	val: 0.770668	test: 0.759644

Epoch: 51
Loss: 0.09697867400826674
train: 0.923448	val: 0.781905	test: 0.758464

Epoch: 52
Loss: 0.09563763658836928
train: 0.935804	val: 0.774027	test: 0.778345

Epoch: 53
Loss: 0.09525251208716388
train: 0.935932	val: 0.799303	test: 0.787615

Epoch: 54
Loss: 0.09385676690359628
train: 0.939540	val: 0.777294	test: 0.756038

Epoch: 55
Loss: 0.09389156758860474
train: 0.938708	val: 0.776143	test: 0.774297

Epoch: 56
Loss: 0.09418113190528979
train: 0.930114	val: 0.789401	test: 0.771419

Epoch: 57
Loss: 0.09371536693156395
train: 0.942027	val: 0.786391	test: 0.780732

Epoch: 58
Loss: 0.09303531503947331
train: 0.937965	val: 0.800145	test: 0.791620

Epoch: 59
Loss: 0.09351482161232752
train: 0.944693	val: 0.798216	test: 0.794834

Epoch: 60
Loss: 0.09167553423251916
train: 0.943083	val: 0.759363	test: 0.788466

Epoch: 61
Loss: 0.09150202846561177
train: 0.942182	val: 0.789673	test: 0.790471

Epoch: 62
Loss: 0.09122498439159513
train: 0.948763	val: 0.773659	test: 0.773611

Epoch: 63
Loss: 0.09169225760540796
train: 0.943791	val: 0.760897	test: 0.780096

Epoch: 64
Loss: 0.09134836305573013
train: 0.949013	val: 0.781767	test: 0.799090

Epoch: 65
Loss: 0.09011889853852156
train: 0.945169	val: 0.772138	test: 0.767218

Epoch: 66
Loss: 0.09147859450919639
train: 0.949506	val: 0.786201	test: 0.758002

Epoch: 67
Loss: 0.0899990582417645
train: 0.951572	val: 0.789284	test: 0.784685

Epoch: 68
Loss: 0.08957397841962264
train: 0.945420	val: 0.786027	test: 0.785361

Epoch: 69
Loss: 0.08796779163317098
train: 0.948452	val: 0.777230	test: 0.768124

Epoch: 70
Loss: 0.08820252145199516
train: 0.932510	val: 0.770726	test: 0.759468

Epoch: 71
Loss: 0.0884673839568766
train: 0.956022	val: 0.786538	test: 0.770436

Epoch: 72
Loss: 0.08767062287787447
train: 0.957523	val: 0.773261	test: 0.783196

Epoch: 73
Loss: 0.08803947138242371
train: 0.956113	val: 0.799946	test: 0.778630

Epoch: 74
Loss: 0.0869771227584694
train: 0.958823	val: 0.769621	test: 0.786989

Epoch: 75
Loss: 0.08716270758338347
train: 0.957569	val: 0.797999	test: 0.769567

Epoch: 76
Loss: 0.08637983267240086
train: 0.958943	val: 0.781960	test: 0.775314

Epoch: 77
Loss: 0.08686419226634605
train: 0.953642	val: 0.789444	test: 0.777613

Epoch: 78
Loss: 0.08674082610057188
train: 0.959249	val: 0.801698	test: 0.779280

Epoch: 79
Loss: 0.08538152826352775
train: 0.958269	val: 0.799423	test: 0.775268

Epoch: 80
Loss: 0.0851387840881732
train: 0.942079	val: 0.769465	test: 0.737911

Epoch: 81
Loss: 0.08412607388158819
train: 0.963638	val: 0.780215	test: 0.771635

Epoch: 82
Loss: 0.08536332946537367
train: 0.963814	val: 0.784995	test: 0.784540

Epoch: 83
Loss: 0.08438366303356315
train: 0.966444	val: 0.778950	test: 0.771734

Epoch: 84
Loss: 0.08397168365509003
train: 0.965363	val: 0.768699	test: 0.764750

Epoch: 85
Loss: 0.0820573435979025
train: 0.965511	val: 0.786504	test: 0.770654

Epoch: 86
Loss: 0.08209021672174446
train: 0.964060	val: 0.745413	test: 0.737652

Epoch: 87
Loss: 0.08288495704498675
train: 0.964903	val: 0.761648	test: 0.761372

Epoch: 88
Loss: 0.08163438705740281
train: 0.966253	val: 0.779793	test: 0.764339

Epoch: 89
Loss: 0.08241454246901422
train: 0.964370	val: 0.781872	test: 0.755951

Epoch: 90
Loss: 0.08159402538907438
train: 0.966439	val: 0.793106	test: 0.772636

Epoch: 91
Loss: 0.08251588975465958
train: 0.968881	val: 0.783308	test: 0.762722

Epoch: 92
Loss: 0.08172751712905832
train: 0.971441	val: 0.786706	test: 0.775813

Epoch: 93
Loss: 0.08049622475653413
train: 0.969875	val: 0.788727	test: 0.775027

Epoch: 94
Loss: 0.0816102457726319
train: 0.970353	val: 0.778807	test: 0.771857

Epoch: 95
Loss: 0.080270104318186
train: 0.968008	val: 0.778678	test: 0.754476

Epoch: 96
Loss: 0.0817713580403675
train: 0.971638	val: 0.785105	test: 0.774020

Epoch: 97
Loss: 0.07900654312203664
train: 0.970831	val: 0.787199	test: 0.769420

Epoch: 98
Loss: 0.08041735987567727
train: 0.970132	val: 0.796474	test: 0.776367

Epoch: 99
Loss: 0.07965169121599901
train: 0.966458	val: 0.792717	test: 0.767251

Epoch: 100
Loss: 0.07692500674607113
train: 0.974300	val: 0.778727	test: 0.759561

best train: 0.877372	val: 0.807482	test: 0.784924
end
