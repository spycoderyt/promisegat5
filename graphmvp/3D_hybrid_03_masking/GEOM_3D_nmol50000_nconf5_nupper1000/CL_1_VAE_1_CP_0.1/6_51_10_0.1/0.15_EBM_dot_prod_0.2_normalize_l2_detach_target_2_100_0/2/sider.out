9242913_2
--dataset=sider --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6842700992936296
train: 0.555933	val: 0.475298	test: 0.499854

Epoch: 2
Loss: 0.6476958069495578
train: 0.564189	val: 0.511895	test: 0.522377

Epoch: 3
Loss: 0.6147539285478496
train: 0.563397	val: 0.536398	test: 0.536899

Epoch: 4
Loss: 0.5958634455559537
train: 0.575284	val: 0.555766	test: 0.553059

Epoch: 5
Loss: 0.5682628004238499
train: 0.584820	val: 0.549416	test: 0.552942

Epoch: 6
Loss: 0.5562409747012065
train: 0.611422	val: 0.564999	test: 0.561469

Epoch: 7
Loss: 0.5428515044019477
train: 0.626714	val: 0.573650	test: 0.573644

Epoch: 8
Loss: 0.5305004470048174
train: 0.639918	val: 0.584997	test: 0.584817

Epoch: 9
Loss: 0.5222156470033923
train: 0.646105	val: 0.563155	test: 0.583367

Epoch: 10
Loss: 0.5143157879570383
train: 0.653010	val: 0.561980	test: 0.585899

Epoch: 11
Loss: 0.5108690846421233
train: 0.665867	val: 0.584872	test: 0.604132

Epoch: 12
Loss: 0.5080041923472732
train: 0.673324	val: 0.590086	test: 0.612134

Epoch: 13
Loss: 0.5000862102683172
train: 0.674962	val: 0.582622	test: 0.613054

Epoch: 14
Loss: 0.49838720858740937
train: 0.678967	val: 0.580068	test: 0.614516

Epoch: 15
Loss: 0.4937330107251018
train: 0.685918	val: 0.589879	test: 0.619063

Epoch: 16
Loss: 0.49022433979266233
train: 0.692860	val: 0.600916	test: 0.624072

Epoch: 17
Loss: 0.4924418338057535
train: 0.696937	val: 0.597411	test: 0.626256

Epoch: 18
Loss: 0.4881028001376094
train: 0.698865	val: 0.599260	test: 0.628224

Epoch: 19
Loss: 0.4865781379292544
train: 0.701693	val: 0.614311	test: 0.626185

Epoch: 20
Loss: 0.48396429594856405
train: 0.709105	val: 0.615415	test: 0.628466

Epoch: 21
Loss: 0.4822926785824027
train: 0.714509	val: 0.603202	test: 0.633981

Epoch: 22
Loss: 0.47937233458321754
train: 0.718751	val: 0.610327	test: 0.643703

Epoch: 23
Loss: 0.4767279005759223
train: 0.722787	val: 0.616418	test: 0.642851

Epoch: 24
Loss: 0.47666509703913224
train: 0.725376	val: 0.611450	test: 0.638744

Epoch: 25
Loss: 0.4764873348448952
train: 0.724453	val: 0.617572	test: 0.641123

Epoch: 26
Loss: 0.47463349830092916
train: 0.729944	val: 0.610230	test: 0.639633

Epoch: 27
Loss: 0.4736365966850049
train: 0.733759	val: 0.609119	test: 0.640238

Epoch: 28
Loss: 0.4731785760654425
train: 0.736708	val: 0.620929	test: 0.642248

Epoch: 29
Loss: 0.4686202933564571
train: 0.740143	val: 0.619909	test: 0.645494

Epoch: 30
Loss: 0.469729597740134
train: 0.739672	val: 0.612078	test: 0.645263

Epoch: 31
Loss: 0.4673784800621144
train: 0.746058	val: 0.623294	test: 0.642954

Epoch: 32
Loss: 0.46996385221497333
train: 0.746104	val: 0.620329	test: 0.644956

Epoch: 33
Loss: 0.4714458350055315
train: 0.749827	val: 0.632947	test: 0.649869

Epoch: 34
Loss: 0.46265719155600343
train: 0.754367	val: 0.629087	test: 0.648039

Epoch: 35
Loss: 0.4638356488853329
train: 0.750732	val: 0.612450	test: 0.643858

Epoch: 36
Loss: 0.46136720223362176
train: 0.752820	val: 0.621350	test: 0.640257

Epoch: 37
Loss: 0.4595786603223142
train: 0.761854	val: 0.625355	test: 0.641031

Epoch: 38
Loss: 0.4617157489907548
train: 0.762670	val: 0.619574	test: 0.640259

Epoch: 39
Loss: 0.4602716429843701
train: 0.763399	val: 0.625341	test: 0.637229

Epoch: 40
Loss: 0.4577618388413878
train: 0.767640	val: 0.620518	test: 0.638805

Epoch: 41
Loss: 0.4614771973800297
train: 0.769560	val: 0.613035	test: 0.635024

Epoch: 42
Loss: 0.4567216189495168
train: 0.765896	val: 0.616857	test: 0.620827

Epoch: 43
Loss: 0.4536191969919354
train: 0.775045	val: 0.610513	test: 0.625817

Epoch: 44
Loss: 0.45825996623505894
train: 0.776801	val: 0.597921	test: 0.633227

Epoch: 45
Loss: 0.4530155639603525
train: 0.782457	val: 0.616296	test: 0.640246

Epoch: 46
Loss: 0.45148076930710024
train: 0.778715	val: 0.624852	test: 0.647506

Epoch: 47
Loss: 0.44657009276747744
train: 0.783612	val: 0.622909	test: 0.633125

Epoch: 48
Loss: 0.4505862890787684
train: 0.781883	val: 0.612686	test: 0.634784

Epoch: 49
Loss: 0.44722624672643524
train: 0.781964	val: 0.625488	test: 0.634085

Epoch: 50
Loss: 0.4461395884935663
train: 0.786744	val: 0.625334	test: 0.623697

Epoch: 51
Loss: 0.4509098397175519
train: 0.791287	val: 0.599313	test: 0.630170

Epoch: 52
Loss: 0.4470942647287927
train: 0.791111	val: 0.597236	test: 0.630723

Epoch: 53
Loss: 0.4470476834712951
train: 0.793065	val: 0.628040	test: 0.631367

Epoch: 54
Loss: 0.4490009456169948
train: 0.789891	val: 0.625316	test: 0.643865

Epoch: 55
Loss: 0.4441497248095895
train: 0.793112	val: 0.603932	test: 0.635361

Epoch: 56
Loss: 0.44404949340363087
train: 0.794502	val: 0.599985	test: 0.624849

Epoch: 57
Loss: 0.438232573031049
train: 0.799577	val: 0.603524	test: 0.623630

Epoch: 58
Loss: 0.44548995027417454
train: 0.803200	val: 0.613870	test: 0.620783

Epoch: 59
Loss: 0.43868428709176904
train: 0.803512	val: 0.627063	test: 0.624067

Epoch: 60
Loss: 0.4387989472370264
train: 0.807105	val: 0.620249	test: 0.631557

Epoch: 61
Loss: 0.44138851986921646
train: 0.809143	val: 0.609284	test: 0.622012

Epoch: 62
Loss: 0.4378199092930165
train: 0.810884	val: 0.596734	test: 0.613828

Epoch: 63
Loss: 0.43330955690362377
train: 0.812366	val: 0.602897	test: 0.624376

Epoch: 64
Loss: 0.433039055147189
train: 0.812533	val: 0.618886	test: 0.624350

Epoch: 65
Loss: 0.428538577043642
train: 0.815562	val: 0.612116	test: 0.623522

Epoch: 66
Loss: 0.4326991855173077
train: 0.816802	val: 0.608264	test: 0.626376

Epoch: 67
Loss: 0.4340791329250423
train: 0.821563	val: 0.611574	test: 0.633556

Epoch: 68
Loss: 0.4275354955617482
train: 0.822905	val: 0.614735	test: 0.631841

Epoch: 69
Loss: 0.42713699298627433
train: 0.822207	val: 0.611403	test: 0.630991

Epoch: 70
Loss: 0.4290920539243029
train: 0.817547	val: 0.607613	test: 0.632117

Epoch: 71
Loss: 0.4291143793350932
train: 0.821982	val: 0.595621	test: 0.619310

Epoch: 72
Loss: 0.4265378075950858
train: 0.822958	val: 0.601087	test: 0.613488

Epoch: 73
Loss: 0.42955148537852245
train: 0.820819	val: 0.622518	test: 0.633151

Epoch: 74
Loss: 0.4232271156225992
train: 0.820268	val: 0.620503	test: 0.623336

Epoch: 75
Loss: 0.4256282821476475
train: 0.829733	val: 0.611924	test: 0.620044

Epoch: 76
Loss: 0.42525020720827833
train: 0.831260	val: 0.605705	test: 0.616151

Epoch: 77
Loss: 0.4171747751839888
train: 0.832965	val: 0.617568	test: 0.621288

Epoch: 78
Loss: 0.4172922895937523
train: 0.825777	val: 0.629834	test: 0.617382

Epoch: 79
Loss: 0.42239666048816255
train: 0.831736	val: 0.620866	test: 0.624858

Epoch: 80
Loss: 0.41826179958569065
train: 0.838116	val: 0.611690	test: 0.629434

Epoch: 81
Loss: 0.42066048746127216
train: 0.837275	val: 0.600297	test: 0.631425

Epoch: 82
Loss: 0.41580763353700484
train: 0.837799	val: 0.612985	test: 0.615707

Epoch: 83
Loss: 0.414146492989852
train: 0.838919	val: 0.620647	test: 0.618436

Epoch: 84
Loss: 0.41511894605301836
train: 0.838826	val: 0.609645	test: 0.629146

Epoch: 85
Loss: 0.4102031196760511
train: 0.836909	val: 0.613872	test: 0.608104

Epoch: 86
Loss: 0.4161842257579246
train: 0.839998	val: 0.612770	test: 0.605364

Epoch: 87
Loss: 0.4114194673554348
train: 0.842877	val: 0.608721	test: 0.621996

Epoch: 88
Loss: 0.4109220211578976
train: 0.845106	val: 0.622606	test: 0.624343

Epoch: 89
Loss: 0.40914435541630406
train: 0.844878	val: 0.614481	test: 0.623640

Epoch: 90
Loss: 0.41023017935690964
train: 0.847129	val: 0.610000	test: 0.624467

Epoch: 91
Loss: 0.41208836774170576
train: 0.850399	val: 0.618945	test: 0.601992

Epoch: 92
Loss: 0.4035166320817294
train: 0.846684	val: 0.616984	test: 0.618744

Epoch: 93
Loss: 0.40760055368250264
train: 0.848118	val: 0.622531	test: 0.623686

Epoch: 94
Loss: 0.4038330558723035
train: 0.851417	val: 0.613588	test: 0.614648

Epoch: 95
Loss: 0.4078219384405187
train: 0.849946	val: 0.607679	test: 0.608901

Epoch: 96
Loss: 0.40027652647157497
train: 0.858328	val: 0.616620	test: 0.609659

Epoch: 97
Loss: 0.4001727057068648
train: 0.860086	val: 0.625371	test: 0.611392

Epoch: 98
Loss: 0.4008833944228085
train: 0.853714	val: 0.619955	test: 0.613401

Epoch: 99
Loss: 0.4026694781354192
train: 0.860774	val: 0.615880	test: 0.617947

Epoch: 100
Loss: 0.39788482734451763
train: 0.854306	val: 0.613706	test: 0.634843

best train: 0.749827	val: 0.632947	test: 0.649869
end
