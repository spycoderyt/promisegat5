9242913_2
--dataset=bace --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6730635716019048
train: 0.670596	val: 0.593773	test: 0.695705

Epoch: 2
Loss: 0.6357921010845191
train: 0.751978	val: 0.596703	test: 0.685794

Epoch: 3
Loss: 0.5891034070294927
train: 0.772894	val: 0.593407	test: 0.706660

Epoch: 4
Loss: 0.564584997908274
train: 0.814018	val: 0.618681	test: 0.741784

Epoch: 5
Loss: 0.5265242644963217
train: 0.817643	val: 0.680952	test: 0.758477

Epoch: 6
Loss: 0.501325281848754
train: 0.854649	val: 0.664469	test: 0.790819

Epoch: 7
Loss: 0.5022941586049153
train: 0.866122	val: 0.682051	test: 0.787863

Epoch: 8
Loss: 0.47471510230349734
train: 0.876610	val: 0.660806	test: 0.789602

Epoch: 9
Loss: 0.4748090766930356
train: 0.878328	val: 0.621612	test: 0.780908

Epoch: 10
Loss: 0.45992379658025995
train: 0.881253	val: 0.693040	test: 0.799165

Epoch: 11
Loss: 0.4643611719939216
train: 0.887043	val: 0.693773	test: 0.795862

Epoch: 12
Loss: 0.4518929496288454
train: 0.892911	val: 0.677289	test: 0.797079

Epoch: 13
Loss: 0.44978904125466446
train: 0.885131	val: 0.598168	test: 0.773431

Epoch: 14
Loss: 0.44537564129126317
train: 0.894546	val: 0.620147	test: 0.799339

Epoch: 15
Loss: 0.43200394481652615
train: 0.901735	val: 0.679853	test: 0.815858

Epoch: 16
Loss: 0.4361442147291532
train: 0.898282	val: 0.639927	test: 0.793601

Epoch: 17
Loss: 0.43178289990075147
train: 0.904737	val: 0.684615	test: 0.808729

Epoch: 18
Loss: 0.43552696039086314
train: 0.904364	val: 0.710623	test: 0.816901

Epoch: 19
Loss: 0.4241680226842209
train: 0.905477	val: 0.663370	test: 0.809946

Epoch: 20
Loss: 0.41487507579517136
train: 0.910362	val: 0.650916	test: 0.819510

Epoch: 21
Loss: 0.4076601740717436
train: 0.910868	val: 0.654945	test: 0.818466

Epoch: 22
Loss: 0.4091933392619871
train: 0.906627	val: 0.607326	test: 0.802643

Epoch: 23
Loss: 0.4194435581302228
train: 0.914475	val: 0.683883	test: 0.806990

Epoch: 24
Loss: 0.4073910035028332
train: 0.917246	val: 0.686813	test: 0.815336

Epoch: 25
Loss: 0.3909129223533454
train: 0.916059	val: 0.674725	test: 0.800209

Epoch: 26
Loss: 0.3958197971336549
train: 0.920291	val: 0.692674	test: 0.817249

Epoch: 27
Loss: 0.39500879343665074
train: 0.921784	val: 0.678022	test: 0.811685

Epoch: 28
Loss: 0.3908707002351017
train: 0.921963	val: 0.657509	test: 0.811511

Epoch: 29
Loss: 0.3920116031310521
train: 0.922283	val: 0.644689	test: 0.810642

Epoch: 30
Loss: 0.3824208548595209
train: 0.924541	val: 0.673260	test: 0.810816

Epoch: 31
Loss: 0.38177161692877926
train: 0.926752	val: 0.678755	test: 0.797079

Epoch: 32
Loss: 0.3795546710485246
train: 0.927874	val: 0.672527	test: 0.798818

Epoch: 33
Loss: 0.37289232753929313
train: 0.928918	val: 0.689011	test: 0.812033

Epoch: 34
Loss: 0.3629149112259228
train: 0.930716	val: 0.669963	test: 0.812554

Epoch: 35
Loss: 0.36474731386808595
train: 0.930297	val: 0.630037	test: 0.810816

Epoch: 36
Loss: 0.3680414671690614
train: 0.933916	val: 0.646154	test: 0.822292

Epoch: 37
Loss: 0.3677281080704279
train: 0.934638	val: 0.681685	test: 0.819684

Epoch: 38
Loss: 0.3696684328195416
train: 0.935905	val: 0.665201	test: 0.822987

Epoch: 39
Loss: 0.36952716651045203
train: 0.937469	val: 0.661172	test: 0.829073

Epoch: 40
Loss: 0.3600183357776241
train: 0.938242	val: 0.675824	test: 0.822640

Epoch: 41
Loss: 0.3581875731568697
train: 0.937660	val: 0.645788	test: 0.824378

Epoch: 42
Loss: 0.3505744510940817
train: 0.939609	val: 0.669963	test: 0.814989

Epoch: 43
Loss: 0.36345285155965623
train: 0.940962	val: 0.679121	test: 0.820553

Epoch: 44
Loss: 0.3445512647958995
train: 0.939977	val: 0.672527	test: 0.829943

Epoch: 45
Loss: 0.35489792821440874
train: 0.942397	val: 0.608791	test: 0.816032

Epoch: 46
Loss: 0.35532764396617444
train: 0.942902	val: 0.614286	test: 0.801252

Epoch: 47
Loss: 0.3519494141306844
train: 0.943339	val: 0.679121	test: 0.799165

Epoch: 48
Loss: 0.3359813853499498
train: 0.944866	val: 0.683883	test: 0.800383

Epoch: 49
Loss: 0.3418497461962115
train: 0.944874	val: 0.630403	test: 0.801252

Epoch: 50
Loss: 0.33938324719570534
train: 0.946601	val: 0.668132	test: 0.796209

Epoch: 51
Loss: 0.335574929124712
train: 0.945728	val: 0.704396	test: 0.799687

Epoch: 52
Loss: 0.3439213365285537
train: 0.948613	val: 0.666667	test: 0.812380

Epoch: 53
Loss: 0.3291942404640392
train: 0.949421	val: 0.639927	test: 0.825596

Epoch: 54
Loss: 0.3287357110954841
train: 0.948299	val: 0.659341	test: 0.818466

Epoch: 55
Loss: 0.33817811439157464
train: 0.948913	val: 0.664835	test: 0.805773

Epoch: 56
Loss: 0.3279270190371806
train: 0.949735	val: 0.650183	test: 0.807338

Epoch: 57
Loss: 0.33078422840876204
train: 0.950180	val: 0.636630	test: 0.820727

Epoch: 58
Loss: 0.3168686025876438
train: 0.951772	val: 0.650183	test: 0.813250

Epoch: 59
Loss: 0.33012417100253644
train: 0.952197	val: 0.651282	test: 0.804556

Epoch: 60
Loss: 0.3239448378384374
train: 0.951924	val: 0.667399	test: 0.823857

Epoch: 61
Loss: 0.3135440711506322
train: 0.954247	val: 0.667399	test: 0.819510

Epoch: 62
Loss: 0.31926086514125834
train: 0.956604	val: 0.658608	test: 0.808903

Epoch: 63
Loss: 0.323998084629818
train: 0.955283	val: 0.653480	test: 0.809424

Epoch: 64
Loss: 0.32421734105918876
train: 0.957243	val: 0.656044	test: 0.809598

Epoch: 65
Loss: 0.3166293162496859
train: 0.956390	val: 0.666667	test: 0.808555

Epoch: 66
Loss: 0.29839545564723585
train: 0.958022	val: 0.673626	test: 0.799861

Epoch: 67
Loss: 0.3163277226554628
train: 0.958516	val: 0.660806	test: 0.797079

Epoch: 68
Loss: 0.2965217320733926
train: 0.959053	val: 0.654945	test: 0.806642

Epoch: 69
Loss: 0.32409682318720795
train: 0.959626	val: 0.649451	test: 0.807686

Epoch: 70
Loss: 0.29967878599186726
train: 0.959849	val: 0.653480	test: 0.799861

Epoch: 71
Loss: 0.3172731905535223
train: 0.957945	val: 0.655678	test: 0.804382

Epoch: 72
Loss: 0.3019696439318639
train: 0.959583	val: 0.623810	test: 0.801078

Epoch: 73
Loss: 0.3058077181833475
train: 0.959267	val: 0.646886	test: 0.810816

Epoch: 74
Loss: 0.2908499128015188
train: 0.959906	val: 0.672161	test: 0.817597

Epoch: 75
Loss: 0.29018275598720455
train: 0.960562	val: 0.663736	test: 0.809946

Epoch: 76
Loss: 0.3129311833158716
train: 0.959284	val: 0.667766	test: 0.801947

Epoch: 77
Loss: 0.30968901060316606
train: 0.960693	val: 0.656410	test: 0.802817

Epoch: 78
Loss: 0.29728214389539015
train: 0.963279	val: 0.648718	test: 0.812207

Epoch: 79
Loss: 0.30588827820509734
train: 0.961809	val: 0.632234	test: 0.794818

Epoch: 80
Loss: 0.2993986745883707
train: 0.964101	val: 0.641392	test: 0.804556

Epoch: 81
Loss: 0.30219985506543806
train: 0.964269	val: 0.672527	test: 0.812902

Epoch: 82
Loss: 0.2903422475700145
train: 0.964926	val: 0.684615	test: 0.814641

Epoch: 83
Loss: 0.29452180493381214
train: 0.964335	val: 0.667033	test: 0.796905

Epoch: 84
Loss: 0.2869334020064792
train: 0.964152	val: 0.646886	test: 0.812033

Epoch: 85
Loss: 0.293900512978297
train: 0.965796	val: 0.656044	test: 0.819857

Epoch: 86
Loss: 0.2964273890810249
train: 0.967497	val: 0.636996	test: 0.807512

Epoch: 87
Loss: 0.2735523505947809
train: 0.966079	val: 0.638095	test: 0.804208

Epoch: 88
Loss: 0.28590943612950814
train: 0.967471	val: 0.633700	test: 0.804556

Epoch: 89
Loss: 0.30612983873638533
train: 0.966056	val: 0.600733	test: 0.803165

Epoch: 90
Loss: 0.27503410226098823
train: 0.966929	val: 0.636996	test: 0.801947

Epoch: 91
Loss: 0.2883413586259082
train: 0.968213	val: 0.655311	test: 0.779343

Epoch: 92
Loss: 0.30529340156304013
train: 0.967988	val: 0.672894	test: 0.798644

Epoch: 93
Loss: 0.28402452006966955
train: 0.967454	val: 0.693040	test: 0.814119

Epoch: 94
Loss: 0.2714822204260908
train: 0.966090	val: 0.641026	test: 0.805947

Epoch: 95
Loss: 0.27428802678238107
train: 0.969115	val: 0.646886	test: 0.808207

Epoch: 96
Loss: 0.2800096499102303
train: 0.967183	val: 0.678388	test: 0.799861

Epoch: 97
Loss: 0.2937558502176051
train: 0.964800	val: 0.673626	test: 0.787515

Epoch: 98
Loss: 0.28113174594479745
train: 0.970365	val: 0.675824	test: 0.816206

Epoch: 99
Loss: 0.26694639465169867
train: 0.970031	val: 0.662637	test: 0.817945

Epoch: 100
Loss: 0.28249390533649
train: 0.968636	val: 0.646154	test: 0.788732

best train: 0.904364	val: 0.710623	test: 0.816901
end
