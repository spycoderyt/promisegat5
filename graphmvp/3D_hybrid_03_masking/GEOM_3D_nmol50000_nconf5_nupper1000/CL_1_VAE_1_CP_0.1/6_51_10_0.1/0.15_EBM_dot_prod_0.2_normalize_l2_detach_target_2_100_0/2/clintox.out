9242913_2
--dataset=clintox --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='clintox', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6713268210263483
train: 0.643451	val: 0.610063	test: 0.472621

Epoch: 2
Loss: 0.5932435597212271
train: 0.639036	val: 0.564285	test: 0.406870

Epoch: 3
Loss: 0.5241597401240442
train: 0.701245	val: 0.743584	test: 0.448801

Epoch: 4
Loss: 0.47675609820066284
train: 0.718727	val: 0.775699	test: 0.449906

Epoch: 5
Loss: 0.43162244070415196
train: 0.750420	val: 0.776673	test: 0.485333

Epoch: 6
Loss: 0.39446833227668254
train: 0.776310	val: 0.754559	test: 0.520012

Epoch: 7
Loss: 0.3582390162672398
train: 0.793558	val: 0.748465	test: 0.571934

Epoch: 8
Loss: 0.32853543574890054
train: 0.809413	val: 0.745668	test: 0.576069

Epoch: 9
Loss: 0.3030760663018895
train: 0.822600	val: 0.746005	test: 0.590005

Epoch: 10
Loss: 0.2813231997758055
train: 0.829117	val: 0.775587	test: 0.587693

Epoch: 11
Loss: 0.27037410084273705
train: 0.835098	val: 0.784365	test: 0.600117

Epoch: 12
Loss: 0.2558736843338437
train: 0.827024	val: 0.817081	test: 0.560835

Epoch: 13
Loss: 0.2502206024449635
train: 0.855231	val: 0.838433	test: 0.614408

Epoch: 14
Loss: 0.2354682228668909
train: 0.859718	val: 0.834188	test: 0.613846

Epoch: 15
Loss: 0.23307019121832478
train: 0.875914	val: 0.822837	test: 0.646444

Epoch: 16
Loss: 0.21710931414004037
train: 0.879295	val: 0.770418	test: 0.650454

Epoch: 17
Loss: 0.21858833384518667
train: 0.882476	val: 0.789123	test: 0.642152

Epoch: 18
Loss: 0.21817374909772252
train: 0.882084	val: 0.787050	test: 0.652801

Epoch: 19
Loss: 0.20814495847415992
train: 0.892027	val: 0.785539	test: 0.685881

Epoch: 20
Loss: 0.22198011675551804
train: 0.888094	val: 0.777548	test: 0.669558

Epoch: 21
Loss: 0.20549396507527548
train: 0.901688	val: 0.754197	test: 0.684605

Epoch: 22
Loss: 0.20616612807162288
train: 0.904621	val: 0.830417	test: 0.647467

Epoch: 23
Loss: 0.18523573274642519
train: 0.903512	val: 0.843778	test: 0.637828

Epoch: 24
Loss: 0.18358159596426793
train: 0.911907	val: 0.851656	test: 0.660333

Epoch: 25
Loss: 0.18621098475114084
train: 0.922245	val: 0.822163	test: 0.679333

Epoch: 26
Loss: 0.18942500909026494
train: 0.926638	val: 0.796527	test: 0.680106

Epoch: 27
Loss: 0.1961360203295081
train: 0.924710	val: 0.759679	test: 0.713604

Epoch: 28
Loss: 0.18465251630673193
train: 0.922558	val: 0.757556	test: 0.710644

Epoch: 29
Loss: 0.18681205205621756
train: 0.933442	val: 0.786663	test: 0.723977

Epoch: 30
Loss: 0.17919812692938247
train: 0.929100	val: 0.794292	test: 0.716869

Epoch: 31
Loss: 0.1814180276586032
train: 0.936822	val: 0.806616	test: 0.721430

Epoch: 32
Loss: 0.1839883817032078
train: 0.941078	val: 0.826808	test: 0.689394

Epoch: 33
Loss: 0.1740452436088394
train: 0.940738	val: 0.836061	test: 0.687577

Epoch: 34
Loss: 0.17201440758240458
train: 0.941613	val: 0.867853	test: 0.681924

Epoch: 35
Loss: 0.1628359132291343
train: 0.939804	val: 0.844227	test: 0.708786

Epoch: 36
Loss: 0.16446714924012512
train: 0.943577	val: 0.806929	test: 0.736756

Epoch: 37
Loss: 0.19040855639456744
train: 0.945320	val: 0.784766	test: 0.733947

Epoch: 38
Loss: 0.17112038111492378
train: 0.951324	val: 0.820240	test: 0.728475

Epoch: 39
Loss: 0.1699486041870196
train: 0.952035	val: 0.817418	test: 0.723652

Epoch: 40
Loss: 0.166324525686498
train: 0.954398	val: 0.800386	test: 0.736692

Epoch: 41
Loss: 0.16707516762879354
train: 0.957463	val: 0.814309	test: 0.746923

Epoch: 42
Loss: 0.16868517623888366
train: 0.957665	val: 0.848697	test: 0.742927

Epoch: 43
Loss: 0.16031397224151694
train: 0.954576	val: 0.789896	test: 0.758966

Epoch: 44
Loss: 0.16393208188751768
train: 0.957247	val: 0.787573	test: 0.758060

Epoch: 45
Loss: 0.16645511694069395
train: 0.953998	val: 0.765533	test: 0.787580

Epoch: 46
Loss: 0.16448875967180482
train: 0.959905	val: 0.836672	test: 0.784112

Epoch: 47
Loss: 0.17189364916029343
train: 0.953437	val: 0.857177	test: 0.766597

Epoch: 48
Loss: 0.16103400950792426
train: 0.960472	val: 0.830754	test: 0.782263

Epoch: 49
Loss: 0.1765447994088465
train: 0.957246	val: 0.762588	test: 0.800364

Epoch: 50
Loss: 0.14946931361512858
train: 0.954901	val: 0.722106	test: 0.802112

Epoch: 51
Loss: 0.16156108706096634
train: 0.963140	val: 0.744557	test: 0.780888

Epoch: 52
Loss: 0.1503553693145474
train: 0.962877	val: 0.782267	test: 0.777515

Epoch: 53
Loss: 0.1609070161289373
train: 0.963183	val: 0.836061	test: 0.780089

Epoch: 54
Loss: 0.15798515685955922
train: 0.963069	val: 0.846262	test: 0.796877

Epoch: 55
Loss: 0.14475111341752508
train: 0.962870	val: 0.851632	test: 0.778789

Epoch: 56
Loss: 0.1557085483887144
train: 0.966013	val: 0.824960	test: 0.774318

Epoch: 57
Loss: 0.1570000792666181
train: 0.966453	val: 0.786351	test: 0.766623

Epoch: 58
Loss: 0.15023607329353408
train: 0.965557	val: 0.757268	test: 0.794567

Epoch: 59
Loss: 0.15693341317804327
train: 0.964535	val: 0.774687	test: 0.807147

Epoch: 60
Loss: 0.14294729925791114
train: 0.972569	val: 0.795016	test: 0.808432

Epoch: 61
Loss: 0.14556253119529483
train: 0.970745	val: 0.792781	test: 0.805022

Epoch: 62
Loss: 0.15888432793111684
train: 0.965806	val: 0.793955	test: 0.803700

Epoch: 63
Loss: 0.14275696296588364
train: 0.969760	val: 0.814446	test: 0.798915

Epoch: 64
Loss: 0.14453557555598326
train: 0.972779	val: 0.806954	test: 0.789545

Epoch: 65
Loss: 0.13495038382628982
train: 0.969018	val: 0.789172	test: 0.807534

Epoch: 66
Loss: 0.15290277210885198
train: 0.972593	val: 0.810837	test: 0.792718

Epoch: 67
Loss: 0.14364209986578963
train: 0.971235	val: 0.819703	test: 0.787347

Epoch: 68
Loss: 0.13624838744395398
train: 0.971718	val: 0.807428	test: 0.814267

Epoch: 69
Loss: 0.13630220094375164
train: 0.972663	val: 0.800772	test: 0.823248

Epoch: 70
Loss: 0.13620315892435003
train: 0.975935	val: 0.798175	test: 0.799587

Epoch: 71
Loss: 0.1400365232329342
train: 0.973173	val: 0.817355	test: 0.805971

Epoch: 72
Loss: 0.1381834525795217
train: 0.974735	val: 0.826471	test: 0.806384

Epoch: 73
Loss: 0.1528191836356194
train: 0.974299	val: 0.848610	test: 0.797008

Epoch: 74
Loss: 0.13560602045489895
train: 0.978494	val: 0.840330	test: 0.816751

Epoch: 75
Loss: 0.13091164308473327
train: 0.978152	val: 0.806616	test: 0.776178

Epoch: 76
Loss: 0.14342195044467115
train: 0.976221	val: 0.791021	test: 0.770263

Epoch: 77
Loss: 0.1287183262277975
train: 0.976109	val: 0.815820	test: 0.772406

Epoch: 78
Loss: 0.12968572192587652
train: 0.977391	val: 0.796752	test: 0.778177

Epoch: 79
Loss: 0.13465278282758145
train: 0.978255	val: 0.774526	test: 0.806109

Epoch: 80
Loss: 0.13869210976839005
train: 0.977490	val: 0.778047	test: 0.806809

Epoch: 81
Loss: 0.13443568116048674
train: 0.979029	val: 0.791270	test: 0.793392

Epoch: 82
Loss: 0.1296496229187231
train: 0.977026	val: 0.759229	test: 0.801025

Epoch: 83
Loss: 0.12680186991506942
train: 0.979686	val: 0.787549	test: 0.818589

Epoch: 84
Loss: 0.12815037974174498
train: 0.980496	val: 0.807790	test: 0.821312

Epoch: 85
Loss: 0.12398319886207676
train: 0.979495	val: 0.819003	test: 0.797895

Epoch: 86
Loss: 0.13271057458249125
train: 0.980369	val: 0.797613	test: 0.807788

Epoch: 87
Loss: 0.12687312584002763
train: 0.980557	val: 0.792107	test: 0.818869

Epoch: 88
Loss: 0.11859937842740417
train: 0.980290	val: 0.776761	test: 0.828319

Epoch: 89
Loss: 0.12885719245488414
train: 0.978815	val: 0.812211	test: 0.803248

Epoch: 90
Loss: 0.13414197930492971
train: 0.978966	val: 0.848834	test: 0.801799

Epoch: 91
Loss: 0.13079510615609463
train: 0.979173	val: 0.835249	test: 0.795703

Epoch: 92
Loss: 0.11860368024405468
train: 0.980122	val: 0.800635	test: 0.784017

Epoch: 93
Loss: 0.12397350846584651
train: 0.975278	val: 0.739262	test: 0.782263

Epoch: 94
Loss: 0.12276158839055802
train: 0.975301	val: 0.749326	test: 0.791700

Epoch: 95
Loss: 0.11932939762871753
train: 0.980036	val: 0.773514	test: 0.810793

Epoch: 96
Loss: 0.12910954997610452
train: 0.978522	val: 0.767308	test: 0.826950

Epoch: 97
Loss: 0.13297828931698238
train: 0.981042	val: 0.783216	test: 0.826982

Epoch: 98
Loss: 0.12634113413694065
train: 0.978228	val: 0.828368	test: 0.811443

Epoch: 99
Loss: 0.12951788038670534
train: 0.977767	val: 0.869163	test: 0.824834

Epoch: 100
Loss: 0.12128003228041842
train: 0.978347	val: 0.804719	test: 0.828106

best train: 0.977767	val: 0.869163	test: 0.824834
end
