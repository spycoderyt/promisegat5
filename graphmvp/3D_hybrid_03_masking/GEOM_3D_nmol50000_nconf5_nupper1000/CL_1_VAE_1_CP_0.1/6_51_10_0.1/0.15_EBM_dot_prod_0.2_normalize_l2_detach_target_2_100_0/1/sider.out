11375797_1
--dataset=sider --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6956644260495203
train: 0.545918	val: 0.496945	test: 0.506019

Epoch: 2
Loss: 0.6576903563033036
train: 0.567394	val: 0.512278	test: 0.527775

Epoch: 3
Loss: 0.6275727704961795
train: 0.567349	val: 0.523531	test: 0.545555

Epoch: 4
Loss: 0.5960898032087922
train: 0.569329	val: 0.535814	test: 0.546041

Epoch: 5
Loss: 0.5767291617331944
train: 0.588321	val: 0.541937	test: 0.545231

Epoch: 6
Loss: 0.5585624839009462
train: 0.619887	val: 0.564020	test: 0.565163

Epoch: 7
Loss: 0.5418999235827731
train: 0.633425	val: 0.567786	test: 0.587846

Epoch: 8
Loss: 0.5327181096986322
train: 0.636915	val: 0.567248	test: 0.589486

Epoch: 9
Loss: 0.5226820080648016
train: 0.649404	val: 0.561485	test: 0.587050

Epoch: 10
Loss: 0.5169879966208112
train: 0.657639	val: 0.568525	test: 0.588915

Epoch: 11
Loss: 0.5083468275571987
train: 0.667293	val: 0.587854	test: 0.591919

Epoch: 12
Loss: 0.5079398490822944
train: 0.670211	val: 0.592256	test: 0.596326

Epoch: 13
Loss: 0.5001975798317239
train: 0.671745	val: 0.597499	test: 0.595857

Epoch: 14
Loss: 0.49900123099510185
train: 0.679488	val: 0.604550	test: 0.601695

Epoch: 15
Loss: 0.49325094475977715
train: 0.690197	val: 0.611832	test: 0.610827

Epoch: 16
Loss: 0.49267141592716657
train: 0.696173	val: 0.617515	test: 0.609403

Epoch: 17
Loss: 0.4892046485838006
train: 0.700201	val: 0.610313	test: 0.610916

Epoch: 18
Loss: 0.48870072574970697
train: 0.704754	val: 0.607158	test: 0.616565

Epoch: 19
Loss: 0.48478519694035294
train: 0.711921	val: 0.618242	test: 0.622438

Epoch: 20
Loss: 0.48541898603947153
train: 0.716183	val: 0.621165	test: 0.626576

Epoch: 21
Loss: 0.48384997242187017
train: 0.718415	val: 0.626822	test: 0.620095

Epoch: 22
Loss: 0.4828914505049203
train: 0.723423	val: 0.627935	test: 0.621379

Epoch: 23
Loss: 0.4799086054535676
train: 0.724445	val: 0.610971	test: 0.631569

Epoch: 24
Loss: 0.47736933233496026
train: 0.720698	val: 0.614414	test: 0.634298

Epoch: 25
Loss: 0.4725618282234406
train: 0.728100	val: 0.622397	test: 0.635058

Epoch: 26
Loss: 0.4729577456139176
train: 0.734732	val: 0.627470	test: 0.632428

Epoch: 27
Loss: 0.4740814109469995
train: 0.734622	val: 0.617641	test: 0.638673

Epoch: 28
Loss: 0.4722732879282413
train: 0.731734	val: 0.624193	test: 0.632354

Epoch: 29
Loss: 0.474493113880854
train: 0.737689	val: 0.616895	test: 0.624151

Epoch: 30
Loss: 0.4737322626066214
train: 0.736380	val: 0.611720	test: 0.625745

Epoch: 31
Loss: 0.4692569111647126
train: 0.735922	val: 0.604577	test: 0.628914

Epoch: 32
Loss: 0.47162982474158743
train: 0.749291	val: 0.610121	test: 0.631071

Epoch: 33
Loss: 0.46485093517991405
train: 0.752443	val: 0.612356	test: 0.634247

Epoch: 34
Loss: 0.4699046420647166
train: 0.755804	val: 0.615115	test: 0.630783

Epoch: 35
Loss: 0.46400091997460347
train: 0.758236	val: 0.613176	test: 0.626151

Epoch: 36
Loss: 0.46321401837008525
train: 0.762631	val: 0.616404	test: 0.627138

Epoch: 37
Loss: 0.46516022706319876
train: 0.763441	val: 0.623447	test: 0.637752

Epoch: 38
Loss: 0.46128517326238816
train: 0.765737	val: 0.625914	test: 0.630652

Epoch: 39
Loss: 0.4663023878437505
train: 0.766454	val: 0.612429	test: 0.634887

Epoch: 40
Loss: 0.4585268928069402
train: 0.773221	val: 0.625697	test: 0.630282

Epoch: 41
Loss: 0.45533017555703237
train: 0.771976	val: 0.634771	test: 0.616556

Epoch: 42
Loss: 0.46371150159885055
train: 0.775811	val: 0.622557	test: 0.625801

Epoch: 43
Loss: 0.4557088727939241
train: 0.773811	val: 0.615998	test: 0.622333

Epoch: 44
Loss: 0.4580067814687697
train: 0.770171	val: 0.611175	test: 0.618605

Epoch: 45
Loss: 0.4566868275810392
train: 0.777585	val: 0.609508	test: 0.624596

Epoch: 46
Loss: 0.45567431487087556
train: 0.782003	val: 0.606984	test: 0.618269

Epoch: 47
Loss: 0.45314373037059064
train: 0.778871	val: 0.617673	test: 0.619226

Epoch: 48
Loss: 0.4501385273010877
train: 0.780107	val: 0.610733	test: 0.631572

Epoch: 49
Loss: 0.4521383749910227
train: 0.788260	val: 0.611565	test: 0.631937

Epoch: 50
Loss: 0.44551628361055035
train: 0.791948	val: 0.619995	test: 0.621028

Epoch: 51
Loss: 0.4488092276343412
train: 0.791059	val: 0.611967	test: 0.623366

Epoch: 52
Loss: 0.4469397406745224
train: 0.792120	val: 0.594315	test: 0.618130

Epoch: 53
Loss: 0.44799798711932104
train: 0.797485	val: 0.603564	test: 0.615146

Epoch: 54
Loss: 0.43571498053692537
train: 0.799648	val: 0.612594	test: 0.620816

Epoch: 55
Loss: 0.44282908817602584
train: 0.795623	val: 0.615356	test: 0.625759

Epoch: 56
Loss: 0.44320966355823826
train: 0.802904	val: 0.610360	test: 0.619496

Epoch: 57
Loss: 0.44093535494157277
train: 0.807434	val: 0.608648	test: 0.611412

Epoch: 58
Loss: 0.43994162949218063
train: 0.802212	val: 0.607342	test: 0.615146

Epoch: 59
Loss: 0.4404502098921343
train: 0.808114	val: 0.599713	test: 0.619207

Epoch: 60
Loss: 0.4375033929406003
train: 0.811157	val: 0.613888	test: 0.608595

Epoch: 61
Loss: 0.4398558480400938
train: 0.810785	val: 0.614385	test: 0.611469

Epoch: 62
Loss: 0.43424678761310415
train: 0.812461	val: 0.614234	test: 0.620314

Epoch: 63
Loss: 0.4337192696139452
train: 0.812854	val: 0.617450	test: 0.624244

Epoch: 64
Loss: 0.4324147989748372
train: 0.817429	val: 0.612217	test: 0.623988

Epoch: 65
Loss: 0.43583077528455094
train: 0.819885	val: 0.610875	test: 0.624978

Epoch: 66
Loss: 0.4247288100219264
train: 0.819106	val: 0.617083	test: 0.622185

Epoch: 67
Loss: 0.42939383551021687
train: 0.823103	val: 0.619182	test: 0.610764

Epoch: 68
Loss: 0.42811512423017206
train: 0.825303	val: 0.614100	test: 0.613990

Epoch: 69
Loss: 0.4246865448370742
train: 0.825337	val: 0.617349	test: 0.619089

Epoch: 70
Loss: 0.42806770832849433
train: 0.823992	val: 0.609076	test: 0.619226

Epoch: 71
Loss: 0.4295398979626176
train: 0.826139	val: 0.608499	test: 0.605853

Epoch: 72
Loss: 0.4244147561218197
train: 0.829712	val: 0.617060	test: 0.615216

Epoch: 73
Loss: 0.4279183910618478
train: 0.832077	val: 0.622715	test: 0.611104

Epoch: 74
Loss: 0.4272997051488292
train: 0.832492	val: 0.620029	test: 0.617096

Epoch: 75
Loss: 0.4239023805594034
train: 0.824057	val: 0.614861	test: 0.609998

Epoch: 76
Loss: 0.42417460553884057
train: 0.837119	val: 0.627578	test: 0.623322

Epoch: 77
Loss: 0.42566491927777445
train: 0.833497	val: 0.618594	test: 0.621851

Epoch: 78
Loss: 0.4229316158161893
train: 0.822556	val: 0.613188	test: 0.625701

Epoch: 79
Loss: 0.41601354128918
train: 0.831765	val: 0.631662	test: 0.614425

Epoch: 80
Loss: 0.41745247840965655
train: 0.842130	val: 0.635608	test: 0.621640

Epoch: 81
Loss: 0.41942033204222523
train: 0.839441	val: 0.630326	test: 0.621051

Epoch: 82
Loss: 0.4157482871395019
train: 0.837091	val: 0.619062	test: 0.625141

Epoch: 83
Loss: 0.4157931874753878
train: 0.844682	val: 0.621249	test: 0.612227

Epoch: 84
Loss: 0.41678944186559513
train: 0.846314	val: 0.626314	test: 0.609879

Epoch: 85
Loss: 0.4138134074365718
train: 0.843549	val: 0.636509	test: 0.598747

Epoch: 86
Loss: 0.4188110865658755
train: 0.847343	val: 0.632647	test: 0.626349

Epoch: 87
Loss: 0.4080661869104868
train: 0.848231	val: 0.627138	test: 0.621138

Epoch: 88
Loss: 0.4050222147409932
train: 0.851704	val: 0.633511	test: 0.616175

Epoch: 89
Loss: 0.40841715130785605
train: 0.850655	val: 0.636264	test: 0.627843

Epoch: 90
Loss: 0.410290780563474
train: 0.854328	val: 0.637325	test: 0.612963

Epoch: 91
Loss: 0.40648661788822454
train: 0.855716	val: 0.640595	test: 0.609222

Epoch: 92
Loss: 0.41284528359208716
train: 0.857026	val: 0.637884	test: 0.620815

Epoch: 93
Loss: 0.4076779478359514
train: 0.852632	val: 0.641413	test: 0.614245

Epoch: 94
Loss: 0.4018781337409205
train: 0.853312	val: 0.645146	test: 0.596082

Epoch: 95
Loss: 0.4063764738704648
train: 0.859218	val: 0.647183	test: 0.618526

Epoch: 96
Loss: 0.39472907335953134
train: 0.857875	val: 0.643648	test: 0.634230

Epoch: 97
Loss: 0.3989643025394994
train: 0.861805	val: 0.647400	test: 0.621697

Epoch: 98
Loss: 0.4022601148717685
train: 0.856456	val: 0.640277	test: 0.606547

Epoch: 99
Loss: 0.40371132511369756
train: 0.853529	val: 0.632607	test: 0.618752

Epoch: 100
Loss: 0.39747791840158114
train: 0.862740	val: 0.634261	test: 0.611574

best train: 0.861805	val: 0.647400	test: 0.621697
end
