11375797_1
--dataset=hiv --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_CP_0.1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25151982887936963
train: 0.759087	val: 0.728928	test: 0.716443

Epoch: 2
Loss: 0.14014927111215258
train: 0.763009	val: 0.711582	test: 0.753361

Epoch: 3
Loss: 0.13548261146455964
train: 0.793714	val: 0.749510	test: 0.764063

Epoch: 4
Loss: 0.13203195173798452
train: 0.801810	val: 0.750750	test: 0.762201

Epoch: 5
Loss: 0.12791132124915
train: 0.815003	val: 0.775996	test: 0.746888

Epoch: 6
Loss: 0.1279288669294355
train: 0.824133	val: 0.754213	test: 0.758072

Epoch: 7
Loss: 0.12562623719029728
train: 0.814195	val: 0.763071	test: 0.700790

Epoch: 8
Loss: 0.12344626746517101
train: 0.834692	val: 0.756963	test: 0.758072

Epoch: 9
Loss: 0.12185357311864989
train: 0.833180	val: 0.750554	test: 0.769754

Epoch: 10
Loss: 0.12238921330644904
train: 0.838996	val: 0.754823	test: 0.753288

Epoch: 11
Loss: 0.11985276018277415
train: 0.854203	val: 0.775791	test: 0.738338

Epoch: 12
Loss: 0.12038333826194547
train: 0.842713	val: 0.763319	test: 0.742116

Epoch: 13
Loss: 0.11803086909864763
train: 0.852556	val: 0.772453	test: 0.770364

Epoch: 14
Loss: 0.11633506529080981
train: 0.860538	val: 0.763641	test: 0.749682

Epoch: 15
Loss: 0.11485151577660298
train: 0.861214	val: 0.767082	test: 0.765432

Epoch: 16
Loss: 0.11492025883317884
train: 0.851102	val: 0.722066	test: 0.743933

Epoch: 17
Loss: 0.1123031376954484
train: 0.873733	val: 0.771776	test: 0.731963

Epoch: 18
Loss: 0.11180604161174454
train: 0.865233	val: 0.766286	test: 0.741642

Epoch: 19
Loss: 0.11204136001207117
train: 0.878318	val: 0.778280	test: 0.755364

Epoch: 20
Loss: 0.10980224524331142
train: 0.882757	val: 0.803544	test: 0.757705

Epoch: 21
Loss: 0.11085632000535249
train: 0.883786	val: 0.788139	test: 0.753551

Epoch: 22
Loss: 0.10937089602740249
train: 0.886610	val: 0.773255	test: 0.753566

Epoch: 23
Loss: 0.1097158670098173
train: 0.884328	val: 0.804478	test: 0.755447

Epoch: 24
Loss: 0.1083255958741686
train: 0.895974	val: 0.786908	test: 0.751318

Epoch: 25
Loss: 0.10809208038498995
train: 0.894752	val: 0.788694	test: 0.759292

Epoch: 26
Loss: 0.10732098122286392
train: 0.897026	val: 0.796431	test: 0.756660

Epoch: 27
Loss: 0.10545916905728801
train: 0.901379	val: 0.788167	test: 0.766517

Epoch: 28
Loss: 0.10562906781502535
train: 0.900059	val: 0.787043	test: 0.749408

Epoch: 29
Loss: 0.10515993105023266
train: 0.894304	val: 0.787509	test: 0.745007

Epoch: 30
Loss: 0.10412333647667271
train: 0.905735	val: 0.784609	test: 0.746586

Epoch: 31
Loss: 0.10405067340252275
train: 0.907463	val: 0.800167	test: 0.764764

Epoch: 32
Loss: 0.10410890599707033
train: 0.905121	val: 0.775350	test: 0.752392

Epoch: 33
Loss: 0.10280022988190692
train: 0.910714	val: 0.791654	test: 0.749897

Epoch: 34
Loss: 0.1026127459761509
train: 0.909442	val: 0.793207	test: 0.753864

Epoch: 35
Loss: 0.10111931091319595
train: 0.909339	val: 0.770013	test: 0.748184

Epoch: 36
Loss: 0.09984039671226445
train: 0.917237	val: 0.765077	test: 0.741409

Epoch: 37
Loss: 0.10053371551292233
train: 0.914373	val: 0.775517	test: 0.742550

Epoch: 38
Loss: 0.10161268233843516
train: 0.919408	val: 0.778304	test: 0.754074

Epoch: 39
Loss: 0.10069479538737752
train: 0.922367	val: 0.764777	test: 0.740563

Epoch: 40
Loss: 0.09671336826487413
train: 0.922475	val: 0.778448	test: 0.758298

Epoch: 41
Loss: 0.10019089263197979
train: 0.918364	val: 0.754112	test: 0.769576

Epoch: 42
Loss: 0.09908210373764173
train: 0.921003	val: 0.773810	test: 0.762622

Epoch: 43
Loss: 0.09849665230543753
train: 0.930016	val: 0.775393	test: 0.758717

Epoch: 44
Loss: 0.09664070333877196
train: 0.929327	val: 0.757235	test: 0.760635

Epoch: 45
Loss: 0.0975638074436839
train: 0.926269	val: 0.767159	test: 0.743767

Epoch: 46
Loss: 0.0984646777836026
train: 0.926865	val: 0.758500	test: 0.771456

Epoch: 47
Loss: 0.09634129369136611
train: 0.930389	val: 0.780849	test: 0.770270

Epoch: 48
Loss: 0.09644468470548616
train: 0.930004	val: 0.776336	test: 0.756691

Epoch: 49
Loss: 0.09581265082636747
train: 0.932764	val: 0.785390	test: 0.746295

Epoch: 50
Loss: 0.09567358117609503
train: 0.931818	val: 0.784419	test: 0.764088

Epoch: 51
Loss: 0.09418875437983912
train: 0.932916	val: 0.745747	test: 0.757095

Epoch: 52
Loss: 0.09472588590204445
train: 0.936115	val: 0.781869	test: 0.761716

Epoch: 53
Loss: 0.09466730545552493
train: 0.938480	val: 0.775497	test: 0.753321

Epoch: 54
Loss: 0.09513965867745978
train: 0.940426	val: 0.764370	test: 0.751816

Epoch: 55
Loss: 0.09287544333557445
train: 0.937886	val: 0.768405	test: 0.748682

Epoch: 56
Loss: 0.09227186603521523
train: 0.944053	val: 0.765861	test: 0.761836

Epoch: 57
Loss: 0.09258774326809575
train: 0.941292	val: 0.749446	test: 0.731082

Epoch: 58
Loss: 0.09197690032228997
train: 0.944420	val: 0.759795	test: 0.764119

Epoch: 59
Loss: 0.09193701481866176
train: 0.947534	val: 0.761770	test: 0.755785

Epoch: 60
Loss: 0.09252452685413382
train: 0.946327	val: 0.774302	test: 0.755069

Epoch: 61
Loss: 0.09141561813277084
train: 0.949875	val: 0.757618	test: 0.751706

Epoch: 62
Loss: 0.09024822226857394
train: 0.948601	val: 0.780111	test: 0.757479

Epoch: 63
Loss: 0.09114353277576916
train: 0.949617	val: 0.777003	test: 0.750464

Epoch: 64
Loss: 0.08999306936578258
train: 0.952316	val: 0.765454	test: 0.763562

Epoch: 65
Loss: 0.09010364513015806
train: 0.954278	val: 0.765772	test: 0.764090

Epoch: 66
Loss: 0.08984337724999346
train: 0.952845	val: 0.771320	test: 0.760001

Epoch: 67
Loss: 0.08874557964084683
train: 0.948248	val: 0.774156	test: 0.730107

Epoch: 68
Loss: 0.08794120959244107
train: 0.953425	val: 0.789511	test: 0.747989

Epoch: 69
Loss: 0.08753972699855059
train: 0.953232	val: 0.777949	test: 0.759534

Epoch: 70
Loss: 0.08724729386842181
train: 0.951841	val: 0.775353	test: 0.747550

Epoch: 71
Loss: 0.08714467122542251
train: 0.958228	val: 0.784557	test: 0.750814

Epoch: 72
Loss: 0.08673178516344368
train: 0.957824	val: 0.792490	test: 0.761610

Epoch: 73
Loss: 0.08686403572643049
train: 0.958098	val: 0.771648	test: 0.760971

Epoch: 74
Loss: 0.08675760133935964
train: 0.955801	val: 0.779563	test: 0.745714

Epoch: 75
Loss: 0.08612226481845754
train: 0.959499	val: 0.761164	test: 0.755018

Epoch: 76
Loss: 0.08574956058398123
train: 0.956602	val: 0.754238	test: 0.758655

Epoch: 77
Loss: 0.08582634198736887
train: 0.960792	val: 0.780117	test: 0.750509

Epoch: 78
Loss: 0.08405460848978141
train: 0.957961	val: 0.770876	test: 0.765318

Epoch: 79
Loss: 0.08542860534975268
train: 0.959241	val: 0.773773	test: 0.758141

Epoch: 80
Loss: 0.0832421999289204
train: 0.961794	val: 0.780815	test: 0.754188

Epoch: 81
Loss: 0.08460205298622864
train: 0.961767	val: 0.778895	test: 0.748122

Epoch: 82
Loss: 0.08370969659616653
train: 0.961772	val: 0.784618	test: 0.759675

Epoch: 83
Loss: 0.08492439341528496
train: 0.967301	val: 0.774606	test: 0.759879

Epoch: 84
Loss: 0.0809416614422191
train: 0.966511	val: 0.783161	test: 0.750575

Epoch: 85
Loss: 0.08216056905069324
train: 0.964565	val: 0.785393	test: 0.752780

Epoch: 86
Loss: 0.08219810695869505
train: 0.970004	val: 0.781109	test: 0.753000

Epoch: 87
Loss: 0.08177117010099817
train: 0.969335	val: 0.784284	test: 0.763912

Epoch: 88
Loss: 0.08229806662605227
train: 0.968338	val: 0.790096	test: 0.759532

Epoch: 89
Loss: 0.08150552512127092
train: 0.968790	val: 0.765778	test: 0.747853

Epoch: 90
Loss: 0.08126096901132598
train: 0.968474	val: 0.765965	test: 0.770762

Epoch: 91
Loss: 0.08028883554862776
train: 0.967630	val: 0.781590	test: 0.751162

Epoch: 92
Loss: 0.08096564617227842
train: 0.967407	val: 0.780460	test: 0.748639

Epoch: 93
Loss: 0.08021798967733229
train: 0.970286	val: 0.800666	test: 0.757631

Epoch: 94
Loss: 0.08064225825203422
train: 0.969089	val: 0.779532	test: 0.754957

Epoch: 95
Loss: 0.08107127446675463
train: 0.974113	val: 0.788984	test: 0.757989

Epoch: 96
Loss: 0.07828329458449737
train: 0.973627	val: 0.781823	test: 0.755003

Epoch: 97
Loss: 0.0782564508699594
train: 0.973868	val: 0.779260	test: 0.732619

Epoch: 98
Loss: 0.07808507991227193
train: 0.972082	val: 0.772009	test: 0.732824

Epoch: 99
Loss: 0.07662842898280074
train: 0.976636	val: 0.798666	test: 0.759719

Epoch: 100
Loss: 0.07929725369350837
train: 0.974621	val: 0.786942	test: 0.745080

best train: 0.884328	val: 0.804478	test: 0.755447
end
