11511809_1
--dataset=hiv --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.24393010221930866
train: 0.774112	val: 0.765882	test: 0.729607

Epoch: 2
Loss: 0.13624835510906108
train: 0.803685	val: 0.768228	test: 0.752991

Epoch: 3
Loss: 0.1312290148657028
train: 0.820216	val: 0.794435	test: 0.760791

Epoch: 4
Loss: 0.12787482154496388
train: 0.822020	val: 0.729660	test: 0.737714

Epoch: 5
Loss: 0.1251562877034308
train: 0.845205	val: 0.804496	test: 0.753025

Epoch: 6
Loss: 0.12266074056299732
train: 0.843392	val: 0.779431	test: 0.744221

Epoch: 7
Loss: 0.1214032952822786
train: 0.836918	val: 0.800179	test: 0.742237

Epoch: 8
Loss: 0.12006385112876519
train: 0.859256	val: 0.785922	test: 0.759756

Epoch: 9
Loss: 0.11820461633550405
train: 0.865452	val: 0.799873	test: 0.763713

Epoch: 10
Loss: 0.11687247595995365
train: 0.866241	val: 0.779658	test: 0.756788

Epoch: 11
Loss: 0.11605310361057723
train: 0.875563	val: 0.797460	test: 0.753620

Epoch: 12
Loss: 0.11488166270960748
train: 0.865831	val: 0.755303	test: 0.725537

Epoch: 13
Loss: 0.11341952657721105
train: 0.879254	val: 0.780555	test: 0.753218

Epoch: 14
Loss: 0.11208767384092637
train: 0.874008	val: 0.781262	test: 0.733763

Epoch: 15
Loss: 0.11241775785218194
train: 0.886146	val: 0.796670	test: 0.763199

Epoch: 16
Loss: 0.11202614752014285
train: 0.886255	val: 0.778326	test: 0.739493

Epoch: 17
Loss: 0.10890408449823301
train: 0.891679	val: 0.812270	test: 0.750534

Epoch: 18
Loss: 0.10886895936320234
train: 0.875374	val: 0.748968	test: 0.728768

Epoch: 19
Loss: 0.1091416307869691
train: 0.888256	val: 0.755101	test: 0.731184

Epoch: 20
Loss: 0.10696923017396343
train: 0.904356	val: 0.793170	test: 0.745321

Epoch: 21
Loss: 0.10691280959252393
train: 0.899909	val: 0.786180	test: 0.742630

Epoch: 22
Loss: 0.10686566292922738
train: 0.908946	val: 0.801600	test: 0.751573

Epoch: 23
Loss: 0.10496721879464768
train: 0.909825	val: 0.818128	test: 0.745511

Epoch: 24
Loss: 0.1029995699631976
train: 0.910892	val: 0.794135	test: 0.741345

Epoch: 25
Loss: 0.10490870839149047
train: 0.909944	val: 0.776657	test: 0.754478

Epoch: 26
Loss: 0.10322765002584396
train: 0.910804	val: 0.767649	test: 0.747494

Epoch: 27
Loss: 0.10249517602916904
train: 0.918850	val: 0.813838	test: 0.759822

Epoch: 28
Loss: 0.10100102644985003
train: 0.919731	val: 0.788724	test: 0.753261

Epoch: 29
Loss: 0.10219295876791072
train: 0.908848	val: 0.791961	test: 0.741834

Epoch: 30
Loss: 0.1009997667769966
train: 0.925947	val: 0.804671	test: 0.762558

Epoch: 31
Loss: 0.10004864269382294
train: 0.923153	val: 0.809138	test: 0.764630

Epoch: 32
Loss: 0.0986792896558707
train: 0.922511	val: 0.791305	test: 0.758672

Epoch: 33
Loss: 0.10010404029094348
train: 0.922939	val: 0.785258	test: 0.745854

Epoch: 34
Loss: 0.0982702371125109
train: 0.924478	val: 0.802980	test: 0.759534

Epoch: 35
Loss: 0.09841634134680974
train: 0.933367	val: 0.789909	test: 0.757433

Epoch: 36
Loss: 0.09737397860652576
train: 0.931768	val: 0.780310	test: 0.764729

Epoch: 37
Loss: 0.09525351866497168
train: 0.937117	val: 0.810035	test: 0.770471

Epoch: 38
Loss: 0.09708927481213134
train: 0.933192	val: 0.807316	test: 0.763678

Epoch: 39
Loss: 0.0955946110573857
train: 0.934704	val: 0.783360	test: 0.739307

Epoch: 40
Loss: 0.09650572209709661
train: 0.939326	val: 0.798084	test: 0.761123

Epoch: 41
Loss: 0.09636899561995654
train: 0.938314	val: 0.793286	test: 0.758765

Epoch: 42
Loss: 0.09504654250640857
train: 0.942849	val: 0.803271	test: 0.757293

Epoch: 43
Loss: 0.09393388361782101
train: 0.930337	val: 0.785922	test: 0.747098

Epoch: 44
Loss: 0.09462446421764355
train: 0.940735	val: 0.792806	test: 0.750105

Epoch: 45
Loss: 0.09357671489844491
train: 0.943454	val: 0.802561	test: 0.745134

Epoch: 46
Loss: 0.09278762895003555
train: 0.942927	val: 0.798905	test: 0.754765

Epoch: 47
Loss: 0.09146432466435456
train: 0.941653	val: 0.790702	test: 0.774436

Epoch: 48
Loss: 0.09077189208603675
train: 0.950542	val: 0.804845	test: 0.751834

Epoch: 49
Loss: 0.09263064397538388
train: 0.946099	val: 0.791884	test: 0.747919

Epoch: 50
Loss: 0.0906124466174469
train: 0.944900	val: 0.807414	test: 0.752552

Epoch: 51
Loss: 0.09170260394901195
train: 0.949718	val: 0.804753	test: 0.749024

Epoch: 52
Loss: 0.0890526232417047
train: 0.947748	val: 0.796511	test: 0.767213

Epoch: 53
Loss: 0.08982852539698544
train: 0.951278	val: 0.823287	test: 0.771892

Epoch: 54
Loss: 0.08919136135921332
train: 0.959195	val: 0.810280	test: 0.778816

Epoch: 55
Loss: 0.08810587177655482
train: 0.955830	val: 0.798844	test: 0.760306

Epoch: 56
Loss: 0.08825277367381919
train: 0.959985	val: 0.801600	test: 0.776568

Epoch: 57
Loss: 0.08746945361400094
train: 0.961236	val: 0.795215	test: 0.757085

Epoch: 58
Loss: 0.08660107361536168
train: 0.959206	val: 0.802843	test: 0.769227

Epoch: 59
Loss: 0.08687523655256069
train: 0.961340	val: 0.786526	test: 0.754032

Epoch: 60
Loss: 0.08664263227548158
train: 0.963564	val: 0.793118	test: 0.759495

Epoch: 61
Loss: 0.08681720591421878
train: 0.961398	val: 0.806790	test: 0.761031

Epoch: 62
Loss: 0.08613951538695443
train: 0.960744	val: 0.793397	test: 0.763522

Epoch: 63
Loss: 0.08439499242420022
train: 0.962814	val: 0.799700	test: 0.758385

Epoch: 64
Loss: 0.08412889379109864
train: 0.967348	val: 0.805993	test: 0.746602

Epoch: 65
Loss: 0.08461207808931537
train: 0.966054	val: 0.786195	test: 0.768402

Epoch: 66
Loss: 0.08309510702273405
train: 0.969344	val: 0.797567	test: 0.768381

Epoch: 67
Loss: 0.08258495447891454
train: 0.965438	val: 0.777698	test: 0.766096

Epoch: 68
Loss: 0.08344005682822492
train: 0.966414	val: 0.802157	test: 0.747751

Epoch: 69
Loss: 0.08260726658347273
train: 0.967966	val: 0.787701	test: 0.763966

Epoch: 70
Loss: 0.08151887072822796
train: 0.969466	val: 0.806823	test: 0.773503

Epoch: 71
Loss: 0.08166457785488089
train: 0.973408	val: 0.789073	test: 0.751882

Epoch: 72
Loss: 0.08152398388580269
train: 0.973964	val: 0.800307	test: 0.773252

Epoch: 73
Loss: 0.07924564434920058
train: 0.970493	val: 0.779244	test: 0.771149

Epoch: 74
Loss: 0.07953220727361164
train: 0.972904	val: 0.809806	test: 0.769057

Epoch: 75
Loss: 0.07991283022258533
train: 0.972046	val: 0.787766	test: 0.759148

Epoch: 76
Loss: 0.07836919381915197
train: 0.971604	val: 0.796244	test: 0.770770

Epoch: 77
Loss: 0.07871561924269997
train: 0.972619	val: 0.794346	test: 0.752550

Epoch: 78
Loss: 0.08006792388420561
train: 0.973796	val: 0.810847	test: 0.752479

Epoch: 79
Loss: 0.07928634482501908
train: 0.976067	val: 0.807818	test: 0.764146

Epoch: 80
Loss: 0.07779077013742682
train: 0.978589	val: 0.808706	test: 0.765590

Epoch: 81
Loss: 0.07649607058076331
train: 0.973954	val: 0.808229	test: 0.747710

Epoch: 82
Loss: 0.07755462675332808
train: 0.975610	val: 0.806884	test: 0.750582

Epoch: 83
Loss: 0.07696831679034657
train: 0.977450	val: 0.819328	test: 0.757743

Epoch: 84
Loss: 0.07636009209649819
train: 0.975074	val: 0.807028	test: 0.769478

Epoch: 85
Loss: 0.07681781442211856
train: 0.977080	val: 0.792353	test: 0.751038

Epoch: 86
Loss: 0.07579641678751046
train: 0.979847	val: 0.804404	test: 0.757375

Epoch: 87
Loss: 0.0745613386391132
train: 0.980089	val: 0.787980	test: 0.774295

Epoch: 88
Loss: 0.07600380748742443
train: 0.980617	val: 0.809512	test: 0.772850

Epoch: 89
Loss: 0.07325358457091513
train: 0.980765	val: 0.793773	test: 0.771515

Epoch: 90
Loss: 0.07459927770203471
train: 0.980156	val: 0.778975	test: 0.772130

Epoch: 91
Loss: 0.07586117868710762
train: 0.982381	val: 0.800255	test: 0.763255

Epoch: 92
Loss: 0.071157581338468
train: 0.981912	val: 0.801211	test: 0.748751

Epoch: 93
Loss: 0.07370647991445331
train: 0.982469	val: 0.810103	test: 0.764107

Epoch: 94
Loss: 0.07179741619632284
train: 0.983119	val: 0.818673	test: 0.760326

Epoch: 95
Loss: 0.07448804880375277
train: 0.981241	val: 0.804772	test: 0.734935

Epoch: 96
Loss: 0.07296687814120578
train: 0.984152	val: 0.809487	test: 0.754514

Epoch: 97
Loss: 0.07244818256379129
train: 0.982439	val: 0.827696	test: 0.757996

Epoch: 98
Loss: 0.07211274644931061
train: 0.984646	val: 0.791612	test: 0.746668

Epoch: 99
Loss: 0.07066184847186512
train: 0.984374	val: 0.825452	test: 0.775446

Epoch: 100
Loss: 0.07158795876994345
train: 0.985587	val: 0.810130	test: 0.761950

best train: 0.982439	val: 0.827696	test: 0.757996
end
