11511809_1
--dataset=sider --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6934942303302531
train: 0.516565	val: 0.523902	test: 0.491049

Epoch: 2
Loss: 0.6476405401362405
train: 0.555009	val: 0.513028	test: 0.520299

Epoch: 3
Loss: 0.6122994973489428
train: 0.567386	val: 0.502099	test: 0.525090

Epoch: 4
Loss: 0.5795186172235949
train: 0.590243	val: 0.517599	test: 0.539715

Epoch: 5
Loss: 0.5598135586873847
train: 0.630723	val: 0.550931	test: 0.568780

Epoch: 6
Loss: 0.5423206384341216
train: 0.651848	val: 0.572822	test: 0.583393

Epoch: 7
Loss: 0.5310829676121775
train: 0.662823	val: 0.571850	test: 0.590794

Epoch: 8
Loss: 0.5216413046104492
train: 0.671999	val: 0.574917	test: 0.589845

Epoch: 9
Loss: 0.5144321717986465
train: 0.682057	val: 0.582419	test: 0.588366

Epoch: 10
Loss: 0.5052342607136933
train: 0.693891	val: 0.587741	test: 0.591850

Epoch: 11
Loss: 0.49895183209595634
train: 0.703105	val: 0.597215	test: 0.594451

Epoch: 12
Loss: 0.4970314055080937
train: 0.714321	val: 0.596118	test: 0.601690

Epoch: 13
Loss: 0.4862939914279806
train: 0.719008	val: 0.590955	test: 0.599041

Epoch: 14
Loss: 0.48582533449439336
train: 0.721941	val: 0.597767	test: 0.599783

Epoch: 15
Loss: 0.4814553474399146
train: 0.730310	val: 0.609330	test: 0.607134

Epoch: 16
Loss: 0.4772097875910646
train: 0.737008	val: 0.618686	test: 0.602480

Epoch: 17
Loss: 0.4726189700246028
train: 0.746512	val: 0.601598	test: 0.612497

Epoch: 18
Loss: 0.4728161613263374
train: 0.751773	val: 0.595119	test: 0.614566

Epoch: 19
Loss: 0.46834712795428135
train: 0.755942	val: 0.603017	test: 0.612098

Epoch: 20
Loss: 0.46320796788200014
train: 0.757815	val: 0.608321	test: 0.605189

Epoch: 21
Loss: 0.4630426727834688
train: 0.759851	val: 0.607742	test: 0.600653

Epoch: 22
Loss: 0.45666867408127604
train: 0.766214	val: 0.611733	test: 0.594175

Epoch: 23
Loss: 0.45728682949899274
train: 0.772882	val: 0.612331	test: 0.598553

Epoch: 24
Loss: 0.4606883578697002
train: 0.778082	val: 0.611392	test: 0.600573

Epoch: 25
Loss: 0.4529847852588048
train: 0.782253	val: 0.608820	test: 0.600092

Epoch: 26
Loss: 0.4524702183683008
train: 0.789123	val: 0.620421	test: 0.597853

Epoch: 27
Loss: 0.4486966296603846
train: 0.788365	val: 0.612636	test: 0.599462

Epoch: 28
Loss: 0.44262182344488793
train: 0.791817	val: 0.614818	test: 0.602164

Epoch: 29
Loss: 0.4455491438432621
train: 0.792596	val: 0.613140	test: 0.602784

Epoch: 30
Loss: 0.44579070523952524
train: 0.793349	val: 0.600670	test: 0.601857

Epoch: 31
Loss: 0.4419238502685956
train: 0.796432	val: 0.598743	test: 0.598515

Epoch: 32
Loss: 0.44540807364758717
train: 0.806558	val: 0.617614	test: 0.598367

Epoch: 33
Loss: 0.4391792591979504
train: 0.804522	val: 0.613331	test: 0.592272

Epoch: 34
Loss: 0.44108769970862677
train: 0.814103	val: 0.611332	test: 0.588500

Epoch: 35
Loss: 0.4346776123728066
train: 0.810245	val: 0.609333	test: 0.588031

Epoch: 36
Loss: 0.4347874554744548
train: 0.816384	val: 0.607154	test: 0.592227

Epoch: 37
Loss: 0.43042398208907723
train: 0.820115	val: 0.605948	test: 0.598135

Epoch: 38
Loss: 0.43712757256365703
train: 0.818278	val: 0.602776	test: 0.593602

Epoch: 39
Loss: 0.42830691153890765
train: 0.824342	val: 0.608384	test: 0.600758

Epoch: 40
Loss: 0.4280090627172525
train: 0.826539	val: 0.620451	test: 0.593325

Epoch: 41
Loss: 0.42288581450486246
train: 0.828408	val: 0.611212	test: 0.589273

Epoch: 42
Loss: 0.42948507961943844
train: 0.831339	val: 0.607637	test: 0.590569

Epoch: 43
Loss: 0.4202809386098913
train: 0.832491	val: 0.616077	test: 0.581014

Epoch: 44
Loss: 0.4210645752544475
train: 0.833822	val: 0.598970	test: 0.588324

Epoch: 45
Loss: 0.4188592290395407
train: 0.837647	val: 0.591219	test: 0.597158

Epoch: 46
Loss: 0.4217583323013388
train: 0.841048	val: 0.604239	test: 0.600313

Epoch: 47
Loss: 0.4154685172681639
train: 0.842393	val: 0.609031	test: 0.595289

Epoch: 48
Loss: 0.41593067223967334
train: 0.841966	val: 0.597702	test: 0.602579

Epoch: 49
Loss: 0.4177261676269294
train: 0.843996	val: 0.609803	test: 0.592976

Epoch: 50
Loss: 0.4076695417786869
train: 0.844896	val: 0.616342	test: 0.585492

Epoch: 51
Loss: 0.41143056214387774
train: 0.844465	val: 0.602797	test: 0.595078

Epoch: 52
Loss: 0.41292229829578747
train: 0.847105	val: 0.606572	test: 0.589613

Epoch: 53
Loss: 0.4128588654151216
train: 0.852707	val: 0.604424	test: 0.592797

Epoch: 54
Loss: 0.4045835330980606
train: 0.851380	val: 0.600056	test: 0.586685

Epoch: 55
Loss: 0.4098462908653057
train: 0.854649	val: 0.594495	test: 0.585084

Epoch: 56
Loss: 0.4028810599090985
train: 0.854563	val: 0.588846	test: 0.595038

Epoch: 57
Loss: 0.40443088042132064
train: 0.856454	val: 0.592524	test: 0.575435

Epoch: 58
Loss: 0.3974154212936794
train: 0.857758	val: 0.603218	test: 0.584087

Epoch: 59
Loss: 0.40604634621869196
train: 0.855113	val: 0.595295	test: 0.603089

Epoch: 60
Loss: 0.40112811359349176
train: 0.862374	val: 0.613666	test: 0.595738

Epoch: 61
Loss: 0.40543123857717295
train: 0.866216	val: 0.604043	test: 0.600511

Epoch: 62
Loss: 0.40160598815645365
train: 0.863902	val: 0.595975	test: 0.598884

Epoch: 63
Loss: 0.39584657361048814
train: 0.868679	val: 0.601189	test: 0.587988

Epoch: 64
Loss: 0.39343888061081567
train: 0.869195	val: 0.598045	test: 0.585928

Epoch: 65
Loss: 0.39553543902709815
train: 0.870569	val: 0.608767	test: 0.584982

Epoch: 66
Loss: 0.39251485814437237
train: 0.868823	val: 0.615687	test: 0.592796

Epoch: 67
Loss: 0.39610016537414705
train: 0.870764	val: 0.620626	test: 0.581698

Epoch: 68
Loss: 0.39575677001210174
train: 0.871145	val: 0.615197	test: 0.596562

Epoch: 69
Loss: 0.3962223241781637
train: 0.873117	val: 0.612669	test: 0.600896

Epoch: 70
Loss: 0.3922294925351554
train: 0.875631	val: 0.599946	test: 0.602375

Epoch: 71
Loss: 0.3913683365378644
train: 0.864938	val: 0.588698	test: 0.598059

Epoch: 72
Loss: 0.3844197223145144
train: 0.878208	val: 0.615329	test: 0.597990

Epoch: 73
Loss: 0.3843068273042802
train: 0.879233	val: 0.629961	test: 0.594955

Epoch: 74
Loss: 0.3862904967703662
train: 0.879108	val: 0.616104	test: 0.600487

Epoch: 75
Loss: 0.3828828084947607
train: 0.885398	val: 0.617209	test: 0.601116

Epoch: 76
Loss: 0.3839352860286673
train: 0.884758	val: 0.611957	test: 0.596510

Epoch: 77
Loss: 0.3804486206346736
train: 0.883187	val: 0.603495	test: 0.592658

Epoch: 78
Loss: 0.38575602185306634
train: 0.886313	val: 0.607859	test: 0.596969

Epoch: 79
Loss: 0.3759389181843467
train: 0.887499	val: 0.619788	test: 0.586572

Epoch: 80
Loss: 0.3767411330703668
train: 0.887713	val: 0.616827	test: 0.577802

Epoch: 81
Loss: 0.3751952645830603
train: 0.889016	val: 0.610803	test: 0.584139

Epoch: 82
Loss: 0.37639869960031713
train: 0.887119	val: 0.607971	test: 0.585883

Epoch: 83
Loss: 0.37775993025689336
train: 0.889463	val: 0.614202	test: 0.581783

Epoch: 84
Loss: 0.37445185273526393
train: 0.890802	val: 0.622584	test: 0.585500

Epoch: 85
Loss: 0.379573601470505
train: 0.893087	val: 0.616232	test: 0.597256

Epoch: 86
Loss: 0.3752096674020587
train: 0.889807	val: 0.609917	test: 0.600831

Epoch: 87
Loss: 0.37473194420038425
train: 0.895440	val: 0.621434	test: 0.590773

Epoch: 88
Loss: 0.36501105904191916
train: 0.896325	val: 0.626470	test: 0.581017

Epoch: 89
Loss: 0.37057048899221984
train: 0.897549	val: 0.616791	test: 0.594760

Epoch: 90
Loss: 0.3680224558316471
train: 0.896514	val: 0.617785	test: 0.603305

Epoch: 91
Loss: 0.36461168014964496
train: 0.898463	val: 0.628655	test: 0.595247

Epoch: 92
Loss: 0.36453585870210087
train: 0.901702	val: 0.623788	test: 0.589936

Epoch: 93
Loss: 0.36363526096608123
train: 0.899448	val: 0.617132	test: 0.589031

Epoch: 94
Loss: 0.36619488063168304
train: 0.899308	val: 0.619898	test: 0.585645

Epoch: 95
Loss: 0.3632038077611031
train: 0.901392	val: 0.623934	test: 0.579562

Epoch: 96
Loss: 0.3617399224009667
train: 0.904734	val: 0.629825	test: 0.585831

Epoch: 97
Loss: 0.36329990978198057
train: 0.906206	val: 0.616004	test: 0.585818

Epoch: 98
Loss: 0.35895748499636015
train: 0.905046	val: 0.614670	test: 0.583691

Epoch: 99
Loss: 0.3587024192125263
train: 0.905638	val: 0.620142	test: 0.588424

Epoch: 100
Loss: 0.3554840953421671
train: 0.908278	val: 0.620014	test: 0.582956

best train: 0.879233	val: 0.629961	test: 0.594955
end
