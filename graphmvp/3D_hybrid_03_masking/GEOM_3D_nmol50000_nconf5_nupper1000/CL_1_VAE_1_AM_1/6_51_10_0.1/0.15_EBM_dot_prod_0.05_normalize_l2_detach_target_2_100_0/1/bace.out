11511809_1
--dataset=bace --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.662701953527004
train: 0.724726	val: 0.627839	test: 0.591723

Epoch: 2
Loss: 0.6166898608265693
train: 0.799107	val: 0.617949	test: 0.635542

Epoch: 3
Loss: 0.5605841522185384
train: 0.842957	val: 0.684982	test: 0.727526

Epoch: 4
Loss: 0.517619215435597
train: 0.858836	val: 0.678388	test: 0.771866

Epoch: 5
Loss: 0.4781733603919148
train: 0.870029	val: 0.691941	test: 0.796035

Epoch: 6
Loss: 0.4735714690668016
train: 0.881861	val: 0.708425	test: 0.804556

Epoch: 7
Loss: 0.4513181687387401
train: 0.890071	val: 0.679853	test: 0.794644

Epoch: 8
Loss: 0.44494694955716
train: 0.901981	val: 0.706227	test: 0.799513

Epoch: 9
Loss: 0.42422573082003695
train: 0.907477	val: 0.728938	test: 0.801947

Epoch: 10
Loss: 0.4149781122206776
train: 0.910297	val: 0.719048	test: 0.791167

Epoch: 11
Loss: 0.41390727534812577
train: 0.911801	val: 0.699634	test: 0.798122

Epoch: 12
Loss: 0.40731668473192134
train: 0.914932	val: 0.693773	test: 0.802121

Epoch: 13
Loss: 0.3876104898334989
train: 0.918410	val: 0.695238	test: 0.806121

Epoch: 14
Loss: 0.39587392274780064
train: 0.920494	val: 0.698535	test: 0.814293

Epoch: 15
Loss: 0.3957543373754925
train: 0.922206	val: 0.712821	test: 0.818640

Epoch: 16
Loss: 0.3790992800223445
train: 0.924700	val: 0.730403	test: 0.816728

Epoch: 17
Loss: 0.38415840305350174
train: 0.926384	val: 0.724542	test: 0.795166

Epoch: 18
Loss: 0.3829413546585131
train: 0.928071	val: 0.709524	test: 0.786994

Epoch: 19
Loss: 0.38679009403569725
train: 0.929760	val: 0.701099	test: 0.800209

Epoch: 20
Loss: 0.37064098345192453
train: 0.931230	val: 0.696337	test: 0.802295

Epoch: 21
Loss: 0.3811845068937343
train: 0.932106	val: 0.712088	test: 0.805773

Epoch: 22
Loss: 0.36686225697989056
train: 0.933693	val: 0.716850	test: 0.801774

Epoch: 23
Loss: 0.3691218615248289
train: 0.936147	val: 0.724542	test: 0.790297

Epoch: 24
Loss: 0.3578300749774026
train: 0.938576	val: 0.736996	test: 0.793079

Epoch: 25
Loss: 0.36982646887825854
train: 0.938987	val: 0.724176	test: 0.801252

Epoch: 26
Loss: 0.3553969943020752
train: 0.940163	val: 0.704029	test: 0.796035

Epoch: 27
Loss: 0.34545395781200056
train: 0.939666	val: 0.713187	test: 0.803686

Epoch: 28
Loss: 0.35956204173195133
train: 0.940654	val: 0.699634	test: 0.801426

Epoch: 29
Loss: 0.35055377869325854
train: 0.941267	val: 0.701465	test: 0.792732

Epoch: 30
Loss: 0.3454771362055761
train: 0.945123	val: 0.696337	test: 0.789080

Epoch: 31
Loss: 0.33879304966430085
train: 0.946692	val: 0.689744	test: 0.800035

Epoch: 32
Loss: 0.3445146841121155
train: 0.948456	val: 0.712821	test: 0.795166

Epoch: 33
Loss: 0.3435607496912187
train: 0.947483	val: 0.739927	test: 0.784038

Epoch: 34
Loss: 0.3483731084285705
train: 0.947637	val: 0.733700	test: 0.787341

Epoch: 35
Loss: 0.31913360623480436
train: 0.950382	val: 0.712454	test: 0.791515

Epoch: 36
Loss: 0.34005012887293273
train: 0.951481	val: 0.704029	test: 0.800904

Epoch: 37
Loss: 0.32328121063176496
train: 0.952334	val: 0.693773	test: 0.798644

Epoch: 38
Loss: 0.328826268517905
train: 0.952977	val: 0.695238	test: 0.794818

Epoch: 39
Loss: 0.31363314461875025
train: 0.953485	val: 0.700733	test: 0.799339

Epoch: 40
Loss: 0.31260245236162987
train: 0.953165	val: 0.705128	test: 0.796383

Epoch: 41
Loss: 0.3187997167216131
train: 0.955979	val: 0.702930	test: 0.788732

Epoch: 42
Loss: 0.32564016448810734
train: 0.956153	val: 0.708791	test: 0.794644

Epoch: 43
Loss: 0.3123744781151624
train: 0.957574	val: 0.710623	test: 0.781255

Epoch: 44
Loss: 0.30986984006555196
train: 0.958353	val: 0.713553	test: 0.778821

Epoch: 45
Loss: 0.31383850623873344
train: 0.960211	val: 0.694872	test: 0.780212

Epoch: 46
Loss: 0.30263093763078586
train: 0.961530	val: 0.689011	test: 0.780386

Epoch: 47
Loss: 0.2881448711696956
train: 0.961518	val: 0.700366	test: 0.781951

Epoch: 48
Loss: 0.31241545734229575
train: 0.960599	val: 0.694139	test: 0.789254

Epoch: 49
Loss: 0.29728982051708924
train: 0.962058	val: 0.691575	test: 0.788559

Epoch: 50
Loss: 0.3026660768192607
train: 0.960788	val: 0.696703	test: 0.774474

Epoch: 51
Loss: 0.3046265490987291
train: 0.961030	val: 0.688278	test: 0.769953

Epoch: 52
Loss: 0.311456279841078
train: 0.961832	val: 0.688645	test: 0.769605

Epoch: 53
Loss: 0.31309579672126253
train: 0.963533	val: 0.704396	test: 0.773952

Epoch: 54
Loss: 0.28861419747875544
train: 0.964058	val: 0.704396	test: 0.777778

Epoch: 55
Loss: 0.3046716057503159
train: 0.965197	val: 0.696337	test: 0.780038

Epoch: 56
Loss: 0.29369817529464015
train: 0.965788	val: 0.693773	test: 0.774474

Epoch: 57
Loss: 0.2824157159375517
train: 0.965285	val: 0.700366	test: 0.781082

Epoch: 58
Loss: 0.2881567029097143
train: 0.967534	val: 0.695238	test: 0.779517

Epoch: 59
Loss: 0.29208675942695916
train: 0.968279	val: 0.697070	test: 0.780734

Epoch: 60
Loss: 0.2948946493885429
train: 0.968916	val: 0.687546	test: 0.784038

Epoch: 61
Loss: 0.27427251675243236
train: 0.968818	val: 0.682418	test: 0.785255

Epoch: 62
Loss: 0.2732268335289403
train: 0.969592	val: 0.698535	test: 0.782473

Epoch: 63
Loss: 0.27684615425125353
train: 0.972240	val: 0.676923	test: 0.779343

Epoch: 64
Loss: 0.28686979495496134
train: 0.970979	val: 0.683516	test: 0.768388

Epoch: 65
Loss: 0.26900941806427203
train: 0.970548	val: 0.720147	test: 0.776561

Epoch: 66
Loss: 0.3057296260326379
train: 0.970922	val: 0.697802	test: 0.772735

Epoch: 67
Loss: 0.27602912978665123
train: 0.971601	val: 0.675458	test: 0.781429

Epoch: 68
Loss: 0.2800659499225711
train: 0.971946	val: 0.704029	test: 0.785081

Epoch: 69
Loss: 0.26540841160975204
train: 0.972805	val: 0.680952	test: 0.765606

Epoch: 70
Loss: 0.2810011930066406
train: 0.973750	val: 0.677656	test: 0.767345

Epoch: 71
Loss: 0.26778185887147926
train: 0.973567	val: 0.688278	test: 0.771170

Epoch: 72
Loss: 0.2676295155453027
train: 0.972959	val: 0.701832	test: 0.784211

Epoch: 73
Loss: 0.27375817144618503
train: 0.972286	val: 0.696337	test: 0.791515

Epoch: 74
Loss: 0.25759460512241955
train: 0.974415	val: 0.706960	test: 0.786646

Epoch: 75
Loss: 0.2695168057002265
train: 0.976005	val: 0.697802	test: 0.778821

Epoch: 76
Loss: 0.2533299640838399
train: 0.976912	val: 0.684615	test: 0.770822

Epoch: 77
Loss: 0.2587362381867926
train: 0.977357	val: 0.684249	test: 0.765258

Epoch: 78
Loss: 0.26237006728989337
train: 0.977743	val: 0.697436	test: 0.771692

Epoch: 79
Loss: 0.2442892674671276
train: 0.978096	val: 0.699634	test: 0.755347

Epoch: 80
Loss: 0.24788507591177494
train: 0.980091	val: 0.691941	test: 0.749087

Epoch: 81
Loss: 0.24958092410791916
train: 0.977959	val: 0.679121	test: 0.751521

Epoch: 82
Loss: 0.23825668103409167
train: 0.979369	val: 0.691575	test: 0.746653

Epoch: 83
Loss: 0.2616859875714467
train: 0.979760	val: 0.676190	test: 0.754999

Epoch: 84
Loss: 0.25009281290834656
train: 0.981550	val: 0.663370	test: 0.768562

Epoch: 85
Loss: 0.24121256128302343
train: 0.981478	val: 0.673993	test: 0.767693

Epoch: 86
Loss: 0.2442928520095983
train: 0.979820	val: 0.697070	test: 0.763867

Epoch: 87
Loss: 0.23862977408558567
train: 0.979027	val: 0.703663	test: 0.771692

Epoch: 88
Loss: 0.2529583735236159
train: 0.979917	val: 0.678755	test: 0.757955

Epoch: 89
Loss: 0.26213898230394417
train: 0.979632	val: 0.684615	test: 0.751174

Epoch: 90
Loss: 0.253799583679155
train: 0.980768	val: 0.693407	test: 0.768040

Epoch: 91
Loss: 0.25036444200107877
train: 0.981079	val: 0.710989	test: 0.766823

Epoch: 92
Loss: 0.25170868797103696
train: 0.980596	val: 0.705128	test: 0.762824

Epoch: 93
Loss: 0.24495075759922366
train: 0.981110	val: 0.690476	test: 0.766475

Epoch: 94
Loss: 0.22793012562604736
train: 0.979712	val: 0.679121	test: 0.764041

Epoch: 95
Loss: 0.22111936069356924
train: 0.981941	val: 0.689011	test: 0.769779

Epoch: 96
Loss: 0.24553549814676373
train: 0.983328	val: 0.671062	test: 0.773431

Epoch: 97
Loss: 0.22745997900131582
train: 0.984278	val: 0.668864	test: 0.775517

Epoch: 98
Loss: 0.2268565917822401
train: 0.983547	val: 0.670330	test: 0.769084

Epoch: 99
Loss: 0.23102209395756837
train: 0.983676	val: 0.685714	test: 0.753434

Epoch: 100
Loss: 0.22138350715473595
train: 0.983496	val: 0.710256	test: 0.749783

best train: 0.947483	val: 0.739927	test: 0.784038
end
