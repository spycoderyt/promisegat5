9112376_2
--dataset=tox21 --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5253048572694835
train: 0.723928	val: 0.633159	test: 0.618131

Epoch: 2
Loss: 0.3333339512336901
train: 0.770311	val: 0.704115	test: 0.664306

Epoch: 3
Loss: 0.24042949929545696
train: 0.794997	val: 0.709127	test: 0.672215

Epoch: 4
Loss: 0.20627677360840407
train: 0.829459	val: 0.750695	test: 0.722598

Epoch: 5
Loss: 0.19348842892113827
train: 0.844131	val: 0.760935	test: 0.726945

Epoch: 6
Loss: 0.1882039300166659
train: 0.854233	val: 0.760757	test: 0.729823

Epoch: 7
Loss: 0.18316556172480852
train: 0.857241	val: 0.758412	test: 0.732793

Epoch: 8
Loss: 0.183118489348175
train: 0.862576	val: 0.767366	test: 0.729942

Epoch: 9
Loss: 0.17826428210387982
train: 0.868808	val: 0.774575	test: 0.738707

Epoch: 10
Loss: 0.17556589989807478
train: 0.871208	val: 0.765045	test: 0.740502

Epoch: 11
Loss: 0.1733550818983934
train: 0.877749	val: 0.769149	test: 0.747180

Epoch: 12
Loss: 0.1736089758727911
train: 0.877934	val: 0.761991	test: 0.750059

Epoch: 13
Loss: 0.17036722680231997
train: 0.884264	val: 0.767826	test: 0.729824

Epoch: 14
Loss: 0.1679460234217866
train: 0.884007	val: 0.778344	test: 0.758433

Epoch: 15
Loss: 0.16682237692726568
train: 0.890845	val: 0.780388	test: 0.749271

Epoch: 16
Loss: 0.16461614660017687
train: 0.891779	val: 0.778669	test: 0.761328

Epoch: 17
Loss: 0.16503647428391685
train: 0.894626	val: 0.776922	test: 0.757160

Epoch: 18
Loss: 0.16226155968563177
train: 0.898053	val: 0.784585	test: 0.763786

Epoch: 19
Loss: 0.16193619509611828
train: 0.898885	val: 0.776956	test: 0.752597

Epoch: 20
Loss: 0.1601990012080022
train: 0.904263	val: 0.785149	test: 0.752008

Epoch: 21
Loss: 0.16189358489026695
train: 0.904147	val: 0.787078	test: 0.761207

Epoch: 22
Loss: 0.16023971926215327
train: 0.907961	val: 0.785994	test: 0.763935

Epoch: 23
Loss: 0.15610681309155272
train: 0.907976	val: 0.777714	test: 0.757955

Epoch: 24
Loss: 0.15822177105221857
train: 0.910879	val: 0.779471	test: 0.753028

Epoch: 25
Loss: 0.15485503569747863
train: 0.914186	val: 0.796719	test: 0.766244

Epoch: 26
Loss: 0.15301513801157077
train: 0.916908	val: 0.785080	test: 0.754742

Epoch: 27
Loss: 0.15544241448160004
train: 0.914799	val: 0.783465	test: 0.764574

Epoch: 28
Loss: 0.15341447507070236
train: 0.915744	val: 0.779309	test: 0.764532

Epoch: 29
Loss: 0.15457223312788373
train: 0.915839	val: 0.786645	test: 0.753335

Epoch: 30
Loss: 0.15109665461198774
train: 0.919887	val: 0.781579	test: 0.761611

Epoch: 31
Loss: 0.15019834515559588
train: 0.924571	val: 0.787600	test: 0.757538

Epoch: 32
Loss: 0.14939628442567796
train: 0.925186	val: 0.774891	test: 0.756721

Epoch: 33
Loss: 0.14888880546830896
train: 0.926929	val: 0.774768	test: 0.745007

Epoch: 34
Loss: 0.1490720934572282
train: 0.929610	val: 0.782383	test: 0.753720

Epoch: 35
Loss: 0.14714007869057924
train: 0.931127	val: 0.782679	test: 0.767561

Epoch: 36
Loss: 0.14436968282304202
train: 0.931341	val: 0.783681	test: 0.752267

Epoch: 37
Loss: 0.14577190676468385
train: 0.933225	val: 0.782259	test: 0.762970

Epoch: 38
Loss: 0.14412310018270055
train: 0.935622	val: 0.784483	test: 0.759164

Epoch: 39
Loss: 0.1429996546572771
train: 0.935176	val: 0.780277	test: 0.758589

Epoch: 40
Loss: 0.1437764078927033
train: 0.937350	val: 0.777240	test: 0.757277

Epoch: 41
Loss: 0.13921096257587195
train: 0.940446	val: 0.777995	test: 0.753937

Epoch: 42
Loss: 0.14107390034285155
train: 0.939784	val: 0.781131	test: 0.760122

Epoch: 43
Loss: 0.1410310976793274
train: 0.942390	val: 0.784483	test: 0.754241

Epoch: 44
Loss: 0.13949649253122687
train: 0.943718	val: 0.775701	test: 0.753865

Epoch: 45
Loss: 0.137857437788561
train: 0.940124	val: 0.763267	test: 0.747214

Epoch: 46
Loss: 0.13748764501154748
train: 0.943898	val: 0.778194	test: 0.753041

Epoch: 47
Loss: 0.13616746731464777
train: 0.946726	val: 0.783715	test: 0.753653

Epoch: 48
Loss: 0.13807701949735973
train: 0.948423	val: 0.781162	test: 0.762835

Epoch: 49
Loss: 0.13391190542193143
train: 0.950084	val: 0.779306	test: 0.750654

Epoch: 50
Loss: 0.134542779854267
train: 0.950166	val: 0.788169	test: 0.756917

Epoch: 51
Loss: 0.13403134856104643
train: 0.950852	val: 0.769882	test: 0.746677

Epoch: 52
Loss: 0.13273097050767174
train: 0.951352	val: 0.783275	test: 0.753877

Epoch: 53
Loss: 0.13477819721090015
train: 0.950510	val: 0.773570	test: 0.755307

Epoch: 54
Loss: 0.13268920405472207
train: 0.953837	val: 0.774637	test: 0.757252

Epoch: 55
Loss: 0.13060933599576413
train: 0.956299	val: 0.776662	test: 0.755819

Epoch: 56
Loss: 0.12912417159673495
train: 0.954269	val: 0.772104	test: 0.762129

Epoch: 57
Loss: 0.12774584138758763
train: 0.957288	val: 0.770360	test: 0.760859

Epoch: 58
Loss: 0.12893519053227162
train: 0.958518	val: 0.775893	test: 0.752305

Epoch: 59
Loss: 0.12638398455964556
train: 0.958967	val: 0.764022	test: 0.749286

Epoch: 60
Loss: 0.12627727236789996
train: 0.957684	val: 0.776636	test: 0.750192

Epoch: 61
Loss: 0.12711821815505067
train: 0.960024	val: 0.777191	test: 0.761239

Epoch: 62
Loss: 0.12743656184384383
train: 0.961792	val: 0.783193	test: 0.754802

Epoch: 63
Loss: 0.12260596118891019
train: 0.962942	val: 0.774418	test: 0.752649

Epoch: 64
Loss: 0.12431817666754964
train: 0.963485	val: 0.777067	test: 0.748739

Epoch: 65
Loss: 0.12413671984919368
train: 0.962459	val: 0.781923	test: 0.756607

Epoch: 66
Loss: 0.12373023825954586
train: 0.964222	val: 0.774463	test: 0.747273

Epoch: 67
Loss: 0.12103998551335597
train: 0.964007	val: 0.778719	test: 0.754925

Epoch: 68
Loss: 0.12075731432645945
train: 0.964716	val: 0.770629	test: 0.752375

Epoch: 69
Loss: 0.12081048493307338
train: 0.967198	val: 0.772202	test: 0.747147

Epoch: 70
Loss: 0.11925182005859142
train: 0.967321	val: 0.777201	test: 0.751022

Epoch: 71
Loss: 0.12098545696046077
train: 0.968887	val: 0.777275	test: 0.754522

Epoch: 72
Loss: 0.11793950812727669
train: 0.968153	val: 0.786036	test: 0.752444

Epoch: 73
Loss: 0.11931222604087281
train: 0.968882	val: 0.780936	test: 0.755096

Epoch: 74
Loss: 0.11659846922482166
train: 0.970925	val: 0.779259	test: 0.749853

Epoch: 75
Loss: 0.1178246866940417
train: 0.970260	val: 0.777717	test: 0.752466

Epoch: 76
Loss: 0.11566225929362202
train: 0.971162	val: 0.779028	test: 0.752565

Epoch: 77
Loss: 0.11537673701593251
train: 0.972071	val: 0.775544	test: 0.747695

Epoch: 78
Loss: 0.11649925041971426
train: 0.970798	val: 0.774606	test: 0.757900

Epoch: 79
Loss: 0.11421866447311325
train: 0.972485	val: 0.770877	test: 0.742751

Epoch: 80
Loss: 0.11339842616472857
train: 0.972799	val: 0.783929	test: 0.749588

Epoch: 81
Loss: 0.1144364077584406
train: 0.973223	val: 0.781668	test: 0.740620

Epoch: 82
Loss: 0.11436901849314214
train: 0.974128	val: 0.776102	test: 0.750201

Epoch: 83
Loss: 0.11478658923799555
train: 0.974005	val: 0.773453	test: 0.744133

Epoch: 84
Loss: 0.11044292076554495
train: 0.975947	val: 0.775184	test: 0.746297

Epoch: 85
Loss: 0.11128005322087998
train: 0.975471	val: 0.777287	test: 0.750920

Epoch: 86
Loss: 0.11074725288126334
train: 0.977352	val: 0.770471	test: 0.744770

Epoch: 87
Loss: 0.11011003202297219
train: 0.975876	val: 0.775815	test: 0.747392

Epoch: 88
Loss: 0.11089708525556213
train: 0.976938	val: 0.768713	test: 0.741757

Epoch: 89
Loss: 0.11143875182256513
train: 0.977187	val: 0.772025	test: 0.749629

Epoch: 90
Loss: 0.10757088879475592
train: 0.976883	val: 0.767928	test: 0.744056

Epoch: 91
Loss: 0.10832552344952365
train: 0.978068	val: 0.768474	test: 0.746187

Epoch: 92
Loss: 0.10695669932073582
train: 0.978547	val: 0.767316	test: 0.742604

Epoch: 93
Loss: 0.10585248072292802
train: 0.978791	val: 0.752424	test: 0.747176

Epoch: 94
Loss: 0.10570466758820803
train: 0.979843	val: 0.762530	test: 0.734054

Epoch: 95
Loss: 0.10593577037342382
train: 0.980623	val: 0.770210	test: 0.745371

Epoch: 96
Loss: 0.10358101374572362
train: 0.980715	val: 0.760322	test: 0.738165

Epoch: 97
Loss: 0.10179306247261097
train: 0.980851	val: 0.778440	test: 0.739988

Epoch: 98
Loss: 0.1043647065943736
train: 0.978740	val: 0.764395	test: 0.737252

Epoch: 99
Loss: 0.1037185859545865
train: 0.979894	val: 0.762766	test: 0.737801

Epoch: 100
Loss: 0.10221180515654986
train: 0.982809	val: 0.767027	test: 0.748016

best train: 0.914186	val: 0.796719	test: 0.766244
end
