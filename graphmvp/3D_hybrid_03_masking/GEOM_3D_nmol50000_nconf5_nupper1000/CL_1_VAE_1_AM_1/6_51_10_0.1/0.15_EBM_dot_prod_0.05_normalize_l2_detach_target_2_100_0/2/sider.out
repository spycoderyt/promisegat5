9112376_2
--dataset=sider --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6867087393663296
train: 0.530227	val: 0.526609	test: 0.503644

Epoch: 2
Loss: 0.6421400159959637
train: 0.555461	val: 0.519694	test: 0.522482

Epoch: 3
Loss: 0.6033403522574954
train: 0.572327	val: 0.513763	test: 0.525016

Epoch: 4
Loss: 0.5792361389740883
train: 0.594931	val: 0.522221	test: 0.533994

Epoch: 5
Loss: 0.5563878230356295
train: 0.629734	val: 0.544019	test: 0.553881

Epoch: 6
Loss: 0.5435882658055411
train: 0.651283	val: 0.563938	test: 0.571247

Epoch: 7
Loss: 0.5304610505247618
train: 0.665342	val: 0.574362	test: 0.580266

Epoch: 8
Loss: 0.5225408752996586
train: 0.678103	val: 0.586069	test: 0.588531

Epoch: 9
Loss: 0.5149186626074094
train: 0.686157	val: 0.589297	test: 0.591203

Epoch: 10
Loss: 0.5050318869459558
train: 0.694272	val: 0.587483	test: 0.595142

Epoch: 11
Loss: 0.5032854684981386
train: 0.704720	val: 0.590022	test: 0.603631

Epoch: 12
Loss: 0.49636409855258307
train: 0.714412	val: 0.606606	test: 0.598329

Epoch: 13
Loss: 0.48797175391596614
train: 0.719383	val: 0.600603	test: 0.596241

Epoch: 14
Loss: 0.4836983263966042
train: 0.720142	val: 0.594284	test: 0.596019

Epoch: 15
Loss: 0.48123246206258274
train: 0.726975	val: 0.603537	test: 0.597897

Epoch: 16
Loss: 0.4751392323849413
train: 0.739432	val: 0.613656	test: 0.606181

Epoch: 17
Loss: 0.47729467309734686
train: 0.743807	val: 0.618183	test: 0.603817

Epoch: 18
Loss: 0.4724962210212226
train: 0.746328	val: 0.614881	test: 0.602250

Epoch: 19
Loss: 0.4665368899787465
train: 0.754416	val: 0.614646	test: 0.612021

Epoch: 20
Loss: 0.46651370960118854
train: 0.758784	val: 0.609113	test: 0.619368

Epoch: 21
Loss: 0.46590797503565035
train: 0.762187	val: 0.604102	test: 0.618281

Epoch: 22
Loss: 0.45911408386273644
train: 0.764960	val: 0.615552	test: 0.609902

Epoch: 23
Loss: 0.45757271084857776
train: 0.771396	val: 0.620869	test: 0.609114

Epoch: 24
Loss: 0.4572492779496125
train: 0.772025	val: 0.622011	test: 0.609040

Epoch: 25
Loss: 0.4524682766120806
train: 0.779522	val: 0.618675	test: 0.615280

Epoch: 26
Loss: 0.45310326979955534
train: 0.785788	val: 0.615491	test: 0.618182

Epoch: 27
Loss: 0.448666718767137
train: 0.787528	val: 0.625917	test: 0.609238

Epoch: 28
Loss: 0.4441018819201397
train: 0.789544	val: 0.623566	test: 0.613787

Epoch: 29
Loss: 0.44238825695760864
train: 0.794777	val: 0.626896	test: 0.611459

Epoch: 30
Loss: 0.4468105296530059
train: 0.799373	val: 0.619575	test: 0.615231

Epoch: 31
Loss: 0.44649157470569667
train: 0.798763	val: 0.614989	test: 0.609763

Epoch: 32
Loss: 0.44718527736902464
train: 0.805404	val: 0.627391	test: 0.617670

Epoch: 33
Loss: 0.44210710425802374
train: 0.807183	val: 0.627073	test: 0.613562

Epoch: 34
Loss: 0.4419688749561619
train: 0.810213	val: 0.616814	test: 0.613096

Epoch: 35
Loss: 0.4372016005391945
train: 0.810715	val: 0.614927	test: 0.613260

Epoch: 36
Loss: 0.43588961271721105
train: 0.813821	val: 0.623494	test: 0.610148

Epoch: 37
Loss: 0.4359044032242668
train: 0.819286	val: 0.630418	test: 0.607643

Epoch: 38
Loss: 0.4311981211463972
train: 0.822719	val: 0.619083	test: 0.610227

Epoch: 39
Loss: 0.43545360159274615
train: 0.820878	val: 0.633439	test: 0.607693

Epoch: 40
Loss: 0.42912874189715833
train: 0.820174	val: 0.630515	test: 0.609099

Epoch: 41
Loss: 0.43400424677600313
train: 0.828200	val: 0.635830	test: 0.607151

Epoch: 42
Loss: 0.4233553350873276
train: 0.830334	val: 0.635009	test: 0.589809

Epoch: 43
Loss: 0.41832805158887226
train: 0.828141	val: 0.625828	test: 0.605008

Epoch: 44
Loss: 0.4230618996058251
train: 0.832964	val: 0.627592	test: 0.615593

Epoch: 45
Loss: 0.42001107398849874
train: 0.832641	val: 0.627850	test: 0.605381

Epoch: 46
Loss: 0.42199916217333405
train: 0.837137	val: 0.628234	test: 0.622701

Epoch: 47
Loss: 0.4239857920599122
train: 0.842323	val: 0.632963	test: 0.622289

Epoch: 48
Loss: 0.4190392162255523
train: 0.838384	val: 0.631339	test: 0.619631

Epoch: 49
Loss: 0.41455642659021885
train: 0.839195	val: 0.629099	test: 0.607169

Epoch: 50
Loss: 0.4144166891136621
train: 0.844485	val: 0.628021	test: 0.608218

Epoch: 51
Loss: 0.4131086277615462
train: 0.845872	val: 0.626665	test: 0.610826

Epoch: 52
Loss: 0.41259152924379405
train: 0.852793	val: 0.636928	test: 0.608393

Epoch: 53
Loss: 0.4142397152927397
train: 0.849512	val: 0.635250	test: 0.609741

Epoch: 54
Loss: 0.40838377060577163
train: 0.854519	val: 0.630407	test: 0.623862

Epoch: 55
Loss: 0.40724429337530504
train: 0.854994	val: 0.625517	test: 0.623088

Epoch: 56
Loss: 0.40744415664514977
train: 0.852219	val: 0.617462	test: 0.616912

Epoch: 57
Loss: 0.40367180619605936
train: 0.852429	val: 0.616450	test: 0.610061

Epoch: 58
Loss: 0.40352066172522694
train: 0.862358	val: 0.632912	test: 0.601338

Epoch: 59
Loss: 0.4021623427525466
train: 0.856086	val: 0.629415	test: 0.599354

Epoch: 60
Loss: 0.40360892231093504
train: 0.859391	val: 0.628820	test: 0.605946

Epoch: 61
Loss: 0.39936935433964377
train: 0.863120	val: 0.627194	test: 0.613598

Epoch: 62
Loss: 0.39821099465664295
train: 0.862379	val: 0.626979	test: 0.604946

Epoch: 63
Loss: 0.3950330271863512
train: 0.865282	val: 0.622579	test: 0.604741

Epoch: 64
Loss: 0.401139074940024
train: 0.868098	val: 0.630411	test: 0.599237

Epoch: 65
Loss: 0.3938790792518521
train: 0.863426	val: 0.626467	test: 0.599787

Epoch: 66
Loss: 0.39133851444115514
train: 0.863233	val: 0.620381	test: 0.601928

Epoch: 67
Loss: 0.39906329246932526
train: 0.870200	val: 0.635251	test: 0.595650

Epoch: 68
Loss: 0.38883646276844447
train: 0.872824	val: 0.629469	test: 0.593660

Epoch: 69
Loss: 0.39924075302277356
train: 0.873369	val: 0.628959	test: 0.605040

Epoch: 70
Loss: 0.39429824713122086
train: 0.872225	val: 0.628544	test: 0.610965

Epoch: 71
Loss: 0.3914931544155111
train: 0.875608	val: 0.632456	test: 0.605594

Epoch: 72
Loss: 0.3880630879028031
train: 0.878596	val: 0.630327	test: 0.611597

Epoch: 73
Loss: 0.38835698136368524
train: 0.880461	val: 0.626877	test: 0.617474

Epoch: 74
Loss: 0.38391585585429777
train: 0.882446	val: 0.620160	test: 0.617033

Epoch: 75
Loss: 0.38604068351004456
train: 0.883613	val: 0.622019	test: 0.609377

Epoch: 76
Loss: 0.38166515484540264
train: 0.880397	val: 0.614587	test: 0.616283

Epoch: 77
Loss: 0.3800080025727878
train: 0.883274	val: 0.634530	test: 0.624433

Epoch: 78
Loss: 0.3829442307943943
train: 0.883439	val: 0.633659	test: 0.618202

Epoch: 79
Loss: 0.3875025982540127
train: 0.886235	val: 0.622218	test: 0.621295

Epoch: 80
Loss: 0.3827527549627459
train: 0.883016	val: 0.621955	test: 0.624890

Epoch: 81
Loss: 0.38521106659268406
train: 0.888180	val: 0.630062	test: 0.621221

Epoch: 82
Loss: 0.3759693053737777
train: 0.889711	val: 0.626521	test: 0.618474

Epoch: 83
Loss: 0.3776077888558591
train: 0.890948	val: 0.625216	test: 0.606888

Epoch: 84
Loss: 0.3797465146371894
train: 0.889188	val: 0.618113	test: 0.615406

Epoch: 85
Loss: 0.3741694499329291
train: 0.893808	val: 0.623667	test: 0.612860

Epoch: 86
Loss: 0.37552858909453773
train: 0.893420	val: 0.621348	test: 0.622964

Epoch: 87
Loss: 0.3767887006187376
train: 0.892981	val: 0.615011	test: 0.619566

Epoch: 88
Loss: 0.3763325472336283
train: 0.894220	val: 0.620264	test: 0.599344

Epoch: 89
Loss: 0.3699634568836861
train: 0.894704	val: 0.627753	test: 0.599274

Epoch: 90
Loss: 0.364775621824357
train: 0.896750	val: 0.627002	test: 0.614568

Epoch: 91
Loss: 0.37228113409258823
train: 0.897782	val: 0.628663	test: 0.605251

Epoch: 92
Loss: 0.3703387491629787
train: 0.898650	val: 0.628770	test: 0.612041

Epoch: 93
Loss: 0.36339531512181694
train: 0.897849	val: 0.622119	test: 0.619729

Epoch: 94
Loss: 0.3615499246004979
train: 0.901399	val: 0.631983	test: 0.613044

Epoch: 95
Loss: 0.3681446140679665
train: 0.901934	val: 0.629295	test: 0.609858

Epoch: 96
Loss: 0.3597050345552258
train: 0.903866	val: 0.624946	test: 0.614813

Epoch: 97
Loss: 0.357982226677071
train: 0.904592	val: 0.634065	test: 0.607469

Epoch: 98
Loss: 0.3636094639912555
train: 0.904476	val: 0.641231	test: 0.617282

Epoch: 99
Loss: 0.36236713723878095
train: 0.906540	val: 0.636887	test: 0.623261

Epoch: 100
Loss: 0.3619545361543574
train: 0.907753	val: 0.626135	test: 0.606020

best train: 0.904476	val: 0.641231	test: 0.617282
end
