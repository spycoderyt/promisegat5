9112376_2
--dataset=hiv --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2562125865103392
train: 0.776576	val: 0.732183	test: 0.737102

Epoch: 2
Loss: 0.1369197515988081
train: 0.796232	val: 0.779673	test: 0.744433

Epoch: 3
Loss: 0.13157445893203235
train: 0.809375	val: 0.752453	test: 0.765021

Epoch: 4
Loss: 0.1280049073764278
train: 0.831723	val: 0.785910	test: 0.778095

Epoch: 5
Loss: 0.12486989253863442
train: 0.844267	val: 0.790696	test: 0.766161

Epoch: 6
Loss: 0.12355544481980928
train: 0.839391	val: 0.802016	test: 0.759688

Epoch: 7
Loss: 0.12043298221809236
train: 0.856445	val: 0.794067	test: 0.763385

Epoch: 8
Loss: 0.11868445655399078
train: 0.854789	val: 0.762557	test: 0.756038

Epoch: 9
Loss: 0.11668895287431547
train: 0.857848	val: 0.786740	test: 0.736851

Epoch: 10
Loss: 0.11722246757702924
train: 0.866623	val: 0.774949	test: 0.738533

Epoch: 11
Loss: 0.11465500983673148
train: 0.862351	val: 0.796918	test: 0.751670

Epoch: 12
Loss: 0.11431990258201336
train: 0.872395	val: 0.770346	test: 0.772916

Epoch: 13
Loss: 0.11360069453151353
train: 0.875969	val: 0.767992	test: 0.743771

Epoch: 14
Loss: 0.11272335781147313
train: 0.879959	val: 0.781320	test: 0.756131

Epoch: 15
Loss: 0.11127269443201641
train: 0.887391	val: 0.780420	test: 0.751038

Epoch: 16
Loss: 0.10999672049441725
train: 0.883796	val: 0.790803	test: 0.758881

Epoch: 17
Loss: 0.1080191322418588
train: 0.894258	val: 0.808461	test: 0.774909

Epoch: 18
Loss: 0.10831250029026782
train: 0.891671	val: 0.793850	test: 0.760140

Epoch: 19
Loss: 0.10832823201403598
train: 0.893923	val: 0.789866	test: 0.746044

Epoch: 20
Loss: 0.10736294660333273
train: 0.901301	val: 0.824028	test: 0.753099

Epoch: 21
Loss: 0.10732947048830863
train: 0.903191	val: 0.793614	test: 0.755747

Epoch: 22
Loss: 0.10586801018281392
train: 0.909486	val: 0.810510	test: 0.751546

Epoch: 23
Loss: 0.10567867448076729
train: 0.913476	val: 0.821468	test: 0.764408

Epoch: 24
Loss: 0.10289464243686049
train: 0.912846	val: 0.817500	test: 0.761504

Epoch: 25
Loss: 0.10281562001339542
train: 0.910594	val: 0.789588	test: 0.782951

Epoch: 26
Loss: 0.103736110992257
train: 0.903039	val: 0.807377	test: 0.757429

Epoch: 27
Loss: 0.10216960156647102
train: 0.919231	val: 0.810899	test: 0.769250

Epoch: 28
Loss: 0.10158135781400757
train: 0.922020	val: 0.800849	test: 0.767811

Epoch: 29
Loss: 0.10202198444760148
train: 0.916225	val: 0.814101	test: 0.762589

Epoch: 30
Loss: 0.09955146893004516
train: 0.926224	val: 0.829209	test: 0.769434

Epoch: 31
Loss: 0.0998624881178974
train: 0.928616	val: 0.799468	test: 0.752904

Epoch: 32
Loss: 0.09847675411483596
train: 0.927079	val: 0.807117	test: 0.771087

Epoch: 33
Loss: 0.09911158436537394
train: 0.932241	val: 0.823820	test: 0.780954

Epoch: 34
Loss: 0.09885563304023991
train: 0.934302	val: 0.811009	test: 0.745049

Epoch: 35
Loss: 0.09800341360756008
train: 0.928525	val: 0.802448	test: 0.749763

Epoch: 36
Loss: 0.09729278785320938
train: 0.932781	val: 0.810249	test: 0.755254

Epoch: 37
Loss: 0.09611614358250843
train: 0.933601	val: 0.811502	test: 0.746747

Epoch: 38
Loss: 0.09654105383431007
train: 0.936364	val: 0.806042	test: 0.764281

Epoch: 39
Loss: 0.09557939408337908
train: 0.940360	val: 0.805289	test: 0.781436

Epoch: 40
Loss: 0.09400714112308466
train: 0.942668	val: 0.808030	test: 0.756942

Epoch: 41
Loss: 0.09485866494416423
train: 0.941594	val: 0.793945	test: 0.774947

Epoch: 42
Loss: 0.09381751273905521
train: 0.941002	val: 0.798213	test: 0.780032

Epoch: 43
Loss: 0.09287153296820103
train: 0.944197	val: 0.825923	test: 0.770138

Epoch: 44
Loss: 0.09407224284081853
train: 0.939299	val: 0.798804	test: 0.771774

Epoch: 45
Loss: 0.09334805560881067
train: 0.946170	val: 0.804888	test: 0.764032

Epoch: 46
Loss: 0.09200589807953789
train: 0.942143	val: 0.817381	test: 0.784169

Epoch: 47
Loss: 0.09266718841451409
train: 0.945530	val: 0.791942	test: 0.734533

Epoch: 48
Loss: 0.090624414707057
train: 0.949394	val: 0.791232	test: 0.787342

Epoch: 49
Loss: 0.09169566016455616
train: 0.951539	val: 0.812390	test: 0.756341

Epoch: 50
Loss: 0.09094991386923519
train: 0.952838	val: 0.812858	test: 0.772093

Epoch: 51
Loss: 0.09025784293877413
train: 0.953635	val: 0.800886	test: 0.755360

Epoch: 52
Loss: 0.09028698167189118
train: 0.953833	val: 0.788069	test: 0.752160

Epoch: 53
Loss: 0.0901977653046444
train: 0.954768	val: 0.808449	test: 0.769065

Epoch: 54
Loss: 0.08789209332165814
train: 0.954796	val: 0.818823	test: 0.749116

Epoch: 55
Loss: 0.08760841819003236
train: 0.959122	val: 0.817586	test: 0.755947

Epoch: 56
Loss: 0.08770585819550045
train: 0.957197	val: 0.808327	test: 0.766388

Epoch: 57
Loss: 0.08724233050932952
train: 0.957811	val: 0.819518	test: 0.748612

Epoch: 58
Loss: 0.0875063433107245
train: 0.958193	val: 0.801991	test: 0.745076

Epoch: 59
Loss: 0.08591881505615365
train: 0.958397	val: 0.811003	test: 0.743580

Epoch: 60
Loss: 0.08612724505882154
train: 0.953904	val: 0.799762	test: 0.759849

Epoch: 61
Loss: 0.08710550095924648
train: 0.961736	val: 0.804505	test: 0.759779

Epoch: 62
Loss: 0.08452291793685174
train: 0.965348	val: 0.793969	test: 0.749848

Epoch: 63
Loss: 0.08325373988663022
train: 0.963957	val: 0.798715	test: 0.757305

Epoch: 64
Loss: 0.0849478515645348
train: 0.965199	val: 0.838049	test: 0.760895

Epoch: 65
Loss: 0.08460303661915024
train: 0.963734	val: 0.806232	test: 0.760256

Epoch: 66
Loss: 0.08488916096386433
train: 0.965975	val: 0.810344	test: 0.749171

Epoch: 67
Loss: 0.08367347288678019
train: 0.963344	val: 0.801884	test: 0.744329

Epoch: 68
Loss: 0.08399617900761468
train: 0.965066	val: 0.821067	test: 0.777978

Epoch: 69
Loss: 0.0822056901163363
train: 0.965884	val: 0.794637	test: 0.727359

Epoch: 70
Loss: 0.08139425220473147
train: 0.966384	val: 0.805216	test: 0.759692

Epoch: 71
Loss: 0.08113048007778083
train: 0.970145	val: 0.813324	test: 0.763615

Epoch: 72
Loss: 0.08147294066638641
train: 0.970947	val: 0.809138	test: 0.754592

Epoch: 73
Loss: 0.08125800782121648
train: 0.966209	val: 0.807230	test: 0.753914

Epoch: 74
Loss: 0.080733048550024
train: 0.967002	val: 0.809331	test: 0.757153

Epoch: 75
Loss: 0.08018633530037952
train: 0.973458	val: 0.817589	test: 0.764810

Epoch: 76
Loss: 0.07856932719021398
train: 0.971797	val: 0.816872	test: 0.761407

Epoch: 77
Loss: 0.07903629170828531
train: 0.975247	val: 0.802221	test: 0.752877

Epoch: 78
Loss: 0.07877877342866224
train: 0.973047	val: 0.810311	test: 0.765380

Epoch: 79
Loss: 0.07770397607701958
train: 0.974088	val: 0.794260	test: 0.742961

Epoch: 80
Loss: 0.07868770749437774
train: 0.974420	val: 0.810005	test: 0.774474

Epoch: 81
Loss: 0.07724619122698213
train: 0.975764	val: 0.801462	test: 0.722693

Epoch: 82
Loss: 0.07764299487283526
train: 0.976736	val: 0.805577	test: 0.749601

Epoch: 83
Loss: 0.07679993731393632
train: 0.977364	val: 0.802870	test: 0.747473

Epoch: 84
Loss: 0.07799887269258776
train: 0.973352	val: 0.811689	test: 0.744411

Epoch: 85
Loss: 0.07621768560368548
train: 0.978564	val: 0.806321	test: 0.745171

Epoch: 86
Loss: 0.07661450108021384
train: 0.979102	val: 0.808780	test: 0.765080

Epoch: 87
Loss: 0.07455173210005656
train: 0.976641	val: 0.798780	test: 0.741681

Epoch: 88
Loss: 0.07584216064778504
train: 0.978551	val: 0.808789	test: 0.781174

Epoch: 89
Loss: 0.07542530982358281
train: 0.977964	val: 0.806992	test: 0.748431

Epoch: 90
Loss: 0.07447076685232656
train: 0.980964	val: 0.810274	test: 0.766834

Epoch: 91
Loss: 0.07421734048564299
train: 0.981371	val: 0.793593	test: 0.765907

Epoch: 92
Loss: 0.0748734757956961
train: 0.981901	val: 0.786477	test: 0.755109

Epoch: 93
Loss: 0.07402294613618954
train: 0.981074	val: 0.795534	test: 0.737851

Epoch: 94
Loss: 0.07304411201662366
train: 0.979894	val: 0.805715	test: 0.757614

Epoch: 95
Loss: 0.07396309420231965
train: 0.980877	val: 0.808884	test: 0.747469

Epoch: 96
Loss: 0.07184423698367959
train: 0.982556	val: 0.798201	test: 0.769750

Epoch: 97
Loss: 0.07283588350158209
train: 0.981977	val: 0.817518	test: 0.734947

Epoch: 98
Loss: 0.07231074763434438
train: 0.984634	val: 0.794456	test: 0.747170

Epoch: 99
Loss: 0.07131740695286197
train: 0.982017	val: 0.818161	test: 0.758779

Epoch: 100
Loss: 0.07134382013247285
train: 0.983938	val: 0.807212	test: 0.753361

best train: 0.965199	val: 0.838049	test: 0.760895
end
