9112376_2
--dataset=bbbp --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6345711245883507
train: 0.771841	val: 0.893606	test: 0.587384

Epoch: 2
Loss: 0.5010895007633385
train: 0.847446	val: 0.905149	test: 0.653356

Epoch: 3
Loss: 0.4216184902704064
train: 0.874987	val: 0.897320	test: 0.667921

Epoch: 4
Loss: 0.36746024061690663
train: 0.894269	val: 0.903142	test: 0.647569

Epoch: 5
Loss: 0.3087358885000742
train: 0.908255	val: 0.911673	test: 0.664545

Epoch: 6
Loss: 0.2832756417084499
train: 0.922032	val: 0.912878	test: 0.671875

Epoch: 7
Loss: 0.27668663147981615
train: 0.937992	val: 0.917394	test: 0.675926

Epoch: 8
Loss: 0.2767650540387251
train: 0.938812	val: 0.913681	test: 0.675637

Epoch: 9
Loss: 0.2617998923669461
train: 0.947188	val: 0.900833	test: 0.690972

Epoch: 10
Loss: 0.245988765705534
train: 0.952257	val: 0.913480	test: 0.701485

Epoch: 11
Loss: 0.2261526599195384
train: 0.953368	val: 0.911472	test: 0.700521

Epoch: 12
Loss: 0.2152353643930351
train: 0.958086	val: 0.919000	test: 0.700714

Epoch: 13
Loss: 0.21608183484466464
train: 0.951525	val: 0.916591	test: 0.702643

Epoch: 14
Loss: 0.22265317584432529
train: 0.962196	val: 0.922915	test: 0.712674

Epoch: 15
Loss: 0.20386428238672546
train: 0.959921	val: 0.908963	test: 0.706211

Epoch: 16
Loss: 0.1985275152033694
train: 0.965038	val: 0.918097	test: 0.706597

Epoch: 17
Loss: 0.1930663099350217
train: 0.965791	val: 0.922313	test: 0.721547

Epoch: 18
Loss: 0.2020983584320877
train: 0.962310	val: 0.919301	test: 0.711227

Epoch: 19
Loss: 0.18353883620430925
train: 0.970294	val: 0.918097	test: 0.708526

Epoch: 20
Loss: 0.18470043758140517
train: 0.968922	val: 0.913279	test: 0.721451

Epoch: 21
Loss: 0.16593392236364765
train: 0.973188	val: 0.917194	test: 0.724826

Epoch: 22
Loss: 0.18674651844799645
train: 0.974886	val: 0.909164	test: 0.711902

Epoch: 23
Loss: 0.1799778767204349
train: 0.976467	val: 0.901134	test: 0.704958

Epoch: 24
Loss: 0.17517378499580002
train: 0.976414	val: 0.902539	test: 0.718075

Epoch: 25
Loss: 0.16964196318705976
train: 0.978991	val: 0.908963	test: 0.718364

Epoch: 26
Loss: 0.16289355339474598
train: 0.979204	val: 0.912577	test: 0.723090

Epoch: 27
Loss: 0.16078364794504513
train: 0.981117	val: 0.919201	test: 0.716435

Epoch: 28
Loss: 0.1481409857718158
train: 0.982814	val: 0.912577	test: 0.717496

Epoch: 29
Loss: 0.1708429190937535
train: 0.978714	val: 0.908160	test: 0.713735

Epoch: 30
Loss: 0.15193745403223574
train: 0.983722	val: 0.911673	test: 0.716435

Epoch: 31
Loss: 0.15554320892921858
train: 0.984179	val: 0.897722	test: 0.699749

Epoch: 32
Loss: 0.15590229422641258
train: 0.983357	val: 0.892402	test: 0.699556

Epoch: 33
Loss: 0.14875863995655506
train: 0.984979	val: 0.901134	test: 0.714506

Epoch: 34
Loss: 0.15004366452027698
train: 0.985875	val: 0.902138	test: 0.707562

Epoch: 35
Loss: 0.1447970984146894
train: 0.987181	val: 0.903041	test: 0.708140

Epoch: 36
Loss: 0.14324801171895143
train: 0.985205	val: 0.896517	test: 0.720968

Epoch: 37
Loss: 0.1390050329729115
train: 0.988174	val: 0.889290	test: 0.721451

Epoch: 38
Loss: 0.14284776251605363
train: 0.987119	val: 0.899328	test: 0.712095

Epoch: 39
Loss: 0.13638562537741233
train: 0.988055	val: 0.902941	test: 0.716725

Epoch: 40
Loss: 0.13793739460834603
train: 0.988283	val: 0.900231	test: 0.714506

Epoch: 41
Loss: 0.13891169503465137
train: 0.990134	val: 0.893004	test: 0.708237

Epoch: 42
Loss: 0.1568837002226723
train: 0.990338	val: 0.894209	test: 0.703511

Epoch: 43
Loss: 0.14374170074904064
train: 0.990707	val: 0.865603	test: 0.699653

Epoch: 44
Loss: 0.14769240292377853
train: 0.992417	val: 0.892302	test: 0.699363

Epoch: 45
Loss: 0.1407420179111414
train: 0.988753	val: 0.898525	test: 0.708044

Epoch: 46
Loss: 0.1340011045873882
train: 0.991727	val: 0.890394	test: 0.717014

Epoch: 47
Loss: 0.151459387450463
train: 0.992799	val: 0.884272	test: 0.717496

Epoch: 48
Loss: 0.12356519621813751
train: 0.990047	val: 0.885376	test: 0.698302

Epoch: 49
Loss: 0.14236665964689485
train: 0.993650	val: 0.890294	test: 0.722994

Epoch: 50
Loss: 0.14931965576129932
train: 0.992804	val: 0.885476	test: 0.724537

Epoch: 51
Loss: 0.12154787010096801
train: 0.992552	val: 0.873934	test: 0.720872

Epoch: 52
Loss: 0.12369407288708842
train: 0.992208	val: 0.888287	test: 0.707369

Epoch: 53
Loss: 0.12016531045697339
train: 0.992824	val: 0.889391	test: 0.704186

Epoch: 54
Loss: 0.11330233131898357
train: 0.993681	val: 0.889290	test: 0.703221

Epoch: 55
Loss: 0.11658816118946615
train: 0.995684	val: 0.897722	test: 0.712963

Epoch: 56
Loss: 0.12047296784703196
train: 0.995516	val: 0.894811	test: 0.710841

Epoch: 57
Loss: 0.12820250799680644
train: 0.995525	val: 0.886781	test: 0.702160

Epoch: 58
Loss: 0.11859951436534424
train: 0.996311	val: 0.884071	test: 0.703125

Epoch: 59
Loss: 0.11784735927207161
train: 0.996770	val: 0.887785	test: 0.694059

Epoch: 60
Loss: 0.10947766157308282
train: 0.996761	val: 0.889190	test: 0.699074

Epoch: 61
Loss: 0.1084304404768649
train: 0.995413	val: 0.878149	test: 0.700714

Epoch: 62
Loss: 0.09975959748459151
train: 0.995994	val: 0.885276	test: 0.709973

Epoch: 63
Loss: 0.11190660552680516
train: 0.997171	val: 0.890394	test: 0.718846

Epoch: 64
Loss: 0.0998713344162425
train: 0.996134	val: 0.882365	test: 0.700521

Epoch: 65
Loss: 0.12203514379918712
train: 0.996887	val: 0.885577	test: 0.692226

Epoch: 66
Loss: 0.10515461868601698
train: 0.996058	val: 0.863796	test: 0.693480

Epoch: 67
Loss: 0.10558523512631621
train: 0.996838	val: 0.886380	test: 0.668789

Epoch: 68
Loss: 0.13057802951652833
train: 0.997031	val: 0.902640	test: 0.715278

Epoch: 69
Loss: 0.09962500399932181
train: 0.994170	val: 0.890696	test: 0.731578

Epoch: 70
Loss: 0.11356508511107728
train: 0.996259	val: 0.888688	test: 0.710359

Epoch: 71
Loss: 0.10888711104149462
train: 0.996444	val: 0.888789	test: 0.685957

Epoch: 72
Loss: 0.10250537209565642
train: 0.997622	val: 0.883067	test: 0.700521

Epoch: 73
Loss: 0.09706481963984259
train: 0.997961	val: 0.881863	test: 0.703704

Epoch: 74
Loss: 0.11047123403324109
train: 0.997690	val: 0.885577	test: 0.708816

Epoch: 75
Loss: 0.08645587085022013
train: 0.997167	val: 0.891800	test: 0.711613

Epoch: 76
Loss: 0.09508548815641747
train: 0.996087	val: 0.885978	test: 0.696277

Epoch: 77
Loss: 0.10770578994114997
train: 0.996813	val: 0.881863	test: 0.687789

Epoch: 78
Loss: 0.0997386753196591
train: 0.997242	val: 0.872528	test: 0.700714

Epoch: 79
Loss: 0.09648131660111012
train: 0.997009	val: 0.887986	test: 0.703318

Epoch: 80
Loss: 0.10044405110167245
train: 0.998356	val: 0.890696	test: 0.688272

Epoch: 81
Loss: 0.08396435005810929
train: 0.997778	val: 0.882064	test: 0.698881

Epoch: 82
Loss: 0.08553617279602362
train: 0.998303	val: 0.893707	test: 0.704186

Epoch: 83
Loss: 0.08807422597208156
train: 0.998533	val: 0.885978	test: 0.693287

Epoch: 84
Loss: 0.09962631664304746
train: 0.998378	val: 0.882867	test: 0.689718

Epoch: 85
Loss: 0.09360069552807837
train: 0.998193	val: 0.885476	test: 0.682002

Epoch: 86
Loss: 0.08186785177575517
train: 0.998241	val: 0.890294	test: 0.687500

Epoch: 87
Loss: 0.098637858049093
train: 0.998522	val: 0.890093	test: 0.688272

Epoch: 88
Loss: 0.10454980627317267
train: 0.998435	val: 0.886380	test: 0.682774

Epoch: 89
Loss: 0.08648689973240052
train: 0.997932	val: 0.884473	test: 0.687693

Epoch: 90
Loss: 0.09359609800428728
train: 0.998418	val: 0.884573	test: 0.695120

Epoch: 91
Loss: 0.08216938752942159
train: 0.998543	val: 0.893807	test: 0.694252

Epoch: 92
Loss: 0.08675132786399367
train: 0.999034	val: 0.890896	test: 0.697145

Epoch: 93
Loss: 0.0743125691649109
train: 0.997801	val: 0.871826	test: 0.683738

Epoch: 94
Loss: 0.08602237652388582
train: 0.998118	val: 0.881763	test: 0.687211

Epoch: 95
Loss: 0.08443647380088884
train: 0.998704	val: 0.885577	test: 0.693962

Epoch: 96
Loss: 0.08110966270030452
train: 0.998656	val: 0.885476	test: 0.679977

Epoch: 97
Loss: 0.09641739416393749
train: 0.998545	val: 0.879855	test: 0.675733

Epoch: 98
Loss: 0.08665683031655355
train: 0.998101	val: 0.867610	test: 0.670428

Epoch: 99
Loss: 0.10626336196271745
train: 0.998846	val: 0.879253	test: 0.666281

Epoch: 100
Loss: 0.0825106911933555
train: 0.998146	val: 0.864699	test: 0.671586

best train: 0.962196	val: 0.922915	test: 0.712674
end
