11511808_0
--dataset=hiv --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.26535220830870054
train: 0.775362	val: 0.713428	test: 0.727208

Epoch: 2
Loss: 0.13571873492781136
train: 0.803005	val: 0.790108	test: 0.749503

Epoch: 3
Loss: 0.13033852568531565
train: 0.816499	val: 0.779842	test: 0.758576

Epoch: 4
Loss: 0.12688221229565114
train: 0.817748	val: 0.766403	test: 0.730327

Epoch: 5
Loss: 0.12531295301315382
train: 0.832801	val: 0.745107	test: 0.759700

Epoch: 6
Loss: 0.12121408282882626
train: 0.841424	val: 0.794827	test: 0.758531

Epoch: 7
Loss: 0.12250557298871509
train: 0.849611	val: 0.797359	test: 0.760472

Epoch: 8
Loss: 0.12019985814229872
train: 0.852806	val: 0.808719	test: 0.755080

Epoch: 9
Loss: 0.1176007593172599
train: 0.862260	val: 0.784937	test: 0.746086

Epoch: 10
Loss: 0.11596455680512154
train: 0.867128	val: 0.779652	test: 0.753060

Epoch: 11
Loss: 0.1164099193204392
train: 0.871437	val: 0.796327	test: 0.764526

Epoch: 12
Loss: 0.11459377257445855
train: 0.875271	val: 0.757312	test: 0.731818

Epoch: 13
Loss: 0.11292143957542294
train: 0.878472	val: 0.802059	test: 0.756745

Epoch: 14
Loss: 0.11036425929326883
train: 0.882342	val: 0.787310	test: 0.755801

Epoch: 15
Loss: 0.11118029656214765
train: 0.884893	val: 0.801547	test: 0.763663

Epoch: 16
Loss: 0.11071715881481241
train: 0.888826	val: 0.803164	test: 0.756786

Epoch: 17
Loss: 0.10876675418258668
train: 0.884617	val: 0.800188	test: 0.761063

Epoch: 18
Loss: 0.10903666215958856
train: 0.890888	val: 0.795004	test: 0.757825

Epoch: 19
Loss: 0.10704610231956492
train: 0.895074	val: 0.804714	test: 0.741030

Epoch: 20
Loss: 0.10651606665448547
train: 0.892172	val: 0.814717	test: 0.764795

Epoch: 21
Loss: 0.10508610860975635
train: 0.903574	val: 0.798320	test: 0.742658

Epoch: 22
Loss: 0.1055721269194973
train: 0.904714	val: 0.811046	test: 0.752405

Epoch: 23
Loss: 0.10455209634216682
train: 0.906649	val: 0.779982	test: 0.744644

Epoch: 24
Loss: 0.10413801213983193
train: 0.905242	val: 0.806094	test: 0.761921

Epoch: 25
Loss: 0.10439195751789838
train: 0.911422	val: 0.807085	test: 0.753819

Epoch: 26
Loss: 0.10315285351474192
train: 0.911029	val: 0.817298	test: 0.748788

Epoch: 27
Loss: 0.10324523310218679
train: 0.915936	val: 0.808492	test: 0.755551

Epoch: 28
Loss: 0.1025357839810817
train: 0.914268	val: 0.782340	test: 0.753937

Epoch: 29
Loss: 0.10142378231819474
train: 0.919647	val: 0.800638	test: 0.756083

Epoch: 30
Loss: 0.10160022960104118
train: 0.919641	val: 0.791808	test: 0.742643

Epoch: 31
Loss: 0.10086418333001854
train: 0.921922	val: 0.783718	test: 0.745655

Epoch: 32
Loss: 0.09924792242562108
train: 0.923547	val: 0.794970	test: 0.755007

Epoch: 33
Loss: 0.09954692059960824
train: 0.924581	val: 0.796958	test: 0.758047

Epoch: 34
Loss: 0.09871250261045231
train: 0.932826	val: 0.810553	test: 0.763730

Epoch: 35
Loss: 0.09673046369295554
train: 0.932113	val: 0.812604	test: 0.748218

Epoch: 36
Loss: 0.10024080802512468
train: 0.927781	val: 0.807828	test: 0.778980

Epoch: 37
Loss: 0.09771424012717865
train: 0.930805	val: 0.799560	test: 0.778003

Epoch: 38
Loss: 0.0972140952170406
train: 0.932243	val: 0.797591	test: 0.763091

Epoch: 39
Loss: 0.09701018598963657
train: 0.937943	val: 0.798672	test: 0.759775

Epoch: 40
Loss: 0.09524992217157519
train: 0.939370	val: 0.796027	test: 0.755524

Epoch: 41
Loss: 0.09485415254838504
train: 0.940872	val: 0.823964	test: 0.761038

Epoch: 42
Loss: 0.09435323651054002
train: 0.937739	val: 0.801517	test: 0.760011

Epoch: 43
Loss: 0.09496176219523028
train: 0.942443	val: 0.795696	test: 0.774380

Epoch: 44
Loss: 0.0930944136796419
train: 0.944047	val: 0.786223	test: 0.779778

Epoch: 45
Loss: 0.09550484060527697
train: 0.945814	val: 0.793201	test: 0.747035

Epoch: 46
Loss: 0.09355815478307769
train: 0.939546	val: 0.794254	test: 0.749972

Epoch: 47
Loss: 0.09277762821343094
train: 0.945538	val: 0.794401	test: 0.733697

Epoch: 48
Loss: 0.09170385655191486
train: 0.947697	val: 0.788645	test: 0.777663

Epoch: 49
Loss: 0.09325812492859621
train: 0.950660	val: 0.804573	test: 0.748118

Epoch: 50
Loss: 0.09044652222701753
train: 0.950843	val: 0.793525	test: 0.754109

Epoch: 51
Loss: 0.09161952449410245
train: 0.954264	val: 0.809294	test: 0.772238

Epoch: 52
Loss: 0.08983846108208461
train: 0.949721	val: 0.790815	test: 0.755188

Epoch: 53
Loss: 0.09016020405615681
train: 0.957850	val: 0.800944	test: 0.751942

Epoch: 54
Loss: 0.08975197989830304
train: 0.950407	val: 0.776253	test: 0.748365

Epoch: 55
Loss: 0.08839633852491333
train: 0.959921	val: 0.816245	test: 0.780160

Epoch: 56
Loss: 0.08884402314382012
train: 0.958820	val: 0.789300	test: 0.758186

Epoch: 57
Loss: 0.08772900270894506
train: 0.958586	val: 0.802999	test: 0.769414

Epoch: 58
Loss: 0.08628488980329826
train: 0.958541	val: 0.789343	test: 0.742162

Epoch: 59
Loss: 0.08736101102660751
train: 0.955125	val: 0.784376	test: 0.745746

Epoch: 60
Loss: 0.08694584783173306
train: 0.956827	val: 0.785840	test: 0.768530

Epoch: 61
Loss: 0.08592104871981573
train: 0.960197	val: 0.803657	test: 0.766660

Epoch: 62
Loss: 0.08540395954971866
train: 0.965252	val: 0.788883	test: 0.746861

Epoch: 63
Loss: 0.08466902823974472
train: 0.959949	val: 0.791033	test: 0.762886

Epoch: 64
Loss: 0.08494460404217927
train: 0.961843	val: 0.793911	test: 0.765610

Epoch: 65
Loss: 0.08395289346842513
train: 0.964615	val: 0.780408	test: 0.763350

Epoch: 66
Loss: 0.08269924806850056
train: 0.963734	val: 0.796673	test: 0.765100

Epoch: 67
Loss: 0.08399263474805878
train: 0.966392	val: 0.805954	test: 0.764005

Epoch: 68
Loss: 0.08269545410544422
train: 0.969581	val: 0.798072	test: 0.748213

Epoch: 69
Loss: 0.08357468972221033
train: 0.962008	val: 0.801526	test: 0.763387

Epoch: 70
Loss: 0.0817316492083294
train: 0.968710	val: 0.786859	test: 0.748587

Epoch: 71
Loss: 0.08269464575806917
train: 0.964050	val: 0.796915	test: 0.764729

Epoch: 72
Loss: 0.08245590240495769
train: 0.968572	val: 0.795617	test: 0.748029

Epoch: 73
Loss: 0.08026876216846979
train: 0.969414	val: 0.790561	test: 0.764043

Epoch: 74
Loss: 0.08160536790573589
train: 0.967758	val: 0.783700	test: 0.742496

Epoch: 75
Loss: 0.08104157397113577
train: 0.974481	val: 0.792270	test: 0.769493

Epoch: 76
Loss: 0.07976667408066636
train: 0.971524	val: 0.773666	test: 0.762520

Epoch: 77
Loss: 0.0813209772552435
train: 0.970114	val: 0.789875	test: 0.756996

Epoch: 78
Loss: 0.07906702256989338
train: 0.974273	val: 0.806563	test: 0.766782

Epoch: 79
Loss: 0.0785187893305898
train: 0.974605	val: 0.798455	test: 0.751129

Epoch: 80
Loss: 0.07769103368541105
train: 0.972969	val: 0.797549	test: 0.755810

Epoch: 81
Loss: 0.07869138347820505
train: 0.972783	val: 0.804588	test: 0.775100

Epoch: 82
Loss: 0.07763794847643926
train: 0.975424	val: 0.797564	test: 0.757040

Epoch: 83
Loss: 0.07748985186098788
train: 0.973350	val: 0.779762	test: 0.766902

Epoch: 84
Loss: 0.07684215777494817
train: 0.977136	val: 0.788828	test: 0.760841

Epoch: 85
Loss: 0.07498660196760087
train: 0.977562	val: 0.790843	test: 0.771714

Epoch: 86
Loss: 0.07673502518597472
train: 0.978581	val: 0.794989	test: 0.754735

Epoch: 87
Loss: 0.076541804508947
train: 0.977258	val: 0.801480	test: 0.756133

Epoch: 88
Loss: 0.0749287713334525
train: 0.980378	val: 0.783850	test: 0.768593

Epoch: 89
Loss: 0.07381801056822278
train: 0.977379	val: 0.804208	test: 0.763130

Epoch: 90
Loss: 0.07477692001675482
train: 0.973955	val: 0.784395	test: 0.752589

Epoch: 91
Loss: 0.07561743398813446
train: 0.980560	val: 0.790240	test: 0.740586

Epoch: 92
Loss: 0.07608611033773113
train: 0.980380	val: 0.803620	test: 0.750016

Epoch: 93
Loss: 0.0738428655569635
train: 0.981080	val: 0.813976	test: 0.758636

Epoch: 94
Loss: 0.07544214749490209
train: 0.980930	val: 0.807273	test: 0.756303

Epoch: 95
Loss: 0.07208488548153208
train: 0.980702	val: 0.797910	test: 0.745109

Epoch: 96
Loss: 0.07202931202287666
train: 0.982820	val: 0.790234	test: 0.764511

Epoch: 97
Loss: 0.07260806907491246
train: 0.982454	val: 0.796511	test: 0.744505

Epoch: 98
Loss: 0.0702009270786304
train: 0.982077	val: 0.790956	test: 0.756305

Epoch: 99
Loss: 0.07192726639180641
train: 0.983015	val: 0.800059	test: 0.761229

Epoch: 100
Loss: 0.07068679207263981
train: 0.983334	val: 0.808973	test: 0.743595

best train: 0.940872	val: 0.823964	test: 0.761038
end
