11511808_0
--dataset=bbbp --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.5988332714999752
train: 0.821322	val: 0.909666	test: 0.622589

Epoch: 2
Loss: 0.4698398221060271
train: 0.865934	val: 0.912677	test: 0.645351

Epoch: 3
Loss: 0.3906157012226849
train: 0.881508	val: 0.909465	test: 0.642843

Epoch: 4
Loss: 0.3267073305962626
train: 0.912612	val: 0.910268	test: 0.680556

Epoch: 5
Loss: 0.299951976446255
train: 0.925007	val: 0.912577	test: 0.675733

Epoch: 6
Loss: 0.27531286943928474
train: 0.933011	val: 0.919904	test: 0.676698

Epoch: 7
Loss: 0.2509663716378056
train: 0.941490	val: 0.912376	test: 0.686246

Epoch: 8
Loss: 0.24054862049522732
train: 0.947942	val: 0.904848	test: 0.681906

Epoch: 9
Loss: 0.24585327654844244
train: 0.953171	val: 0.918800	test: 0.698978

Epoch: 10
Loss: 0.23949100124258088
train: 0.956378	val: 0.907257	test: 0.698302

Epoch: 11
Loss: 0.2476645285050878
train: 0.957232	val: 0.906755	test: 0.679977

Epoch: 12
Loss: 0.22786614598066232
train: 0.960564	val: 0.919603	test: 0.702257

Epoch: 13
Loss: 0.2137731496254097
train: 0.964752	val: 0.913179	test: 0.703125

Epoch: 14
Loss: 0.21747904720141958
train: 0.963494	val: 0.902439	test: 0.714988

Epoch: 15
Loss: 0.19456436362251628
train: 0.969948	val: 0.920205	test: 0.708140

Epoch: 16
Loss: 0.19607821482996085
train: 0.970210	val: 0.901636	test: 0.711902

Epoch: 17
Loss: 0.1887348915571176
train: 0.972472	val: 0.906655	test: 0.713927

Epoch: 18
Loss: 0.18780031626637841
train: 0.973363	val: 0.927030	test: 0.703414

Epoch: 19
Loss: 0.19704987279905176
train: 0.976683	val: 0.908662	test: 0.717496

Epoch: 20
Loss: 0.18374995894430582
train: 0.974602	val: 0.900833	test: 0.724441

Epoch: 21
Loss: 0.18692792751527412
train: 0.973757	val: 0.929941	test: 0.709587

Epoch: 22
Loss: 0.18774164783693945
train: 0.977349	val: 0.918800	test: 0.716725

Epoch: 23
Loss: 0.18106942608398308
train: 0.979542	val: 0.894209	test: 0.719715

Epoch: 24
Loss: 0.16107422211662642
train: 0.980468	val: 0.912476	test: 0.704765

Epoch: 25
Loss: 0.169430792470074
train: 0.982688	val: 0.905751	test: 0.720486

Epoch: 26
Loss: 0.1623491719959456
train: 0.981342	val: 0.904948	test: 0.692805

Epoch: 27
Loss: 0.16983144589747662
train: 0.985634	val: 0.913480	test: 0.692901

Epoch: 28
Loss: 0.16882088737894252
train: 0.983070	val: 0.909967	test: 0.711130

Epoch: 29
Loss: 0.1535532616610599
train: 0.987792	val: 0.905551	test: 0.708816

Epoch: 30
Loss: 0.1521128515737477
train: 0.987309	val: 0.916993	test: 0.695312

Epoch: 31
Loss: 0.14927330369255495
train: 0.987562	val: 0.900231	test: 0.694252

Epoch: 32
Loss: 0.14625677920669564
train: 0.986294	val: 0.906454	test: 0.686535

Epoch: 33
Loss: 0.16282537531035915
train: 0.986421	val: 0.909766	test: 0.700039

Epoch: 34
Loss: 0.13884670901673682
train: 0.989484	val: 0.912075	test: 0.692805

Epoch: 35
Loss: 0.12984926528361104
train: 0.986648	val: 0.912075	test: 0.687886

Epoch: 36
Loss: 0.13420586853885882
train: 0.990128	val: 0.907558	test: 0.702450

Epoch: 37
Loss: 0.13761003629933147
train: 0.988969	val: 0.899026	test: 0.696566

Epoch: 38
Loss: 0.1622027566214029
train: 0.989822	val: 0.897019	test: 0.687211

Epoch: 39
Loss: 0.14506347338176367
train: 0.990846	val: 0.899629	test: 0.717014

Epoch: 40
Loss: 0.13441360213620077
train: 0.990115	val: 0.909666	test: 0.695312

Epoch: 41
Loss: 0.1514344700619686
train: 0.990971	val: 0.905149	test: 0.711902

Epoch: 42
Loss: 0.13818184913685808
train: 0.991361	val: 0.907759	test: 0.708623

Epoch: 43
Loss: 0.13318207367250143
train: 0.992765	val: 0.909967	test: 0.703704

Epoch: 44
Loss: 0.12327354111319068
train: 0.992954	val: 0.906755	test: 0.709298

Epoch: 45
Loss: 0.12240426339517
train: 0.991660	val: 0.897621	test: 0.714892

Epoch: 46
Loss: 0.1416520045545094
train: 0.992616	val: 0.896417	test: 0.719232

Epoch: 47
Loss: 0.11687585571774683
train: 0.992642	val: 0.898324	test: 0.717882

Epoch: 48
Loss: 0.13461434588301557
train: 0.993065	val: 0.894911	test: 0.712288

Epoch: 49
Loss: 0.12488053339743321
train: 0.993869	val: 0.897722	test: 0.697917

Epoch: 50
Loss: 0.11812777553036442
train: 0.993469	val: 0.894811	test: 0.676890

Epoch: 51
Loss: 0.11961402910217862
train: 0.994542	val: 0.899428	test: 0.683160

Epoch: 52
Loss: 0.11498717205124363
train: 0.994903	val: 0.901235	test: 0.690201

Epoch: 53
Loss: 0.1190026858309995
train: 0.995872	val: 0.903242	test: 0.679495

Epoch: 54
Loss: 0.12248415985569562
train: 0.994482	val: 0.902941	test: 0.681424

Epoch: 55
Loss: 0.13037607451039793
train: 0.994774	val: 0.893907	test: 0.710745

Epoch: 56
Loss: 0.10960167423709329
train: 0.995215	val: 0.897621	test: 0.695795

Epoch: 57
Loss: 0.11625950345186652
train: 0.996136	val: 0.897521	test: 0.692612

Epoch: 58
Loss: 0.11988433220237539
train: 0.996072	val: 0.894710	test: 0.700521

Epoch: 59
Loss: 0.11231961026441271
train: 0.996370	val: 0.895413	test: 0.709491

Epoch: 60
Loss: 0.10915567608756152
train: 0.996143	val: 0.874737	test: 0.699363

Epoch: 61
Loss: 0.09784465051185424
train: 0.996876	val: 0.877647	test: 0.685957

Epoch: 62
Loss: 0.10429565234013705
train: 0.996573	val: 0.885978	test: 0.688175

Epoch: 63
Loss: 0.10677501618189016
train: 0.993692	val: 0.882967	test: 0.647569

Epoch: 64
Loss: 0.10571862029659493
train: 0.996488	val: 0.887484	test: 0.685185

Epoch: 65
Loss: 0.10418512636401853
train: 0.996610	val: 0.895815	test: 0.693866

Epoch: 66
Loss: 0.11292444618704181
train: 0.997105	val: 0.885075	test: 0.680170

Epoch: 67
Loss: 0.10699602978744435
train: 0.996667	val: 0.878250	test: 0.674672

Epoch: 68
Loss: 0.10199678718836395
train: 0.997388	val: 0.897119	test: 0.695023

Epoch: 69
Loss: 0.1105074364382153
train: 0.997739	val: 0.890394	test: 0.695602

Epoch: 70
Loss: 0.11473179095819881
train: 0.997087	val: 0.894309	test: 0.686535

Epoch: 71
Loss: 0.10690013254413315
train: 0.997352	val: 0.904346	test: 0.690876

Epoch: 72
Loss: 0.09735163705564996
train: 0.997899	val: 0.883870	test: 0.667631

Epoch: 73
Loss: 0.10466272112221972
train: 0.997513	val: 0.891499	test: 0.688754

Epoch: 74
Loss: 0.1078328674362953
train: 0.997517	val: 0.897019	test: 0.684896

Epoch: 75
Loss: 0.08219267397851704
train: 0.998085	val: 0.879354	test: 0.663098

Epoch: 76
Loss: 0.10139345610058073
train: 0.998003	val: 0.891599	test: 0.687886

Epoch: 77
Loss: 0.09639853257993414
train: 0.998133	val: 0.890896	test: 0.693480

Epoch: 78
Loss: 0.09016161858782842
train: 0.997939	val: 0.890997	test: 0.694541

Epoch: 79
Loss: 0.10828687737784629
train: 0.997600	val: 0.891298	test: 0.667245

Epoch: 80
Loss: 0.108806210350707
train: 0.997665	val: 0.886881	test: 0.675733

Epoch: 81
Loss: 0.08852215927508855
train: 0.995952	val: 0.895714	test: 0.673804

Epoch: 82
Loss: 0.0950841515179928
train: 0.998423	val: 0.883469	test: 0.665027

Epoch: 83
Loss: 0.09442031773613993
train: 0.998207	val: 0.880257	test: 0.666763

Epoch: 84
Loss: 0.0846434549451142
train: 0.998018	val: 0.872829	test: 0.684028

Epoch: 85
Loss: 0.09012479436473023
train: 0.998599	val: 0.887785	test: 0.682292

Epoch: 86
Loss: 0.08030030304570683
train: 0.998725	val: 0.890796	test: 0.687693

Epoch: 87
Loss: 0.08924244731059064
train: 0.998954	val: 0.874536	test: 0.666281

Epoch: 88
Loss: 0.07955353798639718
train: 0.998579	val: 0.860885	test: 0.681038

Epoch: 89
Loss: 0.08584958469184394
train: 0.998454	val: 0.873833	test: 0.704282

Epoch: 90
Loss: 0.09471898180245522
train: 0.998545	val: 0.873733	test: 0.699267

Epoch: 91
Loss: 0.08328323972557465
train: 0.998624	val: 0.873131	test: 0.666860

Epoch: 92
Loss: 0.07914690880337823
train: 0.998774	val: 0.890194	test: 0.663773

Epoch: 93
Loss: 0.08098007877935186
train: 0.998816	val: 0.880458	test: 0.676022

Epoch: 94
Loss: 0.07916786666304887
train: 0.998697	val: 0.871525	test: 0.693480

Epoch: 95
Loss: 0.07279098391083182
train: 0.999010	val: 0.878450	test: 0.697434

Epoch: 96
Loss: 0.0824183476634928
train: 0.999094	val: 0.877246	test: 0.699460

Epoch: 97
Loss: 0.07811608028112522
train: 0.998923	val: 0.872729	test: 0.698592

Epoch: 98
Loss: 0.09055664562086949
train: 0.999268	val: 0.861989	test: 0.694541

Epoch: 99
Loss: 0.07392870963880253
train: 0.999220	val: 0.871525	test: 0.700714

Epoch: 100
Loss: 0.08461841839595985
train: 0.999217	val: 0.864699	test: 0.686150

best train: 0.973757	val: 0.929941	test: 0.709587
end
