11511808_0
--dataset=clintox --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='clintox', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6338879363661839
train: 0.656136	val: 0.674156	test: 0.431446

Epoch: 2
Loss: 0.5526897598019398
train: 0.709296	val: 0.775861	test: 0.473198

Epoch: 3
Loss: 0.49091591520134614
train: 0.747399	val: 0.841342	test: 0.520144

Epoch: 4
Loss: 0.4342756483562423
train: 0.788198	val: 0.876705	test: 0.554204

Epoch: 5
Loss: 0.3981601513098364
train: 0.800629	val: 0.871448	test: 0.559126

Epoch: 6
Loss: 0.3639123823681597
train: 0.812378	val: 0.824584	test: 0.587958

Epoch: 7
Loss: 0.32807121933517286
train: 0.836097	val: 0.833562	test: 0.578514

Epoch: 8
Loss: 0.30689104888225216
train: 0.848389	val: 0.839269	test: 0.553949

Epoch: 9
Loss: 0.28765556043536356
train: 0.861095	val: 0.827694	test: 0.572793

Epoch: 10
Loss: 0.27077513639007605
train: 0.870357	val: 0.839719	test: 0.584535

Epoch: 11
Loss: 0.2510988532172537
train: 0.876428	val: 0.863256	test: 0.599274

Epoch: 12
Loss: 0.2419423783618841
train: 0.889579	val: 0.846961	test: 0.588874

Epoch: 13
Loss: 0.23222380076240748
train: 0.906646	val: 0.841817	test: 0.613039

Epoch: 14
Loss: 0.21660034695060784
train: 0.913705	val: 0.855378	test: 0.615512

Epoch: 15
Loss: 0.20637505686776686
train: 0.923507	val: 0.827332	test: 0.633025

Epoch: 16
Loss: 0.2062827635477277
train: 0.920487	val: 0.813746	test: 0.617447

Epoch: 17
Loss: 0.1980133506989032
train: 0.930436	val: 0.834687	test: 0.638521

Epoch: 18
Loss: 0.20461023473847706
train: 0.932492	val: 0.822525	test: 0.661351

Epoch: 19
Loss: 0.18941803617022993
train: 0.928265	val: 0.860972	test: 0.637866

Epoch: 20
Loss: 0.18781492021318255
train: 0.930282	val: 0.887307	test: 0.638990

Epoch: 21
Loss: 0.1741380354556123
train: 0.947164	val: 0.835611	test: 0.684374

Epoch: 22
Loss: 0.1820510033786194
train: 0.945472	val: 0.845739	test: 0.662543

Epoch: 23
Loss: 0.17606671515815803
train: 0.950330	val: 0.893263	test: 0.698902

Epoch: 24
Loss: 0.17502031525661435
train: 0.949660	val: 0.907523	test: 0.736528

Epoch: 25
Loss: 0.17182014822313255
train: 0.952929	val: 0.890329	test: 0.738063

Epoch: 26
Loss: 0.15841841911339044
train: 0.953224	val: 0.866054	test: 0.744353

Epoch: 27
Loss: 0.167565009454411
train: 0.954693	val: 0.852581	test: 0.729687

Epoch: 28
Loss: 0.1612443196242333
train: 0.958574	val: 0.852693	test: 0.738655

Epoch: 29
Loss: 0.16348978526364832
train: 0.961614	val: 0.848473	test: 0.762865

Epoch: 30
Loss: 0.16276607818576547
train: 0.961774	val: 0.852855	test: 0.763947

Epoch: 31
Loss: 0.1587283870820055
train: 0.963543	val: 0.861158	test: 0.741492

Epoch: 32
Loss: 0.15432219445970155
train: 0.961736	val: 0.843577	test: 0.728900

Epoch: 33
Loss: 0.15443881899284936
train: 0.965335	val: 0.818578	test: 0.746751

Epoch: 34
Loss: 0.1453084838514484
train: 0.965920	val: 0.842379	test: 0.765913

Epoch: 35
Loss: 0.15183801813923248
train: 0.966482	val: 0.884260	test: 0.754377

Epoch: 36
Loss: 0.14485491479077264
train: 0.964403	val: 0.888006	test: 0.741191

Epoch: 37
Loss: 0.1519788564965535
train: 0.968328	val: 0.834325	test: 0.749205

Epoch: 38
Loss: 0.1377008545487849
train: 0.967274	val: 0.815394	test: 0.760361

Epoch: 39
Loss: 0.1405270336327264
train: 0.968549	val: 0.837734	test: 0.769148

Epoch: 40
Loss: 0.14308057473431143
train: 0.969552	val: 0.842404	test: 0.747013

Epoch: 41
Loss: 0.1495811282181578
train: 0.972059	val: 0.844164	test: 0.749712

Epoch: 42
Loss: 0.14032218287849454
train: 0.974350	val: 0.873609	test: 0.766937

Epoch: 43
Loss: 0.13806342536163554
train: 0.975588	val: 0.869863	test: 0.767512

Epoch: 44
Loss: 0.14458975251228426
train: 0.974979	val: 0.885771	test: 0.771184

Epoch: 45
Loss: 0.13745810200850583
train: 0.975034	val: 0.906487	test: 0.792382

Epoch: 46
Loss: 0.14322451500961525
train: 0.973648	val: 0.892339	test: 0.787810

Epoch: 47
Loss: 0.1392869187457824
train: 0.976825	val: 0.897895	test: 0.787149

Epoch: 48
Loss: 0.13031673581515085
train: 0.974509	val: 0.827381	test: 0.749816

Epoch: 49
Loss: 0.14030951996996083
train: 0.976037	val: 0.821425	test: 0.766612

Epoch: 50
Loss: 0.1306306024367068
train: 0.976907	val: 0.865716	test: 0.785294

Epoch: 51
Loss: 0.13078983748037437
train: 0.977872	val: 0.879952	test: 0.786338

Epoch: 52
Loss: 0.1422094670376292
train: 0.975022	val: 0.877067	test: 0.805862

Epoch: 53
Loss: 0.1349296943852385
train: 0.977456	val: 0.912106	test: 0.806443

Epoch: 54
Loss: 0.11783895787374254
train: 0.975876	val: 0.910408	test: 0.818835

Epoch: 55
Loss: 0.13694669412928823
train: 0.978614	val: 0.910570	test: 0.824251

Epoch: 56
Loss: 0.13266819500445926
train: 0.974716	val: 0.885384	test: 0.783572

Epoch: 57
Loss: 0.1366815116749592
train: 0.978472	val: 0.857363	test: 0.828961

Epoch: 58
Loss: 0.12229315075191347
train: 0.980514	val: 0.826408	test: 0.845736

Epoch: 59
Loss: 0.129262457628791
train: 0.979132	val: 0.819752	test: 0.816136

Epoch: 60
Loss: 0.12594011947868242
train: 0.982205	val: 0.854478	test: 0.813370

Epoch: 61
Loss: 0.12584575378632626
train: 0.980963	val: 0.903777	test: 0.820191

Epoch: 62
Loss: 0.12798245997424762
train: 0.980445	val: 0.898720	test: 0.798791

Epoch: 63
Loss: 0.12076349140814209
train: 0.979358	val: 0.873609	test: 0.783264

Epoch: 64
Loss: 0.13157855913873137
train: 0.982283	val: 0.861222	test: 0.785251

Epoch: 65
Loss: 0.11411132503195591
train: 0.980771	val: 0.833763	test: 0.782954

Epoch: 66
Loss: 0.12036667557876664
train: 0.982440	val: 0.885321	test: 0.768930

Epoch: 67
Loss: 0.12478025447657377
train: 0.984287	val: 0.901929	test: 0.785668

Epoch: 68
Loss: 0.1220044691343322
train: 0.984230	val: 0.893038	test: 0.786136

Epoch: 69
Loss: 0.1275374884567237
train: 0.983391	val: 0.869301	test: 0.788198

Epoch: 70
Loss: 0.11695110493085766
train: 0.983841	val: 0.878416	test: 0.806992

Epoch: 71
Loss: 0.1255549113955639
train: 0.983365	val: 0.853529	test: 0.807323

Epoch: 72
Loss: 0.12477769739287012
train: 0.983430	val: 0.823336	test: 0.801989

Epoch: 73
Loss: 0.11214316368395463
train: 0.983814	val: 0.826046	test: 0.807866

Epoch: 74
Loss: 0.1112793877071739
train: 0.984444	val: 0.835049	test: 0.829159

Epoch: 75
Loss: 0.10783399205636413
train: 0.985170	val: 0.836672	test: 0.824162

Epoch: 76
Loss: 0.12082022875148415
train: 0.984057	val: 0.857613	test: 0.805160

Epoch: 77
Loss: 0.1194092582023067
train: 0.984623	val: 0.889679	test: 0.816990

Epoch: 78
Loss: 0.11677576256617525
train: 0.983351	val: 0.876743	test: 0.807047

Epoch: 79
Loss: 0.12165048412776536
train: 0.983310	val: 0.909372	test: 0.817496

Epoch: 80
Loss: 0.11798124370240178
train: 0.980512	val: 0.924405	test: 0.830712

Epoch: 81
Loss: 0.10098571229374917
train: 0.982676	val: 0.885746	test: 0.821711

Epoch: 82
Loss: 0.12461850160751849
train: 0.985857	val: 0.841680	test: 0.835603

Epoch: 83
Loss: 0.11926489362402326
train: 0.982689	val: 0.825234	test: 0.809183

Epoch: 84
Loss: 0.10570661743104744
train: 0.984772	val: 0.823361	test: 0.786715

Epoch: 85
Loss: 0.11778951995322381
train: 0.985348	val: 0.814808	test: 0.795004

Epoch: 86
Loss: 0.11105007064423607
train: 0.986034	val: 0.841617	test: 0.834904

Epoch: 87
Loss: 0.11486476820094087
train: 0.986024	val: 0.835861	test: 0.835154

Epoch: 88
Loss: 0.10978946756718048
train: 0.985731	val: 0.843353	test: 0.827915

Epoch: 89
Loss: 0.1007084256314533
train: 0.986037	val: 0.822525	test: 0.829122

Epoch: 90
Loss: 0.10027063945257328
train: 0.985929	val: 0.817830	test: 0.818891

Epoch: 91
Loss: 0.10489909438988614
train: 0.984280	val: 0.850795	test: 0.802475

Epoch: 92
Loss: 0.09802168510818059
train: 0.985791	val: 0.862283	test: 0.830883

Epoch: 93
Loss: 0.10717006604376852
train: 0.985865	val: 0.852605	test: 0.838154

Epoch: 94
Loss: 0.10088262561142938
train: 0.985170	val: 0.795178	test: 0.818742

Epoch: 95
Loss: 0.10524897389024834
train: 0.984711	val: 0.820202	test: 0.812619

Epoch: 96
Loss: 0.10962376791543652
train: 0.984564	val: 0.858649	test: 0.809450

Epoch: 97
Loss: 0.10569307889091414
train: 0.987133	val: 0.851969	test: 0.820886

Epoch: 98
Loss: 0.11233368008289131
train: 0.986056	val: 0.842379	test: 0.840873

Epoch: 99
Loss: 0.10898153328241042
train: 0.986585	val: 0.854791	test: 0.825383

Epoch: 100
Loss: 0.1027870672635062
train: 0.987456	val: 0.880876	test: 0.821973

best train: 0.980512	val: 0.924405	test: 0.830712
end
