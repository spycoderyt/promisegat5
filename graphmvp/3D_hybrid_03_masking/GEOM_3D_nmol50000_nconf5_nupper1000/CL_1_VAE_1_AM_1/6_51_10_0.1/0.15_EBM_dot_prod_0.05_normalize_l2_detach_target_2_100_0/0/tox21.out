11511808_0
--dataset=tox21 --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.521535497202077
train: 0.745807	val: 0.695455	test: 0.667413

Epoch: 2
Loss: 0.33219182399855335
train: 0.771390	val: 0.702546	test: 0.669372

Epoch: 3
Loss: 0.23847616331063073
train: 0.806072	val: 0.751444	test: 0.707576

Epoch: 4
Loss: 0.2055935083901476
train: 0.823727	val: 0.763025	test: 0.719210

Epoch: 5
Loss: 0.19477085568784872
train: 0.833893	val: 0.737058	test: 0.715172

Epoch: 6
Loss: 0.18973786138915383
train: 0.841378	val: 0.747834	test: 0.716873

Epoch: 7
Loss: 0.18656091136664713
train: 0.850487	val: 0.744339	test: 0.716881

Epoch: 8
Loss: 0.18147944677622305
train: 0.859187	val: 0.765859	test: 0.735493

Epoch: 9
Loss: 0.18041029689878404
train: 0.864692	val: 0.766298	test: 0.736163

Epoch: 10
Loss: 0.17865634334924035
train: 0.871034	val: 0.761768	test: 0.745255

Epoch: 11
Loss: 0.17530344689124672
train: 0.872067	val: 0.755521	test: 0.743898

Epoch: 12
Loss: 0.1736859141078547
train: 0.875564	val: 0.774739	test: 0.738640

Epoch: 13
Loss: 0.17113207904327138
train: 0.877960	val: 0.757666	test: 0.729214

Epoch: 14
Loss: 0.170579223093849
train: 0.884404	val: 0.777782	test: 0.750842

Epoch: 15
Loss: 0.16808326775671906
train: 0.883649	val: 0.786841	test: 0.754372

Epoch: 16
Loss: 0.16772906308128757
train: 0.888980	val: 0.778413	test: 0.754454

Epoch: 17
Loss: 0.16452836851868935
train: 0.892602	val: 0.781436	test: 0.750624

Epoch: 18
Loss: 0.16577857106122973
train: 0.895127	val: 0.791881	test: 0.755601

Epoch: 19
Loss: 0.16277942576798743
train: 0.896904	val: 0.792929	test: 0.748632

Epoch: 20
Loss: 0.16199142535798644
train: 0.901069	val: 0.793567	test: 0.757649

Epoch: 21
Loss: 0.15966583987665137
train: 0.902981	val: 0.790020	test: 0.767021

Epoch: 22
Loss: 0.16086074874181655
train: 0.899872	val: 0.791669	test: 0.756297

Epoch: 23
Loss: 0.15864419675942687
train: 0.905098	val: 0.790826	test: 0.751759

Epoch: 24
Loss: 0.15767113017766887
train: 0.907698	val: 0.789347	test: 0.767399

Epoch: 25
Loss: 0.15647481709431996
train: 0.910906	val: 0.777445	test: 0.751306

Epoch: 26
Loss: 0.15631160508749206
train: 0.913748	val: 0.787095	test: 0.753316

Epoch: 27
Loss: 0.15520040411805036
train: 0.911838	val: 0.783290	test: 0.750658

Epoch: 28
Loss: 0.15506533202201264
train: 0.916328	val: 0.787277	test: 0.754377

Epoch: 29
Loss: 0.1546504384412532
train: 0.917151	val: 0.783718	test: 0.765972

Epoch: 30
Loss: 0.15321148522066502
train: 0.919896	val: 0.774710	test: 0.750080

Epoch: 31
Loss: 0.15029988442932188
train: 0.920646	val: 0.777277	test: 0.758474

Epoch: 32
Loss: 0.15085678546078735
train: 0.924040	val: 0.779441	test: 0.755080

Epoch: 33
Loss: 0.14900694602997208
train: 0.925953	val: 0.773582	test: 0.755484

Epoch: 34
Loss: 0.14925958536596456
train: 0.924287	val: 0.782979	test: 0.755592

Epoch: 35
Loss: 0.14758798403290677
train: 0.926410	val: 0.781970	test: 0.759203

Epoch: 36
Loss: 0.1480516150056854
train: 0.930228	val: 0.781791	test: 0.753452

Epoch: 37
Loss: 0.14574768283755676
train: 0.931342	val: 0.785746	test: 0.756395

Epoch: 38
Loss: 0.14666125266345356
train: 0.929683	val: 0.780434	test: 0.760329

Epoch: 39
Loss: 0.14552415765535975
train: 0.931409	val: 0.778410	test: 0.760386

Epoch: 40
Loss: 0.1444404273940004
train: 0.934366	val: 0.789503	test: 0.759853

Epoch: 41
Loss: 0.14351504519668046
train: 0.933652	val: 0.779967	test: 0.753228

Epoch: 42
Loss: 0.14070497658961936
train: 0.938446	val: 0.789593	test: 0.763331

Epoch: 43
Loss: 0.1412923446500553
train: 0.939313	val: 0.783881	test: 0.759675

Epoch: 44
Loss: 0.14169962736962355
train: 0.938468	val: 0.778894	test: 0.766907

Epoch: 45
Loss: 0.14105584955281333
train: 0.938317	val: 0.794701	test: 0.758348

Epoch: 46
Loss: 0.14060639534817548
train: 0.941817	val: 0.782890	test: 0.758543

Epoch: 47
Loss: 0.13719859617037516
train: 0.943837	val: 0.783267	test: 0.756818

Epoch: 48
Loss: 0.13772972553413587
train: 0.945606	val: 0.790986	test: 0.746115

Epoch: 49
Loss: 0.13728202196444758
train: 0.945544	val: 0.779101	test: 0.755558

Epoch: 50
Loss: 0.135230848038557
train: 0.945930	val: 0.786404	test: 0.764406

Epoch: 51
Loss: 0.13497611374809862
train: 0.948213	val: 0.782125	test: 0.759286

Epoch: 52
Loss: 0.13593173293944874
train: 0.950997	val: 0.783207	test: 0.752308

Epoch: 53
Loss: 0.1333355613322075
train: 0.949565	val: 0.778957	test: 0.757276

Epoch: 54
Loss: 0.13429095226660778
train: 0.949717	val: 0.773113	test: 0.760336

Epoch: 55
Loss: 0.1336212248509361
train: 0.950846	val: 0.780868	test: 0.758156

Epoch: 56
Loss: 0.13330383762187972
train: 0.951192	val: 0.780334	test: 0.758750

Epoch: 57
Loss: 0.1319844784829321
train: 0.953207	val: 0.787156	test: 0.765552

Epoch: 58
Loss: 0.12990599049536652
train: 0.955249	val: 0.784369	test: 0.760837

Epoch: 59
Loss: 0.13158695240155416
train: 0.955940	val: 0.776970	test: 0.748212

Epoch: 60
Loss: 0.128128279089016
train: 0.956469	val: 0.776176	test: 0.762119

Epoch: 61
Loss: 0.1285394456184797
train: 0.958444	val: 0.767882	test: 0.741140

Epoch: 62
Loss: 0.12808137466569447
train: 0.957906	val: 0.775154	test: 0.758262

Epoch: 63
Loss: 0.12725804678677005
train: 0.959929	val: 0.778806	test: 0.753223

Epoch: 64
Loss: 0.12452540256247234
train: 0.961571	val: 0.773587	test: 0.763024

Epoch: 65
Loss: 0.12383562614223086
train: 0.961853	val: 0.769446	test: 0.752964

Epoch: 66
Loss: 0.12494809724558825
train: 0.963636	val: 0.771076	test: 0.755593

Epoch: 67
Loss: 0.12280691642708272
train: 0.963636	val: 0.783795	test: 0.745383

Epoch: 68
Loss: 0.12372740125826968
train: 0.961292	val: 0.775846	test: 0.747138

Epoch: 69
Loss: 0.12252663643702234
train: 0.964187	val: 0.772850	test: 0.750937

Epoch: 70
Loss: 0.1237584098768282
train: 0.967167	val: 0.778978	test: 0.752451

Epoch: 71
Loss: 0.12223878216976637
train: 0.963035	val: 0.774204	test: 0.748778

Epoch: 72
Loss: 0.12040567410996748
train: 0.966159	val: 0.776469	test: 0.746969

Epoch: 73
Loss: 0.12205080702558543
train: 0.968136	val: 0.775763	test: 0.752160

Epoch: 74
Loss: 0.12018422621805908
train: 0.967249	val: 0.759955	test: 0.744088

Epoch: 75
Loss: 0.11968014412209531
train: 0.968777	val: 0.763683	test: 0.748568

Epoch: 76
Loss: 0.11849913030397573
train: 0.969089	val: 0.768162	test: 0.752207

Epoch: 77
Loss: 0.11773005129129407
train: 0.969969	val: 0.770010	test: 0.753062

Epoch: 78
Loss: 0.116776990427889
train: 0.969358	val: 0.769491	test: 0.751428

Epoch: 79
Loss: 0.11644270037569139
train: 0.969934	val: 0.783072	test: 0.747768

Epoch: 80
Loss: 0.11704186117744507
train: 0.970692	val: 0.779414	test: 0.741021

Epoch: 81
Loss: 0.11492415304124877
train: 0.971915	val: 0.775150	test: 0.750095

Epoch: 82
Loss: 0.11599815828255189
train: 0.971606	val: 0.777286	test: 0.747397

Epoch: 83
Loss: 0.11582813841124649
train: 0.971995	val: 0.777672	test: 0.753442

Epoch: 84
Loss: 0.11275673453945828
train: 0.973229	val: 0.771823	test: 0.751789

Epoch: 85
Loss: 0.11188733959688425
train: 0.973224	val: 0.771703	test: 0.744443

Epoch: 86
Loss: 0.111933470630295
train: 0.974587	val: 0.771776	test: 0.738347

Epoch: 87
Loss: 0.1117672693959753
train: 0.973977	val: 0.773909	test: 0.749604

Epoch: 88
Loss: 0.11132526626822659
train: 0.975038	val: 0.771617	test: 0.750168

Epoch: 89
Loss: 0.11060560896974449
train: 0.975489	val: 0.767251	test: 0.741418

Epoch: 90
Loss: 0.10988125595814091
train: 0.977364	val: 0.774579	test: 0.747847

Epoch: 91
Loss: 0.10955414658059082
train: 0.975047	val: 0.762520	test: 0.747748

Epoch: 92
Loss: 0.10781807292334193
train: 0.976139	val: 0.768385	test: 0.757446

Epoch: 93
Loss: 0.10629560178442161
train: 0.978769	val: 0.769232	test: 0.744489

Epoch: 94
Loss: 0.10683029058555338
train: 0.978339	val: 0.772779	test: 0.750144

Epoch: 95
Loss: 0.10906266370995521
train: 0.975603	val: 0.774907	test: 0.752741

Epoch: 96
Loss: 0.10688785026158759
train: 0.978613	val: 0.767426	test: 0.749971

Epoch: 97
Loss: 0.10527634008526711
train: 0.978279	val: 0.763162	test: 0.745122

Epoch: 98
Loss: 0.10605735818845222
train: 0.979418	val: 0.774141	test: 0.749884

Epoch: 99
Loss: 0.10680434213538316
train: 0.980881	val: 0.766726	test: 0.753677

Epoch: 100
Loss: 0.1058311695993786
train: 0.980114	val: 0.760701	test: 0.741865

best train: 0.938317	val: 0.794701	test: 0.758348
end
