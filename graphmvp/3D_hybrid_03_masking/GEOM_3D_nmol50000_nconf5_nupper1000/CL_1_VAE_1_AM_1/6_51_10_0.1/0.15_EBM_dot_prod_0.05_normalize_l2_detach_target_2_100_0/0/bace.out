11511808_0
--dataset=bace --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.15_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6664791425378531
train: 0.720382	val: 0.587546	test: 0.636063

Epoch: 2
Loss: 0.5939423825007414
train: 0.773890	val: 0.616117	test: 0.703878

Epoch: 3
Loss: 0.5392256836996809
train: 0.843134	val: 0.709890	test: 0.773778

Epoch: 4
Loss: 0.49659138088345306
train: 0.865950	val: 0.699267	test: 0.795166

Epoch: 5
Loss: 0.4664301914506435
train: 0.885188	val: 0.681319	test: 0.807512

Epoch: 6
Loss: 0.4633442047460389
train: 0.892591	val: 0.639927	test: 0.816728

Epoch: 7
Loss: 0.43451394538718235
train: 0.895933	val: 0.668132	test: 0.813945

Epoch: 8
Loss: 0.4155309535075796
train: 0.904070	val: 0.696337	test: 0.822640

Epoch: 9
Loss: 0.4178253984434847
train: 0.910505	val: 0.702930	test: 0.824378

Epoch: 10
Loss: 0.4064016071300565
train: 0.913970	val: 0.696703	test: 0.812207

Epoch: 11
Loss: 0.3965097705115993
train: 0.918322	val: 0.711355	test: 0.813598

Epoch: 12
Loss: 0.40791039756762915
train: 0.920317	val: 0.724908	test: 0.814467

Epoch: 13
Loss: 0.38713141181434646
train: 0.921136	val: 0.712821	test: 0.808381

Epoch: 14
Loss: 0.3953618051840214
train: 0.923950	val: 0.703663	test: 0.804208

Epoch: 15
Loss: 0.37465666932312186
train: 0.926264	val: 0.675092	test: 0.798818

Epoch: 16
Loss: 0.3936414919002907
train: 0.926501	val: 0.684615	test: 0.794992

Epoch: 17
Loss: 0.37950560680848033
train: 0.928251	val: 0.704762	test: 0.806990

Epoch: 18
Loss: 0.3676723685984669
train: 0.925585	val: 0.694872	test: 0.795862

Epoch: 19
Loss: 0.37452172501847125
train: 0.930928	val: 0.715018	test: 0.794471

Epoch: 20
Loss: 0.36129052527971045
train: 0.935163	val: 0.709158	test: 0.791167

Epoch: 21
Loss: 0.37235866053745215
train: 0.934244	val: 0.706227	test: 0.777952

Epoch: 22
Loss: 0.3707587581617018
train: 0.935728	val: 0.705128	test: 0.773257

Epoch: 23
Loss: 0.3642999842047574
train: 0.934869	val: 0.683150	test: 0.767693

Epoch: 24
Loss: 0.3587263951202582
train: 0.935300	val: 0.671795	test: 0.762824

Epoch: 25
Loss: 0.35776793409698904
train: 0.936062	val: 0.687179	test: 0.782646

Epoch: 26
Loss: 0.36113548950522645
train: 0.939446	val: 0.685714	test: 0.784385

Epoch: 27
Loss: 0.36292189841984396
train: 0.937263	val: 0.689011	test: 0.775691

Epoch: 28
Loss: 0.3401439497129743
train: 0.943105	val: 0.686813	test: 0.788385

Epoch: 29
Loss: 0.358275625728555
train: 0.943376	val: 0.680952	test: 0.786472

Epoch: 30
Loss: 0.3557240690658434
train: 0.943419	val: 0.661538	test: 0.782820

Epoch: 31
Loss: 0.34015464116700916
train: 0.942069	val: 0.676190	test: 0.783690

Epoch: 32
Loss: 0.3407388931923282
train: 0.944509	val: 0.690476	test: 0.776908

Epoch: 33
Loss: 0.33394951638703074
train: 0.945388	val: 0.670330	test: 0.776734

Epoch: 34
Loss: 0.33724674515378944
train: 0.944535	val: 0.634432	test: 0.784385

Epoch: 35
Loss: 0.3410693470373332
train: 0.948533	val: 0.658242	test: 0.780734

Epoch: 36
Loss: 0.348726490475164
train: 0.947049	val: 0.685714	test: 0.774474

Epoch: 37
Loss: 0.3361643934553106
train: 0.947617	val: 0.695238	test: 0.771344

Epoch: 38
Loss: 0.3325274942410969
train: 0.950200	val: 0.701099	test: 0.774474

Epoch: 39
Loss: 0.3224642426285145
train: 0.952834	val: 0.694139	test: 0.780038

Epoch: 40
Loss: 0.3178704516658643
train: 0.954880	val: 0.684615	test: 0.774300

Epoch: 41
Loss: 0.31887505858011106
train: 0.954643	val: 0.660073	test: 0.780908

Epoch: 42
Loss: 0.32148457099399297
train: 0.951998	val: 0.668498	test: 0.783690

Epoch: 43
Loss: 0.32215177234307213
train: 0.952723	val: 0.683150	test: 0.768388

Epoch: 44
Loss: 0.31567130894222506
train: 0.955982	val: 0.689744	test: 0.764041

Epoch: 45
Loss: 0.3103460822540987
train: 0.957337	val: 0.680952	test: 0.777082

Epoch: 46
Loss: 0.3068169984316343
train: 0.958142	val: 0.675092	test: 0.787167

Epoch: 47
Loss: 0.32479818875833744
train: 0.960274	val: 0.673260	test: 0.792732

Epoch: 48
Loss: 0.31994228191667534
train: 0.959261	val: 0.689377	test: 0.798644

Epoch: 49
Loss: 0.2956388215478782
train: 0.957922	val: 0.684249	test: 0.787167

Epoch: 50
Loss: 0.2986418925018041
train: 0.960451	val: 0.675092	test: 0.793427

Epoch: 51
Loss: 0.28935897417651113
train: 0.962443	val: 0.673260	test: 0.800383

Epoch: 52
Loss: 0.29238163760854874
train: 0.962714	val: 0.665201	test: 0.795340

Epoch: 53
Loss: 0.29847122849386293
train: 0.962226	val: 0.663004	test: 0.789602

Epoch: 54
Loss: 0.2964881347788566
train: 0.963696	val: 0.669963	test: 0.792906

Epoch: 55
Loss: 0.2833735133401051
train: 0.964187	val: 0.674359	test: 0.790471

Epoch: 56
Loss: 0.2988036156176556
train: 0.963947	val: 0.677656	test: 0.779169

Epoch: 57
Loss: 0.2814979884186798
train: 0.962680	val: 0.697802	test: 0.773605

Epoch: 58
Loss: 0.30532788946507344
train: 0.967295	val: 0.679487	test: 0.780560

Epoch: 59
Loss: 0.2980432343293568
train: 0.966356	val: 0.669231	test: 0.778647

Epoch: 60
Loss: 0.2749718703141259
train: 0.965882	val: 0.671795	test: 0.785429

Epoch: 61
Loss: 0.3044280791679551
train: 0.969215	val: 0.643590	test: 0.789254

Epoch: 62
Loss: 0.2760259145493216
train: 0.968445	val: 0.635531	test: 0.780386

Epoch: 63
Loss: 0.28757019107550014
train: 0.967189	val: 0.640659	test: 0.771344

Epoch: 64
Loss: 0.2714602778081707
train: 0.968955	val: 0.675824	test: 0.773083

Epoch: 65
Loss: 0.29491921749539357
train: 0.966992	val: 0.689744	test: 0.774126

Epoch: 66
Loss: 0.28182525359380783
train: 0.968642	val: 0.698535	test: 0.774126

Epoch: 67
Loss: 0.2748353544091081
train: 0.972397	val: 0.693407	test: 0.780386

Epoch: 68
Loss: 0.27450286879579866
train: 0.970870	val: 0.687546	test: 0.775865

Epoch: 69
Loss: 0.27016565030100037
train: 0.968841	val: 0.689377	test: 0.757781

Epoch: 70
Loss: 0.25161278704987045
train: 0.971592	val: 0.679121	test: 0.762998

Epoch: 71
Loss: 0.26748169858342496
train: 0.972754	val: 0.682051	test: 0.764910

Epoch: 72
Loss: 0.26883252158031573
train: 0.971658	val: 0.695604	test: 0.767345

Epoch: 73
Loss: 0.2792637516663406
train: 0.972215	val: 0.702564	test: 0.779864

Epoch: 74
Loss: 0.255524318805635
train: 0.973074	val: 0.712821	test: 0.784559

Epoch: 75
Loss: 0.25491311616694706
train: 0.975214	val: 0.702198	test: 0.780212

Epoch: 76
Loss: 0.2651093320291437
train: 0.973121	val: 0.671795	test: 0.770301

Epoch: 77
Loss: 0.25615891309729155
train: 0.974284	val: 0.657143	test: 0.773778

Epoch: 78
Loss: 0.2535230377307339
train: 0.975888	val: 0.643956	test: 0.774474

Epoch: 79
Loss: 0.24672650201033736
train: 0.974977	val: 0.650916	test: 0.749435

Epoch: 80
Loss: 0.25695827312808445
train: 0.974281	val: 0.640293	test: 0.748392

Epoch: 81
Loss: 0.24121331710463859
train: 0.975896	val: 0.649084	test: 0.763693

Epoch: 82
Loss: 0.2489123943054304
train: 0.978610	val: 0.661905	test: 0.774126

Epoch: 83
Loss: 0.2517949802582072
train: 0.977626	val: 0.655311	test: 0.768910

Epoch: 84
Loss: 0.24450262230797062
train: 0.976772	val: 0.676557	test: 0.764215

Epoch: 85
Loss: 0.2590575803229503
train: 0.978756	val: 0.667766	test: 0.760911

Epoch: 86
Loss: 0.26614756473384005
train: 0.979041	val: 0.677289	test: 0.767519

Epoch: 87
Loss: 0.24070664023322683
train: 0.978599	val: 0.653114	test: 0.761954

Epoch: 88
Loss: 0.2432417122223467
train: 0.977277	val: 0.638462	test: 0.754477

Epoch: 89
Loss: 0.25556735436873684
train: 0.977186	val: 0.640293	test: 0.755347

Epoch: 90
Loss: 0.24709491409308199
train: 0.978291	val: 0.667033	test: 0.754999

Epoch: 91
Loss: 0.23279876520622245
train: 0.977788	val: 0.667399	test: 0.754999

Epoch: 92
Loss: 0.2557890480862652
train: 0.979652	val: 0.662271	test: 0.771692

Epoch: 93
Loss: 0.24262634115560733
train: 0.981387	val: 0.652381	test: 0.763867

Epoch: 94
Loss: 0.23494866502046002
train: 0.981644	val: 0.658242	test: 0.760389

Epoch: 95
Loss: 0.24912801467029783
train: 0.982417	val: 0.677656	test: 0.765084

Epoch: 96
Loss: 0.2354587495908845
train: 0.981030	val: 0.669231	test: 0.755695

Epoch: 97
Loss: 0.23290292096978193
train: 0.981567	val: 0.673993	test: 0.767345

Epoch: 98
Loss: 0.23397946792445473
train: 0.982275	val: 0.675092	test: 0.774822

Epoch: 99
Loss: 0.22643663760210586
train: 0.982295	val: 0.659341	test: 0.777082

Epoch: 100
Loss: 0.22944805155849884
train: 0.983099	val: 0.671429	test: 0.773431

best train: 0.920317	val: 0.724908	test: 0.814467
end
