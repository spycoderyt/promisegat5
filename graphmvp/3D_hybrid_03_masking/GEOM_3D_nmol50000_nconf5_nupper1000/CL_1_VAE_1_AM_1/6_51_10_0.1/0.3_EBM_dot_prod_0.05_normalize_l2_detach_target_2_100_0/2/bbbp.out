9112533_2
--dataset=bbbp --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6264956440504087
train: 0.815501	val: 0.902640	test: 0.623264

Epoch: 2
Loss: 0.48419602926072375
train: 0.866862	val: 0.911774	test: 0.646316

Epoch: 3
Loss: 0.4071195419299599
train: 0.894360	val: 0.917294	test: 0.649691

Epoch: 4
Loss: 0.34128141632551706
train: 0.911767	val: 0.921409	test: 0.656829

Epoch: 5
Loss: 0.295051805445223
train: 0.922014	val: 0.918097	test: 0.673997

Epoch: 6
Loss: 0.27961096747094694
train: 0.935669	val: 0.919201	test: 0.679109

Epoch: 7
Loss: 0.2803033629102308
train: 0.940323	val: 0.925524	test: 0.683835

Epoch: 8
Loss: 0.27135217641595355
train: 0.946997	val: 0.925223	test: 0.686053

Epoch: 9
Loss: 0.2638545468699597
train: 0.953685	val: 0.919904	test: 0.702450

Epoch: 10
Loss: 0.23007686901646926
train: 0.956370	val: 0.921409	test: 0.696084

Epoch: 11
Loss: 0.22698304440779868
train: 0.958879	val: 0.918498	test: 0.693576

Epoch: 12
Loss: 0.2140085320180491
train: 0.963299	val: 0.918900	test: 0.694637

Epoch: 13
Loss: 0.19784071731587852
train: 0.958851	val: 0.916090	test: 0.698881

Epoch: 14
Loss: 0.22105745192687148
train: 0.966237	val: 0.918298	test: 0.706211

Epoch: 15
Loss: 0.2101435520697076
train: 0.968936	val: 0.916491	test: 0.706019

Epoch: 16
Loss: 0.19524522111941364
train: 0.966605	val: 0.925926	test: 0.688272

Epoch: 17
Loss: 0.19408810180742295
train: 0.967939	val: 0.912978	test: 0.702450

Epoch: 18
Loss: 0.18391902912557065
train: 0.970482	val: 0.910770	test: 0.698399

Epoch: 19
Loss: 0.18631880091106218
train: 0.973725	val: 0.926829	test: 0.694927

Epoch: 20
Loss: 0.1758940912207465
train: 0.971164	val: 0.923918	test: 0.705150

Epoch: 21
Loss: 0.16541831331936074
train: 0.976140	val: 0.920205	test: 0.703897

Epoch: 22
Loss: 0.17977351814732698
train: 0.977710	val: 0.922212	test: 0.703800

Epoch: 23
Loss: 0.181052496199337
train: 0.979461	val: 0.932049	test: 0.688850

Epoch: 24
Loss: 0.176949478104824
train: 0.975429	val: 0.919101	test: 0.697145

Epoch: 25
Loss: 0.1570775273772265
train: 0.978479	val: 0.915287	test: 0.699267

Epoch: 26
Loss: 0.170580098379954
train: 0.982199	val: 0.916391	test: 0.705729

Epoch: 27
Loss: 0.17028627484782494
train: 0.984911	val: 0.913580	test: 0.703511

Epoch: 28
Loss: 0.14489993475988797
train: 0.981987	val: 0.914383	test: 0.701100

Epoch: 29
Loss: 0.16438068534500072
train: 0.984046	val: 0.917796	test: 0.708816

Epoch: 30
Loss: 0.14230064937829787
train: 0.984952	val: 0.920606	test: 0.706404

Epoch: 31
Loss: 0.13904119999296832
train: 0.984388	val: 0.912476	test: 0.700521

Epoch: 32
Loss: 0.1393333658246735
train: 0.987910	val: 0.894710	test: 0.699074

Epoch: 33
Loss: 0.1448105403883215
train: 0.985683	val: 0.906755	test: 0.715278

Epoch: 34
Loss: 0.1462559705341087
train: 0.987675	val: 0.903242	test: 0.720679

Epoch: 35
Loss: 0.15234135723381323
train: 0.987276	val: 0.900331	test: 0.707176

Epoch: 36
Loss: 0.14137755845982963
train: 0.990034	val: 0.907658	test: 0.708430

Epoch: 37
Loss: 0.1323326455251196
train: 0.989645	val: 0.906655	test: 0.716435

Epoch: 38
Loss: 0.1475534399060782
train: 0.990984	val: 0.910067	test: 0.704379

Epoch: 39
Loss: 0.12065024080617244
train: 0.991751	val: 0.901937	test: 0.695505

Epoch: 40
Loss: 0.12668631107346406
train: 0.990135	val: 0.904647	test: 0.710455

Epoch: 41
Loss: 0.12618284182315234
train: 0.991438	val: 0.908963	test: 0.718750

Epoch: 42
Loss: 0.15079157451565872
train: 0.991354	val: 0.902138	test: 0.714699

Epoch: 43
Loss: 0.12269962570433017
train: 0.992780	val: 0.892201	test: 0.709491

Epoch: 44
Loss: 0.1347668889827576
train: 0.992703	val: 0.896417	test: 0.693191

Epoch: 45
Loss: 0.12551245341826425
train: 0.992218	val: 0.905450	test: 0.692515

Epoch: 46
Loss: 0.12618617904574883
train: 0.992943	val: 0.905049	test: 0.691551

Epoch: 47
Loss: 0.12314500117684747
train: 0.993015	val: 0.902339	test: 0.689525

Epoch: 48
Loss: 0.1089172220305379
train: 0.993880	val: 0.894911	test: 0.700810

Epoch: 49
Loss: 0.13486201882972318
train: 0.992717	val: 0.910870	test: 0.722608

Epoch: 50
Loss: 0.12402905245614242
train: 0.992636	val: 0.897922	test: 0.714506

Epoch: 51
Loss: 0.13018233482587158
train: 0.994639	val: 0.905049	test: 0.717689

Epoch: 52
Loss: 0.11762546339516475
train: 0.994443	val: 0.898826	test: 0.712770

Epoch: 53
Loss: 0.11244141359370732
train: 0.994687	val: 0.903242	test: 0.712384

Epoch: 54
Loss: 0.11238964629117912
train: 0.994757	val: 0.905651	test: 0.699171

Epoch: 55
Loss: 0.10925816782419824
train: 0.995077	val: 0.908863	test: 0.716435

Epoch: 56
Loss: 0.11475919105670802
train: 0.993061	val: 0.893205	test: 0.720583

Epoch: 57
Loss: 0.13129068496271584
train: 0.995070	val: 0.902640	test: 0.705922

Epoch: 58
Loss: 0.1258914407062765
train: 0.993570	val: 0.905149	test: 0.704765

Epoch: 59
Loss: 0.10917756510756531
train: 0.994354	val: 0.903643	test: 0.695312

Epoch: 60
Loss: 0.10870893300504635
train: 0.995301	val: 0.902339	test: 0.704572

Epoch: 61
Loss: 0.11209455527410488
train: 0.995955	val: 0.892502	test: 0.711902

Epoch: 62
Loss: 0.10025228332772576
train: 0.996405	val: 0.888989	test: 0.693769

Epoch: 63
Loss: 0.0989422577552774
train: 0.996973	val: 0.893707	test: 0.691165

Epoch: 64
Loss: 0.10948190636847892
train: 0.996180	val: 0.883569	test: 0.699074

Epoch: 65
Loss: 0.09930065307147083
train: 0.995629	val: 0.886681	test: 0.698592

Epoch: 66
Loss: 0.1019006105594678
train: 0.996373	val: 0.886982	test: 0.689043

Epoch: 67
Loss: 0.10683592846206007
train: 0.996033	val: 0.894008	test: 0.681134

Epoch: 68
Loss: 0.11496181258381712
train: 0.997214	val: 0.908562	test: 0.694444

Epoch: 69
Loss: 0.10221065491016776
train: 0.997203	val: 0.901335	test: 0.703318

Epoch: 70
Loss: 0.09374496945333308
train: 0.996924	val: 0.889993	test: 0.677662

Epoch: 71
Loss: 0.10205379271449441
train: 0.997200	val: 0.901636	test: 0.665895

Epoch: 72
Loss: 0.09462652820811492
train: 0.997705	val: 0.902339	test: 0.678434

Epoch: 73
Loss: 0.08434044240392892
train: 0.997527	val: 0.892000	test: 0.695312

Epoch: 74
Loss: 0.0983551747366901
train: 0.997318	val: 0.898023	test: 0.695698

Epoch: 75
Loss: 0.08575877428192873
train: 0.997063	val: 0.893907	test: 0.694444

Epoch: 76
Loss: 0.09691278082070577
train: 0.997326	val: 0.894209	test: 0.683160

Epoch: 77
Loss: 0.11611576650475457
train: 0.996707	val: 0.895614	test: 0.678916

Epoch: 78
Loss: 0.09493166325920321
train: 0.997552	val: 0.890093	test: 0.703125

Epoch: 79
Loss: 0.09322350042369663
train: 0.998350	val: 0.895112	test: 0.695698

Epoch: 80
Loss: 0.09751397336346332
train: 0.998381	val: 0.897722	test: 0.684992

Epoch: 81
Loss: 0.09957703726518678
train: 0.997507	val: 0.885376	test: 0.685764

Epoch: 82
Loss: 0.07947261100627324
train: 0.998012	val: 0.890997	test: 0.684317

Epoch: 83
Loss: 0.08708699822796047
train: 0.998788	val: 0.894710	test: 0.685860

Epoch: 84
Loss: 0.09411373232869165
train: 0.998674	val: 0.887283	test: 0.687404

Epoch: 85
Loss: 0.09118406005102024
train: 0.998467	val: 0.887584	test: 0.692226

Epoch: 86
Loss: 0.09008601949598358
train: 0.998550	val: 0.902038	test: 0.707755

Epoch: 87
Loss: 0.08581069330691495
train: 0.998613	val: 0.894008	test: 0.702353

Epoch: 88
Loss: 0.09262182066770422
train: 0.998860	val: 0.883770	test: 0.677758

Epoch: 89
Loss: 0.07827995605352116
train: 0.998781	val: 0.881863	test: 0.664834

Epoch: 90
Loss: 0.09147035129159964
train: 0.998909	val: 0.896718	test: 0.690490

Epoch: 91
Loss: 0.07890598791490926
train: 0.998457	val: 0.907458	test: 0.692419

Epoch: 92
Loss: 0.08582707289821571
train: 0.997798	val: 0.900733	test: 0.671971

Epoch: 93
Loss: 0.06789724010736335
train: 0.998829	val: 0.894209	test: 0.675733

Epoch: 94
Loss: 0.0942692412489609
train: 0.998931	val: 0.896517	test: 0.685667

Epoch: 95
Loss: 0.08418417685853176
train: 0.998694	val: 0.897119	test: 0.669753

Epoch: 96
Loss: 0.09283859924546024
train: 0.998955	val: 0.899026	test: 0.688561

Epoch: 97
Loss: 0.07642715298884231
train: 0.998662	val: 0.881060	test: 0.694059

Epoch: 98
Loss: 0.0723156726598694
train: 0.998942	val: 0.887283	test: 0.685860

Epoch: 99
Loss: 0.08488085286027794
train: 0.999285	val: 0.898625	test: 0.686921

Epoch: 100
Loss: 0.08440393677579434
train: 0.997610	val: 0.884874	test: 0.694059

best train: 0.979461	val: 0.932049	test: 0.688850
end
