Clear
9112533_2
--dataset=hiv --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2577285165510704
train: 0.770411	val: 0.743199	test: 0.751303

Epoch: 2
Loss: 0.13648847664205438
train: 0.786499	val: 0.762064	test: 0.729730

Epoch: 3
Loss: 0.1329446110255717
train: 0.811312	val: 0.773617	test: 0.744234

Epoch: 4
Loss: 0.12831991652571031
train: 0.821055	val: 0.774186	test: 0.763217

Epoch: 5
Loss: 0.12489963263004215
train: 0.835734	val: 0.803470	test: 0.751766

Epoch: 6
Loss: 0.12443171341692037
train: 0.832712	val: 0.793865	test: 0.753286

Epoch: 7
Loss: 0.1224204292230694
train: 0.838326	val: 0.776219	test: 0.733456

Epoch: 8
Loss: 0.12026450191289116
train: 0.851075	val: 0.769612	test: 0.744665

Epoch: 9
Loss: 0.11782230483435627
train: 0.845676	val: 0.763335	test: 0.741441

Epoch: 10
Loss: 0.1169115019414625
train: 0.863524	val: 0.780638	test: 0.765725

Epoch: 11
Loss: 0.1163602489675583
train: 0.858663	val: 0.811894	test: 0.764793

Epoch: 12
Loss: 0.1133092876975877
train: 0.873346	val: 0.779808	test: 0.765082

Epoch: 13
Loss: 0.11242769796188773
train: 0.871604	val: 0.784679	test: 0.751096

Epoch: 14
Loss: 0.1137617596923618
train: 0.876086	val: 0.787637	test: 0.767580

Epoch: 15
Loss: 0.11144601350241971
train: 0.877645	val: 0.800038	test: 0.760109

Epoch: 16
Loss: 0.10995579289094441
train: 0.881050	val: 0.795194	test: 0.767639

Epoch: 17
Loss: 0.10955878317205153
train: 0.890756	val: 0.800222	test: 0.770081

Epoch: 18
Loss: 0.10799761534874817
train: 0.887630	val: 0.783200	test: 0.757033

Epoch: 19
Loss: 0.1088020148023719
train: 0.895336	val: 0.803847	test: 0.744574

Epoch: 20
Loss: 0.10793158741817219
train: 0.895037	val: 0.811199	test: 0.776954

Epoch: 21
Loss: 0.10600097244884826
train: 0.883520	val: 0.772083	test: 0.753234

Epoch: 22
Loss: 0.10720903997346633
train: 0.905103	val: 0.797515	test: 0.761668

Epoch: 23
Loss: 0.10539004947029579
train: 0.908162	val: 0.812935	test: 0.744831

Epoch: 24
Loss: 0.10422426015816863
train: 0.910391	val: 0.801939	test: 0.759949

Epoch: 25
Loss: 0.10345112604907414
train: 0.906493	val: 0.797471	test: 0.758373

Epoch: 26
Loss: 0.1030675541169326
train: 0.909280	val: 0.801893	test: 0.756156

Epoch: 27
Loss: 0.10194537976666367
train: 0.913534	val: 0.786605	test: 0.742183

Epoch: 28
Loss: 0.10208077074017291
train: 0.917217	val: 0.801250	test: 0.771114

Epoch: 29
Loss: 0.1014528668315488
train: 0.924092	val: 0.790693	test: 0.780199

Epoch: 30
Loss: 0.10130279515966585
train: 0.922173	val: 0.796707	test: 0.759439

Epoch: 31
Loss: 0.1005591804920417
train: 0.916940	val: 0.802582	test: 0.776180

Epoch: 32
Loss: 0.10020975444822822
train: 0.919643	val: 0.794156	test: 0.763827

Epoch: 33
Loss: 0.09960047519859724
train: 0.926624	val: 0.832164	test: 0.768563

Epoch: 34
Loss: 0.1004026466992178
train: 0.921873	val: 0.782015	test: 0.747361

Epoch: 35
Loss: 0.09835796084156441
train: 0.927187	val: 0.787150	test: 0.749408

Epoch: 36
Loss: 0.09710415965058085
train: 0.919864	val: 0.798660	test: 0.747915

Epoch: 37
Loss: 0.0980751074382406
train: 0.924525	val: 0.786464	test: 0.744545

Epoch: 38
Loss: 0.09716163196532746
train: 0.931219	val: 0.795910	test: 0.747826

Epoch: 39
Loss: 0.09555859568879836
train: 0.934942	val: 0.795522	test: 0.765789

Epoch: 40
Loss: 0.09635783800887843
train: 0.932045	val: 0.801704	test: 0.757280

Epoch: 41
Loss: 0.09403365852652776
train: 0.930257	val: 0.790316	test: 0.754528

Epoch: 42
Loss: 0.0952450330886792
train: 0.932704	val: 0.787300	test: 0.757195

Epoch: 43
Loss: 0.09327651722249591
train: 0.938244	val: 0.818165	test: 0.760810

Epoch: 44
Loss: 0.09451459654126193
train: 0.933173	val: 0.781036	test: 0.757523

Epoch: 45
Loss: 0.09332701132007182
train: 0.940167	val: 0.774281	test: 0.754219

Epoch: 46
Loss: 0.09293363306528388
train: 0.934894	val: 0.797742	test: 0.768321

Epoch: 47
Loss: 0.09192632780659926
train: 0.938892	val: 0.788804	test: 0.732233

Epoch: 48
Loss: 0.09195074444616957
train: 0.944624	val: 0.784290	test: 0.764704

Epoch: 49
Loss: 0.09206363389665916
train: 0.940218	val: 0.798045	test: 0.755544

Epoch: 50
Loss: 0.0916663291296703
train: 0.949829	val: 0.802399	test: 0.775751

Epoch: 51
Loss: 0.09177442139720106
train: 0.950683	val: 0.817081	test: 0.779828

Epoch: 52
Loss: 0.08990382246935853
train: 0.948549	val: 0.793917	test: 0.760019

Epoch: 53
Loss: 0.09038091374010235
train: 0.949137	val: 0.801097	test: 0.763785

Epoch: 54
Loss: 0.08989504712046148
train: 0.944978	val: 0.766443	test: 0.750387

Epoch: 55
Loss: 0.08920238060538932
train: 0.952555	val: 0.783347	test: 0.736702

Epoch: 56
Loss: 0.08916198852658064
train: 0.948300	val: 0.800243	test: 0.773677

Epoch: 57
Loss: 0.0877594850888796
train: 0.951707	val: 0.780723	test: 0.747666

Epoch: 58
Loss: 0.087441784165801
train: 0.950361	val: 0.790926	test: 0.755926

Epoch: 59
Loss: 0.08978896260484957
train: 0.956203	val: 0.796345	test: 0.761002

Epoch: 60
Loss: 0.08749804236195247
train: 0.953699	val: 0.805552	test: 0.781384

Epoch: 61
Loss: 0.08787180860945945
train: 0.955023	val: 0.792172	test: 0.765330

Epoch: 62
Loss: 0.08640973417167266
train: 0.958895	val: 0.803767	test: 0.766013

Epoch: 63
Loss: 0.08581080972213437
train: 0.959129	val: 0.796740	test: 0.757311

Epoch: 64
Loss: 0.08779044466611219
train: 0.958476	val: 0.801547	test: 0.751189

Epoch: 65
Loss: 0.0845613641469976
train: 0.956881	val: 0.794753	test: 0.754777

Epoch: 66
Loss: 0.08589957292986886
train: 0.961734	val: 0.796808	test: 0.735777

Epoch: 67
Loss: 0.08375446892537418
train: 0.961240	val: 0.809564	test: 0.756583

Epoch: 68
Loss: 0.08513520707696376
train: 0.960065	val: 0.821416	test: 0.773632

Epoch: 69
Loss: 0.0826621048885133
train: 0.965632	val: 0.778742	test: 0.731719

Epoch: 70
Loss: 0.08344753557239067
train: 0.965299	val: 0.788210	test: 0.762595

Epoch: 71
Loss: 0.08370946563280873
train: 0.964136	val: 0.788485	test: 0.757751

Epoch: 72
Loss: 0.08308340643458033
train: 0.964222	val: 0.795564	test: 0.757664

Epoch: 73
Loss: 0.0812257780609111
train: 0.967896	val: 0.799462	test: 0.761270

Epoch: 74
Loss: 0.08194101700890992
train: 0.968614	val: 0.812371	test: 0.762790

Epoch: 75
Loss: 0.08140049168114903
train: 0.969155	val: 0.799138	test: 0.766479

Epoch: 76
Loss: 0.08150900131365202
train: 0.970749	val: 0.810396	test: 0.769435

Epoch: 77
Loss: 0.08117479787898144
train: 0.971378	val: 0.792129	test: 0.769441

Epoch: 78
Loss: 0.08031164184729676
train: 0.969006	val: 0.799695	test: 0.759574

Epoch: 79
Loss: 0.07918212773106521
train: 0.971746	val: 0.794037	test: 0.752056

Epoch: 80
Loss: 0.07831216698284693
train: 0.971990	val: 0.796431	test: 0.753157

Epoch: 81
Loss: 0.08032120355827964
train: 0.969207	val: 0.781593	test: 0.745911

Epoch: 82
Loss: 0.07861410394680308
train: 0.970948	val: 0.784153	test: 0.755268

Epoch: 83
Loss: 0.07991087800170793
train: 0.971505	val: 0.812108	test: 0.748064

Epoch: 84
Loss: 0.07798385879906
train: 0.973141	val: 0.801336	test: 0.759835

Epoch: 85
Loss: 0.07660312128913949
train: 0.975059	val: 0.802714	test: 0.752635

Epoch: 86
Loss: 0.07818952669359962
train: 0.976259	val: 0.806030	test: 0.756129

Epoch: 87
Loss: 0.07557219959834298
train: 0.977184	val: 0.777818	test: 0.756117

Epoch: 88
Loss: 0.07536716693257237
train: 0.971595	val: 0.799989	test: 0.773507

Epoch: 89
Loss: 0.07673763430560705
train: 0.976670	val: 0.806263	test: 0.744568

Epoch: 90
Loss: 0.07561193427617885
train: 0.974539	val: 0.814845	test: 0.764686

Epoch: 91
Loss: 0.07546898869600946
train: 0.975979	val: 0.789291	test: 0.749733

Epoch: 92
Loss: 0.07738312049064834
train: 0.977511	val: 0.807175	test: 0.778113

Epoch: 93
Loss: 0.07451290301920342
train: 0.980062	val: 0.798311	test: 0.763572

Epoch: 94
Loss: 0.07467159859146524
train: 0.978257	val: 0.825342	test: 0.760092

Epoch: 95
Loss: 0.07473784885455174
train: 0.977944	val: 0.789067	test: 0.746164

Epoch: 96
Loss: 0.07359548561346944
train: 0.978791	val: 0.791863	test: 0.782429

Epoch: 97
Loss: 0.07439063139981303
train: 0.979201	val: 0.792353	test: 0.768661

Epoch: 98
Loss: 0.07446795721449376
train: 0.979136	val: 0.803029	test: 0.769712

Epoch: 99
Loss: 0.0726972047867142
train: 0.974731	val: 0.797356	test: 0.756784

Epoch: 100
Loss: 0.07222719474991479
train: 0.979826	val: 0.812276	test: 0.760107

best train: 0.926624	val: 0.832164	test: 0.768563
end
