9112533_2
--dataset=bace --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6636276274051145
train: 0.763699	val: 0.650916	test: 0.707703

Epoch: 2
Loss: 0.609103096929366
train: 0.815317	val: 0.635897	test: 0.736046

Epoch: 3
Loss: 0.5468838913984884
train: 0.847842	val: 0.670330	test: 0.770996

Epoch: 4
Loss: 0.5041199734879309
train: 0.865616	val: 0.672527	test: 0.796035

Epoch: 5
Loss: 0.4838402003536908
train: 0.872763	val: 0.667033	test: 0.790297

Epoch: 6
Loss: 0.4686309654070535
train: 0.885214	val: 0.692674	test: 0.807164

Epoch: 7
Loss: 0.4576949063711214
train: 0.895154	val: 0.717582	test: 0.798470

Epoch: 8
Loss: 0.43696322188323816
train: 0.903990	val: 0.725641	test: 0.811511

Epoch: 9
Loss: 0.42409791313030787
train: 0.906630	val: 0.694872	test: 0.813424

Epoch: 10
Loss: 0.4094078436428594
train: 0.910859	val: 0.709524	test: 0.822292

Epoch: 11
Loss: 0.42357738407381956
train: 0.913059	val: 0.705128	test: 0.826465

Epoch: 12
Loss: 0.4177601907577655
train: 0.912235	val: 0.692674	test: 0.813945

Epoch: 13
Loss: 0.4064113960006
train: 0.916053	val: 0.689377	test: 0.817945

Epoch: 14
Loss: 0.404190479977051
train: 0.919795	val: 0.710623	test: 0.817423

Epoch: 15
Loss: 0.3928253914919657
train: 0.921407	val: 0.721978	test: 0.821422

Epoch: 16
Loss: 0.391402088769226
train: 0.923353	val: 0.712821	test: 0.818640

Epoch: 17
Loss: 0.39831397510929606
train: 0.924963	val: 0.717949	test: 0.819510

Epoch: 18
Loss: 0.38828131382200876
train: 0.928944	val: 0.730403	test: 0.822987

Epoch: 19
Loss: 0.38365338340829175
train: 0.931453	val: 0.716484	test: 0.824204

Epoch: 20
Loss: 0.36456737080546614
train: 0.932446	val: 0.703663	test: 0.821944

Epoch: 21
Loss: 0.3706682383987862
train: 0.931992	val: 0.714286	test: 0.833942

Epoch: 22
Loss: 0.3665602230675761
train: 0.935628	val: 0.702930	test: 0.824204

Epoch: 23
Loss: 0.38095656439311815
train: 0.936667	val: 0.703297	test: 0.823857

Epoch: 24
Loss: 0.37872705636792425
train: 0.937312	val: 0.709524	test: 0.816206

Epoch: 25
Loss: 0.36617344031478505
train: 0.938296	val: 0.699634	test: 0.808729

Epoch: 26
Loss: 0.35432187606756826
train: 0.938884	val: 0.696703	test: 0.805773

Epoch: 27
Loss: 0.3379286090642022
train: 0.940251	val: 0.708059	test: 0.808381

Epoch: 28
Loss: 0.36731607946437733
train: 0.940322	val: 0.713187	test: 0.806990

Epoch: 29
Loss: 0.3500447459864449
train: 0.940913	val: 0.697070	test: 0.790819

Epoch: 30
Loss: 0.3486871763190251
train: 0.943388	val: 0.709890	test: 0.806642

Epoch: 31
Loss: 0.3316127944874455
train: 0.944498	val: 0.717949	test: 0.805251

Epoch: 32
Loss: 0.36466831715604997
train: 0.945311	val: 0.700733	test: 0.794123

Epoch: 33
Loss: 0.34941941001108257
train: 0.945046	val: 0.708059	test: 0.810816

Epoch: 34
Loss: 0.3330775679547435
train: 0.944209	val: 0.706960	test: 0.815684

Epoch: 35
Loss: 0.34462033723532143
train: 0.949486	val: 0.676190	test: 0.797948

Epoch: 36
Loss: 0.33226322589671664
train: 0.950371	val: 0.675458	test: 0.805947

Epoch: 37
Loss: 0.32313630381155484
train: 0.948593	val: 0.695971	test: 0.813424

Epoch: 38
Loss: 0.3301062395377534
train: 0.952483	val: 0.688278	test: 0.809424

Epoch: 39
Loss: 0.3222350439202749
train: 0.953045	val: 0.669963	test: 0.798818

Epoch: 40
Loss: 0.3291071408441253
train: 0.953522	val: 0.687546	test: 0.802643

Epoch: 41
Loss: 0.2991913097576666
train: 0.949826	val: 0.705128	test: 0.798644

Epoch: 42
Loss: 0.32695640164207335
train: 0.953305	val: 0.677289	test: 0.802469

Epoch: 43
Loss: 0.3203082623007849
train: 0.957449	val: 0.668132	test: 0.808555

Epoch: 44
Loss: 0.30967042555625796
train: 0.956207	val: 0.687546	test: 0.806468

Epoch: 45
Loss: 0.3186378593906934
train: 0.957646	val: 0.687179	test: 0.802295

Epoch: 46
Loss: 0.31832269125007506
train: 0.958308	val: 0.680952	test: 0.802643

Epoch: 47
Loss: 0.3338491852485898
train: 0.957537	val: 0.681319	test: 0.804382

Epoch: 48
Loss: 0.3029735694038992
train: 0.956670	val: 0.672527	test: 0.787515

Epoch: 49
Loss: 0.3059194896985242
train: 0.959315	val: 0.674725	test: 0.796731

Epoch: 50
Loss: 0.30178596253078893
train: 0.960674	val: 0.673626	test: 0.798296

Epoch: 51
Loss: 0.3199744517300775
train: 0.963507	val: 0.659707	test: 0.798296

Epoch: 52
Loss: 0.31470208962756446
train: 0.963485	val: 0.647985	test: 0.790819

Epoch: 53
Loss: 0.2944513213375664
train: 0.962962	val: 0.656777	test: 0.790471

Epoch: 54
Loss: 0.27673326464396925
train: 0.962554	val: 0.672527	test: 0.780560

Epoch: 55
Loss: 0.30157049287661186
train: 0.964466	val: 0.670330	test: 0.785776

Epoch: 56
Loss: 0.28282515739797065
train: 0.965111	val: 0.651648	test: 0.806816

Epoch: 57
Loss: 0.28107246762734733
train: 0.965654	val: 0.683516	test: 0.806121

Epoch: 58
Loss: 0.27728319960969355
train: 0.962240	val: 0.719414	test: 0.793427

Epoch: 59
Loss: 0.2868068066529435
train: 0.966070	val: 0.691941	test: 0.787689

Epoch: 60
Loss: 0.2862450723608211
train: 0.968562	val: 0.680952	test: 0.804556

Epoch: 61
Loss: 0.28836144780374245
train: 0.967714	val: 0.685348	test: 0.796383

Epoch: 62
Loss: 0.283951462403613
train: 0.966918	val: 0.709158	test: 0.797774

Epoch: 63
Loss: 0.30623922414573557
train: 0.967723	val: 0.697070	test: 0.785255

Epoch: 64
Loss: 0.27889264619087983
train: 0.967900	val: 0.647253	test: 0.780038

Epoch: 65
Loss: 0.28276543997860293
train: 0.965848	val: 0.673993	test: 0.782820

Epoch: 66
Loss: 0.27916772503930015
train: 0.970771	val: 0.654579	test: 0.782473

Epoch: 67
Loss: 0.2775611589136447
train: 0.971550	val: 0.652747	test: 0.775517

Epoch: 68
Loss: 0.2699193484716682
train: 0.971578	val: 0.683883	test: 0.789602

Epoch: 69
Loss: 0.26647870495229103
train: 0.972446	val: 0.669597	test: 0.781429

Epoch: 70
Loss: 0.28578946023560137
train: 0.972939	val: 0.670696	test: 0.766128

Epoch: 71
Loss: 0.2675329399567369
train: 0.973830	val: 0.692308	test: 0.764910

Epoch: 72
Loss: 0.27339654515519873
train: 0.975950	val: 0.690842	test: 0.765606

Epoch: 73
Loss: 0.26419603359868493
train: 0.974372	val: 0.700733	test: 0.771170

Epoch: 74
Loss: 0.27052565606280193
train: 0.974892	val: 0.704762	test: 0.766823

Epoch: 75
Loss: 0.2722464100377815
train: 0.973850	val: 0.692674	test: 0.762476

Epoch: 76
Loss: 0.26544110858633385
train: 0.975322	val: 0.695238	test: 0.751521

Epoch: 77
Loss: 0.2542588498442819
train: 0.975982	val: 0.704029	test: 0.750304

Epoch: 78
Loss: 0.2499441258061977
train: 0.976301	val: 0.706960	test: 0.753956

Epoch: 79
Loss: 0.2576838219050498
train: 0.977369	val: 0.678388	test: 0.774300

Epoch: 80
Loss: 0.26930857608113845
train: 0.975759	val: 0.667033	test: 0.761433

Epoch: 81
Loss: 0.2536271356876848
train: 0.978556	val: 0.677656	test: 0.762998

Epoch: 82
Loss: 0.23986411362872131
train: 0.978473	val: 0.686447	test: 0.763172

Epoch: 83
Loss: 0.2454827232302787
train: 0.978188	val: 0.691209	test: 0.752391

Epoch: 84
Loss: 0.25245399440968475
train: 0.980805	val: 0.684982	test: 0.764910

Epoch: 85
Loss: 0.26927070632393935
train: 0.981079	val: 0.671795	test: 0.771170

Epoch: 86
Loss: 0.25478241297349247
train: 0.980742	val: 0.683883	test: 0.752739

Epoch: 87
Loss: 0.2453096336534811
train: 0.977925	val: 0.711722	test: 0.756042

Epoch: 88
Loss: 0.2429189277530759
train: 0.980157	val: 0.718315	test: 0.753782

Epoch: 89
Loss: 0.2456402275180849
train: 0.980285	val: 0.711722	test: 0.738306

Epoch: 90
Loss: 0.2302888915711346
train: 0.977808	val: 0.703663	test: 0.746305

Epoch: 91
Loss: 0.24337766606728536
train: 0.980976	val: 0.695604	test: 0.756564

Epoch: 92
Loss: 0.23387310155607074
train: 0.981261	val: 0.689744	test: 0.757433

Epoch: 93
Loss: 0.24689226254195237
train: 0.981187	val: 0.695604	test: 0.763346

Epoch: 94
Loss: 0.23286458036246716
train: 0.979772	val: 0.691941	test: 0.737437

Epoch: 95
Loss: 0.23158735758073937
train: 0.980965	val: 0.689377	test: 0.725091

Epoch: 96
Loss: 0.21584354885626533
train: 0.982554	val: 0.678755	test: 0.737611

Epoch: 97
Loss: 0.22687588030616754
train: 0.985696	val: 0.664103	test: 0.743349

Epoch: 98
Loss: 0.2536420829734505
train: 0.984632	val: 0.663736	test: 0.749957

Epoch: 99
Loss: 0.21740291188239222
train: 0.983807	val: 0.658974	test: 0.740741

Epoch: 100
Loss: 0.2285780196312507
train: 0.983445	val: 0.684982	test: 0.748044

best train: 0.928944	val: 0.730403	test: 0.822987
end
