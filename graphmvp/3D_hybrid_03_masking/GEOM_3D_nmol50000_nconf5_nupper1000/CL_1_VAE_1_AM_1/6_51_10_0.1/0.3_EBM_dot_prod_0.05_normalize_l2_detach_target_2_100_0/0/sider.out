9149428_0
--dataset=sider --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6757374743929354
train: 0.544477	val: 0.534526	test: 0.496707

Epoch: 2
Loss: 0.6331597641564317
train: 0.558517	val: 0.516739	test: 0.510383

Epoch: 3
Loss: 0.5981985797971066
train: 0.570945	val: 0.508805	test: 0.522405

Epoch: 4
Loss: 0.5746467056080438
train: 0.598582	val: 0.514327	test: 0.534836

Epoch: 5
Loss: 0.5519948155089385
train: 0.637648	val: 0.527447	test: 0.555617

Epoch: 6
Loss: 0.5373411113047468
train: 0.658247	val: 0.553909	test: 0.566067

Epoch: 7
Loss: 0.5242640649482571
train: 0.669179	val: 0.569573	test: 0.573039

Epoch: 8
Loss: 0.5214373557858332
train: 0.677124	val: 0.563449	test: 0.571686

Epoch: 9
Loss: 0.5117034928656761
train: 0.689508	val: 0.564251	test: 0.582791

Epoch: 10
Loss: 0.5039106395270282
train: 0.701689	val: 0.570601	test: 0.584199

Epoch: 11
Loss: 0.4914278321507801
train: 0.708387	val: 0.567075	test: 0.587119

Epoch: 12
Loss: 0.4909696363462621
train: 0.716721	val: 0.572351	test: 0.589698

Epoch: 13
Loss: 0.4851509357891799
train: 0.724807	val: 0.587354	test: 0.595600

Epoch: 14
Loss: 0.48124348810801215
train: 0.729021	val: 0.596305	test: 0.601294

Epoch: 15
Loss: 0.4783490620529072
train: 0.738530	val: 0.588671	test: 0.602262

Epoch: 16
Loss: 0.4779985180328931
train: 0.745802	val: 0.593241	test: 0.605505

Epoch: 17
Loss: 0.4675335809766633
train: 0.750362	val: 0.606131	test: 0.610997

Epoch: 18
Loss: 0.4655224189825661
train: 0.756458	val: 0.614095	test: 0.611355

Epoch: 19
Loss: 0.467554630824234
train: 0.756622	val: 0.610698	test: 0.599344

Epoch: 20
Loss: 0.46530268541830566
train: 0.760907	val: 0.608198	test: 0.611065

Epoch: 21
Loss: 0.4633000488893567
train: 0.767635	val: 0.612647	test: 0.614605

Epoch: 22
Loss: 0.4586254828552909
train: 0.772474	val: 0.618397	test: 0.615914

Epoch: 23
Loss: 0.4562885540053766
train: 0.778749	val: 0.626678	test: 0.622133

Epoch: 24
Loss: 0.45671914089100174
train: 0.778783	val: 0.627208	test: 0.620798

Epoch: 25
Loss: 0.4533480074322255
train: 0.781099	val: 0.624342	test: 0.618723

Epoch: 26
Loss: 0.4511783488024287
train: 0.783362	val: 0.630971	test: 0.616777

Epoch: 27
Loss: 0.45330785705075255
train: 0.790110	val: 0.636249	test: 0.609900

Epoch: 28
Loss: 0.4474730579609031
train: 0.790879	val: 0.619540	test: 0.608840

Epoch: 29
Loss: 0.44509648078836117
train: 0.794915	val: 0.621013	test: 0.609482

Epoch: 30
Loss: 0.4479282526526237
train: 0.799650	val: 0.634043	test: 0.606145

Epoch: 31
Loss: 0.4376891161169613
train: 0.798266	val: 0.627658	test: 0.614269

Epoch: 32
Loss: 0.44131768558628803
train: 0.801835	val: 0.632025	test: 0.610765

Epoch: 33
Loss: 0.44004016356609366
train: 0.808328	val: 0.622816	test: 0.602441

Epoch: 34
Loss: 0.4416279363850597
train: 0.807836	val: 0.623258	test: 0.593521

Epoch: 35
Loss: 0.44067326286071024
train: 0.814335	val: 0.615945	test: 0.607676

Epoch: 36
Loss: 0.43467206405100256
train: 0.812764	val: 0.625483	test: 0.598463

Epoch: 37
Loss: 0.4350155536230023
train: 0.813900	val: 0.634519	test: 0.594034

Epoch: 38
Loss: 0.4336062461230707
train: 0.819627	val: 0.632300	test: 0.608397

Epoch: 39
Loss: 0.4314167923336023
train: 0.820857	val: 0.622160	test: 0.603693

Epoch: 40
Loss: 0.4303800797517586
train: 0.827624	val: 0.626031	test: 0.599803

Epoch: 41
Loss: 0.4268389149958745
train: 0.829310	val: 0.617786	test: 0.594109

Epoch: 42
Loss: 0.42934699786789193
train: 0.831002	val: 0.624120	test: 0.588763

Epoch: 43
Loss: 0.42403093049599044
train: 0.832543	val: 0.617339	test: 0.596199

Epoch: 44
Loss: 0.42528377676721946
train: 0.831181	val: 0.622857	test: 0.604001

Epoch: 45
Loss: 0.4258441414683224
train: 0.835736	val: 0.633868	test: 0.607402

Epoch: 46
Loss: 0.42378640809928736
train: 0.838417	val: 0.635112	test: 0.609011

Epoch: 47
Loss: 0.41926052578182543
train: 0.842023	val: 0.639073	test: 0.605630

Epoch: 48
Loss: 0.4177771679836358
train: 0.840677	val: 0.632446	test: 0.607212

Epoch: 49
Loss: 0.41963514541080943
train: 0.841315	val: 0.628876	test: 0.603694

Epoch: 50
Loss: 0.4114896747791671
train: 0.846846	val: 0.628949	test: 0.598970

Epoch: 51
Loss: 0.4198145784781526
train: 0.849205	val: 0.623555	test: 0.587549

Epoch: 52
Loss: 0.4137222034148499
train: 0.848976	val: 0.619558	test: 0.596783

Epoch: 53
Loss: 0.4046943718350871
train: 0.847886	val: 0.619685	test: 0.597332

Epoch: 54
Loss: 0.41368960313852554
train: 0.850908	val: 0.622910	test: 0.597298

Epoch: 55
Loss: 0.40760594956370594
train: 0.854484	val: 0.632325	test: 0.597301

Epoch: 56
Loss: 0.4051218895859211
train: 0.857710	val: 0.624545	test: 0.603514

Epoch: 57
Loss: 0.4038775821896302
train: 0.859895	val: 0.633140	test: 0.602841

Epoch: 58
Loss: 0.3999116049240792
train: 0.861972	val: 0.639650	test: 0.591966

Epoch: 59
Loss: 0.3974156141290413
train: 0.863588	val: 0.640171	test: 0.598145

Epoch: 60
Loss: 0.39822772502628717
train: 0.862943	val: 0.630700	test: 0.602359

Epoch: 61
Loss: 0.40001354457617583
train: 0.862528	val: 0.634329	test: 0.578155

Epoch: 62
Loss: 0.3968202473675855
train: 0.862400	val: 0.623089	test: 0.586555

Epoch: 63
Loss: 0.3931803705565398
train: 0.869241	val: 0.631518	test: 0.591871

Epoch: 64
Loss: 0.39344521122164855
train: 0.869742	val: 0.636864	test: 0.599018

Epoch: 65
Loss: 0.39087268246641227
train: 0.867011	val: 0.628820	test: 0.607902

Epoch: 66
Loss: 0.3996987645764702
train: 0.868622	val: 0.626954	test: 0.606303

Epoch: 67
Loss: 0.391769147493659
train: 0.871317	val: 0.627554	test: 0.602234

Epoch: 68
Loss: 0.3915391160450883
train: 0.873376	val: 0.621848	test: 0.590646

Epoch: 69
Loss: 0.39705845975033077
train: 0.875350	val: 0.613326	test: 0.571582

Epoch: 70
Loss: 0.3935703951614784
train: 0.876086	val: 0.606683	test: 0.586018

Epoch: 71
Loss: 0.3909452415651347
train: 0.880465	val: 0.626189	test: 0.598300

Epoch: 72
Loss: 0.3880985321314005
train: 0.880964	val: 0.628209	test: 0.591077

Epoch: 73
Loss: 0.3894469429372397
train: 0.883284	val: 0.623784	test: 0.596978

Epoch: 74
Loss: 0.3829152704066755
train: 0.883004	val: 0.639013	test: 0.603511

Epoch: 75
Loss: 0.380297226089605
train: 0.885554	val: 0.630973	test: 0.606042

Epoch: 76
Loss: 0.37747174306618125
train: 0.884436	val: 0.618331	test: 0.605854

Epoch: 77
Loss: 0.378476879594232
train: 0.887115	val: 0.615680	test: 0.597089

Epoch: 78
Loss: 0.3820214487104582
train: 0.888438	val: 0.623040	test: 0.589106

Epoch: 79
Loss: 0.3732963133358887
train: 0.891365	val: 0.629432	test: 0.589441

Epoch: 80
Loss: 0.37170426771828347
train: 0.891822	val: 0.632460	test: 0.595970

Epoch: 81
Loss: 0.3742602464535464
train: 0.890748	val: 0.621995	test: 0.593902

Epoch: 82
Loss: 0.3710038935555744
train: 0.892776	val: 0.621101	test: 0.594632

Epoch: 83
Loss: 0.3720395626905383
train: 0.893725	val: 0.621943	test: 0.598723

Epoch: 84
Loss: 0.3630985428392255
train: 0.896241	val: 0.621879	test: 0.589485

Epoch: 85
Loss: 0.36906299453251423
train: 0.896393	val: 0.613019	test: 0.592371

Epoch: 86
Loss: 0.37252722506890285
train: 0.896123	val: 0.619440	test: 0.598758

Epoch: 87
Loss: 0.36570967277961114
train: 0.898035	val: 0.625013	test: 0.609599

Epoch: 88
Loss: 0.37319499073628626
train: 0.900771	val: 0.635245	test: 0.602727

Epoch: 89
Loss: 0.37326525917377784
train: 0.901358	val: 0.629443	test: 0.599378

Epoch: 90
Loss: 0.36229032798519734
train: 0.896997	val: 0.624092	test: 0.607493

Epoch: 91
Loss: 0.3627978465859736
train: 0.900198	val: 0.625044	test: 0.605149

Epoch: 92
Loss: 0.36190849011652454
train: 0.901499	val: 0.627229	test: 0.597545

Epoch: 93
Loss: 0.36211462890581964
train: 0.902677	val: 0.632387	test: 0.603470

Epoch: 94
Loss: 0.35917958124347066
train: 0.905870	val: 0.629949	test: 0.596700

Epoch: 95
Loss: 0.36236661458223907
train: 0.903459	val: 0.628762	test: 0.597918

Epoch: 96
Loss: 0.36135273981673965
train: 0.899976	val: 0.626642	test: 0.592591

Epoch: 97
Loss: 0.3579946993467966
train: 0.906527	val: 0.628087	test: 0.612967

Epoch: 98
Loss: 0.35348405674645217
train: 0.904246	val: 0.634677	test: 0.590491

Epoch: 99
Loss: 0.3534112023496389
train: 0.911026	val: 0.628201	test: 0.586162

Epoch: 100
Loss: 0.35053976934042697
train: 0.910842	val: 0.617331	test: 0.591077

best train: 0.863588	val: 0.640171	test: 0.598145
end
