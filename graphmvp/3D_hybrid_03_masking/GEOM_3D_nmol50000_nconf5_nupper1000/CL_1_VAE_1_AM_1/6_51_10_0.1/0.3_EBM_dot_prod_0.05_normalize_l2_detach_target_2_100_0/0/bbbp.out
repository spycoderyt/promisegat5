9149428_0
--dataset=bbbp --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6062031936463969
train: 0.837533	val: 0.906855	test: 0.626543

Epoch: 2
Loss: 0.47717344554131164
train: 0.878041	val: 0.910569	test: 0.646508

Epoch: 3
Loss: 0.3963879276362528
train: 0.898861	val: 0.901335	test: 0.656829

Epoch: 4
Loss: 0.3279671674404984
train: 0.926413	val: 0.898826	test: 0.683835

Epoch: 5
Loss: 0.2895934551111724
train: 0.931520	val: 0.916290	test: 0.675444

Epoch: 6
Loss: 0.27428259461055265
train: 0.938237	val: 0.915688	test: 0.682677

Epoch: 7
Loss: 0.24815182829820248
train: 0.943205	val: 0.915989	test: 0.689718

Epoch: 8
Loss: 0.24851418527794822
train: 0.954044	val: 0.906655	test: 0.705536

Epoch: 9
Loss: 0.2228016525441665
train: 0.957751	val: 0.911071	test: 0.700135

Epoch: 10
Loss: 0.2293265569859981
train: 0.960711	val: 0.905249	test: 0.695120

Epoch: 11
Loss: 0.2282666371428901
train: 0.962980	val: 0.910469	test: 0.685860

Epoch: 12
Loss: 0.20700754826572118
train: 0.966494	val: 0.923417	test: 0.697724

Epoch: 13
Loss: 0.19975325870048724
train: 0.967080	val: 0.920305	test: 0.701678

Epoch: 14
Loss: 0.20495388900177905
train: 0.967441	val: 0.919803	test: 0.707272

Epoch: 15
Loss: 0.19139878754925038
train: 0.967493	val: 0.928034	test: 0.700231

Epoch: 16
Loss: 0.19506031106822896
train: 0.970517	val: 0.907156	test: 0.726852

Epoch: 17
Loss: 0.17317075265425644
train: 0.975060	val: 0.920807	test: 0.712481

Epoch: 18
Loss: 0.18039118091495585
train: 0.974980	val: 0.927331	test: 0.700424

Epoch: 19
Loss: 0.19054414580446122
train: 0.976728	val: 0.909766	test: 0.715760

Epoch: 20
Loss: 0.1810172869193992
train: 0.977511	val: 0.900933	test: 0.717496

Epoch: 21
Loss: 0.17941949245838013
train: 0.978640	val: 0.920406	test: 0.699556

Epoch: 22
Loss: 0.1750582486242101
train: 0.980299	val: 0.914182	test: 0.720197

Epoch: 23
Loss: 0.1865271195912898
train: 0.978803	val: 0.905049	test: 0.722897

Epoch: 24
Loss: 0.15715185725077876
train: 0.974123	val: 0.926629	test: 0.705536

Epoch: 25
Loss: 0.16852338816388798
train: 0.981145	val: 0.916290	test: 0.700521

Epoch: 26
Loss: 0.16052293831457773
train: 0.983168	val: 0.911673	test: 0.711806

Epoch: 27
Loss: 0.16152551673816593
train: 0.985160	val: 0.922814	test: 0.706019

Epoch: 28
Loss: 0.16329157355960183
train: 0.983662	val: 0.926629	test: 0.716725

Epoch: 29
Loss: 0.16244063795792402
train: 0.985712	val: 0.915889	test: 0.712963

Epoch: 30
Loss: 0.15386447289372773
train: 0.986111	val: 0.906253	test: 0.730035

Epoch: 31
Loss: 0.15095904477638933
train: 0.985883	val: 0.904446	test: 0.720004

Epoch: 32
Loss: 0.14537794655568487
train: 0.986691	val: 0.910870	test: 0.699460

Epoch: 33
Loss: 0.1626713171120968
train: 0.987158	val: 0.920707	test: 0.706019

Epoch: 34
Loss: 0.15727387586529326
train: 0.988473	val: 0.902740	test: 0.720872

Epoch: 35
Loss: 0.15001672861973894
train: 0.990240	val: 0.914383	test: 0.718075

Epoch: 36
Loss: 0.152731706613039
train: 0.989478	val: 0.900030	test: 0.722608

Epoch: 37
Loss: 0.13971424942746155
train: 0.990323	val: 0.906052	test: 0.708044

Epoch: 38
Loss: 0.1537970067662042
train: 0.990334	val: 0.914785	test: 0.706404

Epoch: 39
Loss: 0.13278596588916428
train: 0.991108	val: 0.909365	test: 0.727527

Epoch: 40
Loss: 0.12761113070788394
train: 0.992583	val: 0.913179	test: 0.710262

Epoch: 41
Loss: 0.1411112691856561
train: 0.992468	val: 0.896015	test: 0.713156

Epoch: 42
Loss: 0.1267326150987987
train: 0.992421	val: 0.902941	test: 0.703125

Epoch: 43
Loss: 0.12594007983165897
train: 0.993462	val: 0.902439	test: 0.711034

Epoch: 44
Loss: 0.1062886197676438
train: 0.993257	val: 0.909565	test: 0.706404

Epoch: 45
Loss: 0.12093301812186129
train: 0.994135	val: 0.900733	test: 0.706501

Epoch: 46
Loss: 0.12000784093291712
train: 0.994419	val: 0.896617	test: 0.699363

Epoch: 47
Loss: 0.11068577153737787
train: 0.993572	val: 0.911573	test: 0.706887

Epoch: 48
Loss: 0.13493628817756215
train: 0.993990	val: 0.899629	test: 0.715856

Epoch: 49
Loss: 0.12333350864192864
train: 0.994558	val: 0.895212	test: 0.714699

Epoch: 50
Loss: 0.12377309062574159
train: 0.995140	val: 0.903844	test: 0.691165

Epoch: 51
Loss: 0.11048674404988489
train: 0.995443	val: 0.906052	test: 0.686150

Epoch: 52
Loss: 0.11676266537795794
train: 0.994374	val: 0.894008	test: 0.723380

Epoch: 53
Loss: 0.11002199889797963
train: 0.995883	val: 0.901134	test: 0.715760

Epoch: 54
Loss: 0.119373376983596
train: 0.993350	val: 0.895413	test: 0.698206

Epoch: 55
Loss: 0.11545479872226848
train: 0.995788	val: 0.903543	test: 0.697917

Epoch: 56
Loss: 0.10465284919129021
train: 0.995335	val: 0.914985	test: 0.707948

Epoch: 57
Loss: 0.10973853721920215
train: 0.996607	val: 0.899127	test: 0.708912

Epoch: 58
Loss: 0.12038228989433448
train: 0.996597	val: 0.896718	test: 0.708237

Epoch: 59
Loss: 0.12987750051411037
train: 0.996604	val: 0.898725	test: 0.710648

Epoch: 60
Loss: 0.1212595113287083
train: 0.990427	val: 0.869618	test: 0.734664

Epoch: 61
Loss: 0.11415498548206146
train: 0.996016	val: 0.892101	test: 0.725019

Epoch: 62
Loss: 0.10972246645671015
train: 0.995841	val: 0.914182	test: 0.704572

Epoch: 63
Loss: 0.10406341404556359
train: 0.995046	val: 0.893606	test: 0.690876

Epoch: 64
Loss: 0.11326536692957326
train: 0.997016	val: 0.895915	test: 0.690104

Epoch: 65
Loss: 0.10051912799597797
train: 0.997534	val: 0.901837	test: 0.705826

Epoch: 66
Loss: 0.10390880240833192
train: 0.997094	val: 0.898725	test: 0.710841

Epoch: 67
Loss: 0.09765881189367699
train: 0.998007	val: 0.892201	test: 0.703800

Epoch: 68
Loss: 0.09979934701367645
train: 0.998397	val: 0.900231	test: 0.711420

Epoch: 69
Loss: 0.1010911387221443
train: 0.997749	val: 0.904145	test: 0.700424

Epoch: 70
Loss: 0.10701043318330684
train: 0.997312	val: 0.889391	test: 0.701968

Epoch: 71
Loss: 0.0972786695653757
train: 0.996503	val: 0.904346	test: 0.709491

Epoch: 72
Loss: 0.11773077133185887
train: 0.997499	val: 0.894309	test: 0.703125

Epoch: 73
Loss: 0.10681671495874985
train: 0.996972	val: 0.877246	test: 0.696663

Epoch: 74
Loss: 0.10865801640466959
train: 0.997033	val: 0.889090	test: 0.704475

Epoch: 75
Loss: 0.08844059085491739
train: 0.997702	val: 0.898826	test: 0.701775

Epoch: 76
Loss: 0.10639524678662685
train: 0.998097	val: 0.885677	test: 0.687693

Epoch: 77
Loss: 0.0828454725800227
train: 0.997716	val: 0.890696	test: 0.685185

Epoch: 78
Loss: 0.09726186284769722
train: 0.998249	val: 0.892201	test: 0.680266

Epoch: 79
Loss: 0.10580112784234015
train: 0.998409	val: 0.891800	test: 0.699942

Epoch: 80
Loss: 0.09170284611793492
train: 0.996374	val: 0.872227	test: 0.701582

Epoch: 81
Loss: 0.08903627977930652
train: 0.998161	val: 0.880859	test: 0.685764

Epoch: 82
Loss: 0.08427572718201669
train: 0.998611	val: 0.883268	test: 0.693287

Epoch: 83
Loss: 0.09466288163912316
train: 0.998404	val: 0.888789	test: 0.689043

Epoch: 84
Loss: 0.08685557581051849
train: 0.998516	val: 0.896617	test: 0.709684

Epoch: 85
Loss: 0.08260576880730817
train: 0.998832	val: 0.895815	test: 0.701485

Epoch: 86
Loss: 0.08703888335897227
train: 0.998924	val: 0.893205	test: 0.691165

Epoch: 87
Loss: 0.08387791771699771
train: 0.998756	val: 0.879354	test: 0.694637

Epoch: 88
Loss: 0.08083440802968853
train: 0.998766	val: 0.878852	test: 0.696277

Epoch: 89
Loss: 0.08526215548570215
train: 0.998680	val: 0.888789	test: 0.699074

Epoch: 90
Loss: 0.08537694201318123
train: 0.998210	val: 0.883469	test: 0.714796

Epoch: 91
Loss: 0.08226219022315386
train: 0.998724	val: 0.885276	test: 0.711130

Epoch: 92
Loss: 0.09623055710022976
train: 0.998343	val: 0.893907	test: 0.697724

Epoch: 93
Loss: 0.08396792898441197
train: 0.998529	val: 0.888387	test: 0.690490

Epoch: 94
Loss: 0.08804440609911617
train: 0.998890	val: 0.896316	test: 0.696759

Epoch: 95
Loss: 0.08302989315722223
train: 0.998753	val: 0.880658	test: 0.698978

Epoch: 96
Loss: 0.08181982106304138
train: 0.998536	val: 0.874034	test: 0.706983

Epoch: 97
Loss: 0.07889988370656345
train: 0.998906	val: 0.900331	test: 0.703511

Epoch: 98
Loss: 0.07207321722402539
train: 0.999252	val: 0.900532	test: 0.688272

Epoch: 99
Loss: 0.06894665888359262
train: 0.998986	val: 0.897420	test: 0.679688

Epoch: 100
Loss: 0.07407097637916214
train: 0.998919	val: 0.890194	test: 0.680363

best train: 0.967493	val: 0.928034	test: 0.700231
end
