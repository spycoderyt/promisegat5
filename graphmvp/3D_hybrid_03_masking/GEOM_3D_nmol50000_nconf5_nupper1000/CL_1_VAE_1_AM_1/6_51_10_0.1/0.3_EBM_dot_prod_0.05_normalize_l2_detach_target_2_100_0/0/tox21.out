9149428_0
--dataset=tox21 --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, seed=42, split='scaffold', split_path=None, verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5237167568730093
train: 0.742872	val: 0.673688	test: 0.627704

Epoch: 2
Loss: 0.33052797440639076
train: 0.783055	val: 0.715215	test: 0.674823

Epoch: 3
Loss: 0.2370260717187468
train: 0.802733	val: 0.755955	test: 0.708256

Epoch: 4
Loss: 0.20639396769779933
train: 0.819798	val: 0.751131	test: 0.714232

Epoch: 5
Loss: 0.19428246675165556
train: 0.836228	val: 0.755164	test: 0.723174

Epoch: 6
Loss: 0.1905515465512778
train: 0.842448	val: 0.748405	test: 0.715005

Epoch: 7
Loss: 0.1861086693672153
train: 0.850433	val: 0.748424	test: 0.731446

Epoch: 8
Loss: 0.18267614188425024
train: 0.859610	val: 0.759728	test: 0.730911

Epoch: 9
Loss: 0.178120665785899
train: 0.864187	val: 0.760699	test: 0.727447

Epoch: 10
Loss: 0.17702010064082027
train: 0.868635	val: 0.767028	test: 0.730107

Epoch: 11
Loss: 0.17718258181754398
train: 0.871771	val: 0.761249	test: 0.734845

Epoch: 12
Loss: 0.17284718438048022
train: 0.873956	val: 0.768365	test: 0.736980

Epoch: 13
Loss: 0.17115586521945797
train: 0.881060	val: 0.771965	test: 0.734971

Epoch: 14
Loss: 0.1696633055495907
train: 0.884740	val: 0.771240	test: 0.739720

Epoch: 15
Loss: 0.16924038145231493
train: 0.887194	val: 0.782087	test: 0.743036

Epoch: 16
Loss: 0.16770330587336685
train: 0.891665	val: 0.774481	test: 0.737887

Epoch: 17
Loss: 0.16556003630071822
train: 0.890453	val: 0.765922	test: 0.738086

Epoch: 18
Loss: 0.1643129755221852
train: 0.895320	val: 0.775478	test: 0.745373

Epoch: 19
Loss: 0.16360830769401208
train: 0.897436	val: 0.762113	test: 0.728651

Epoch: 20
Loss: 0.16071130036538153
train: 0.903831	val: 0.776401	test: 0.742416

Epoch: 21
Loss: 0.16176098209858533
train: 0.900092	val: 0.785753	test: 0.754326

Epoch: 22
Loss: 0.1595304336536509
train: 0.902207	val: 0.781305	test: 0.737123

Epoch: 23
Loss: 0.1581289421265786
train: 0.906639	val: 0.783222	test: 0.742266

Epoch: 24
Loss: 0.15800955876652178
train: 0.907255	val: 0.772502	test: 0.748831

Epoch: 25
Loss: 0.156990624557949
train: 0.910591	val: 0.778212	test: 0.750725

Epoch: 26
Loss: 0.1564596860935671
train: 0.914810	val: 0.779077	test: 0.750503

Epoch: 27
Loss: 0.15548809718158543
train: 0.912826	val: 0.770438	test: 0.736374

Epoch: 28
Loss: 0.15437236610057753
train: 0.917174	val: 0.773878	test: 0.743627

Epoch: 29
Loss: 0.15378714896699883
train: 0.919802	val: 0.780405	test: 0.752814

Epoch: 30
Loss: 0.15296606515659325
train: 0.918260	val: 0.774166	test: 0.745080

Epoch: 31
Loss: 0.15117960723594098
train: 0.920928	val: 0.772200	test: 0.751452

Epoch: 32
Loss: 0.1497420216103006
train: 0.921306	val: 0.775702	test: 0.741550

Epoch: 33
Loss: 0.148427266363998
train: 0.925369	val: 0.776618	test: 0.747168

Epoch: 34
Loss: 0.14878571532458174
train: 0.926796	val: 0.764192	test: 0.744250

Epoch: 35
Loss: 0.14784868945931506
train: 0.930055	val: 0.773660	test: 0.748790

Epoch: 36
Loss: 0.14608325952094825
train: 0.930611	val: 0.772370	test: 0.747054

Epoch: 37
Loss: 0.14615559739634645
train: 0.930614	val: 0.774571	test: 0.751870

Epoch: 38
Loss: 0.146683797106862
train: 0.930971	val: 0.781314	test: 0.746117

Epoch: 39
Loss: 0.14495128496556986
train: 0.932200	val: 0.771889	test: 0.753818

Epoch: 40
Loss: 0.1447472900600147
train: 0.934119	val: 0.765711	test: 0.750393

Epoch: 41
Loss: 0.1442250709676312
train: 0.934588	val: 0.774266	test: 0.751066

Epoch: 42
Loss: 0.1417156909061913
train: 0.937913	val: 0.773062	test: 0.747478

Epoch: 43
Loss: 0.14023358380347598
train: 0.938619	val: 0.776888	test: 0.746995

Epoch: 44
Loss: 0.14060485533789227
train: 0.940289	val: 0.767050	test: 0.745739

Epoch: 45
Loss: 0.13862888263318285
train: 0.942006	val: 0.771130	test: 0.741080

Epoch: 46
Loss: 0.1392753512058866
train: 0.943213	val: 0.780925	test: 0.747478

Epoch: 47
Loss: 0.13872761629739672
train: 0.938858	val: 0.768528	test: 0.746461

Epoch: 48
Loss: 0.1380585082862731
train: 0.944328	val: 0.769497	test: 0.727663

Epoch: 49
Loss: 0.13522968253364345
train: 0.946474	val: 0.769490	test: 0.741391

Epoch: 50
Loss: 0.13542748775128066
train: 0.946861	val: 0.781288	test: 0.755559

Epoch: 51
Loss: 0.13412367745543757
train: 0.947848	val: 0.785088	test: 0.755040

Epoch: 52
Loss: 0.1345988051263974
train: 0.948735	val: 0.772956	test: 0.738993

Epoch: 53
Loss: 0.13340018862445308
train: 0.948791	val: 0.765589	test: 0.747367

Epoch: 54
Loss: 0.13414202130389008
train: 0.948029	val: 0.771541	test: 0.754180

Epoch: 55
Loss: 0.13326399185260232
train: 0.949670	val: 0.769696	test: 0.746619

Epoch: 56
Loss: 0.13177266985510702
train: 0.952017	val: 0.774728	test: 0.752859

Epoch: 57
Loss: 0.13050883483951514
train: 0.953263	val: 0.776366	test: 0.758322

Epoch: 58
Loss: 0.1309150123620511
train: 0.952817	val: 0.783810	test: 0.753954

Epoch: 59
Loss: 0.13026515051428766
train: 0.955077	val: 0.760783	test: 0.741169

Epoch: 60
Loss: 0.12932320109978968
train: 0.956090	val: 0.772023	test: 0.743335

Epoch: 61
Loss: 0.12757256999416178
train: 0.957870	val: 0.770076	test: 0.735449

Epoch: 62
Loss: 0.1278197708874549
train: 0.958592	val: 0.769041	test: 0.736295

Epoch: 63
Loss: 0.12832229367955544
train: 0.956478	val: 0.783189	test: 0.735959

Epoch: 64
Loss: 0.12795042029916534
train: 0.955558	val: 0.776111	test: 0.748341

Epoch: 65
Loss: 0.12537593453400023
train: 0.959280	val: 0.775203	test: 0.747022

Epoch: 66
Loss: 0.12415750014211585
train: 0.962088	val: 0.769823	test: 0.742891

Epoch: 67
Loss: 0.1243331636948954
train: 0.960468	val: 0.765996	test: 0.731625

Epoch: 68
Loss: 0.12527717412799405
train: 0.963003	val: 0.773269	test: 0.746171

Epoch: 69
Loss: 0.1232873295045598
train: 0.963508	val: 0.767160	test: 0.743688

Epoch: 70
Loss: 0.12171238521559888
train: 0.964751	val: 0.781228	test: 0.739068

Epoch: 71
Loss: 0.1214603076992025
train: 0.964420	val: 0.773086	test: 0.737932

Epoch: 72
Loss: 0.11881796394474867
train: 0.965403	val: 0.758619	test: 0.736781

Epoch: 73
Loss: 0.12172814927601695
train: 0.966176	val: 0.764312	test: 0.740997

Epoch: 74
Loss: 0.12113754962435827
train: 0.965774	val: 0.773454	test: 0.743100

Epoch: 75
Loss: 0.12039748726198814
train: 0.966863	val: 0.770323	test: 0.737364

Epoch: 76
Loss: 0.11986356630683949
train: 0.968213	val: 0.766583	test: 0.739324

Epoch: 77
Loss: 0.11728621956601662
train: 0.969075	val: 0.767426	test: 0.738203

Epoch: 78
Loss: 0.11794948368230272
train: 0.969285	val: 0.761964	test: 0.735502

Epoch: 79
Loss: 0.11570897195032344
train: 0.969443	val: 0.773668	test: 0.749216

Epoch: 80
Loss: 0.11661100206251178
train: 0.970789	val: 0.781623	test: 0.744264

Epoch: 81
Loss: 0.11530591769490496
train: 0.971293	val: 0.770041	test: 0.737368

Epoch: 82
Loss: 0.11563264971123659
train: 0.971118	val: 0.766391	test: 0.732577

Epoch: 83
Loss: 0.1156605872534995
train: 0.970637	val: 0.774816	test: 0.750271

Epoch: 84
Loss: 0.11626039725292481
train: 0.972415	val: 0.774064	test: 0.733527

Epoch: 85
Loss: 0.11334887836550932
train: 0.973001	val: 0.774086	test: 0.744664

Epoch: 86
Loss: 0.11144658208332023
train: 0.973970	val: 0.772828	test: 0.743219

Epoch: 87
Loss: 0.109614127439137
train: 0.975005	val: 0.773708	test: 0.737510

Epoch: 88
Loss: 0.11278400525212495
train: 0.974222	val: 0.773493	test: 0.736859

Epoch: 89
Loss: 0.11093772092070385
train: 0.973597	val: 0.767552	test: 0.732384

Epoch: 90
Loss: 0.10971729956116155
train: 0.975785	val: 0.774382	test: 0.736645

Epoch: 91
Loss: 0.10748206005754256
train: 0.975146	val: 0.767851	test: 0.738717

Epoch: 92
Loss: 0.10912514788490434
train: 0.976477	val: 0.774545	test: 0.735056

Epoch: 93
Loss: 0.1077498176819082
train: 0.977831	val: 0.771234	test: 0.732351

Epoch: 94
Loss: 0.10825166460493717
train: 0.976355	val: 0.763466	test: 0.734162

Epoch: 95
Loss: 0.10762092977194251
train: 0.976969	val: 0.778809	test: 0.742752

Epoch: 96
Loss: 0.10780106132018336
train: 0.978883	val: 0.762135	test: 0.735602

Epoch: 97
Loss: 0.10611821981667244
train: 0.979189	val: 0.771317	test: 0.743529

Epoch: 98
Loss: 0.10635820262663792
train: 0.978860	val: 0.768494	test: 0.736732

Epoch: 99
Loss: 0.1061331899550951
train: 0.979836	val: 0.769663	test: 0.737925

Epoch: 100
Loss: 0.10529002682054475
train: 0.979546	val: 0.758075	test: 0.727498

best train: 0.900092	val: 0.785753	test: 0.754326
end
