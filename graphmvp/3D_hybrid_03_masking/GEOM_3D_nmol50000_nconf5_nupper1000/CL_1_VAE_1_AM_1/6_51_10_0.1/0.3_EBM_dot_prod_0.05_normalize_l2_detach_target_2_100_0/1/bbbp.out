11558877_1
--dataset=bbbp --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6414149683299925
train: 0.826110	val: 0.913279	test: 0.638214

Epoch: 2
Loss: 0.4897905937670419
train: 0.869271	val: 0.906354	test: 0.649402

Epoch: 3
Loss: 0.4116166431614475
train: 0.900940	val: 0.902740	test: 0.660976

Epoch: 4
Loss: 0.34541826753792243
train: 0.912189	val: 0.904446	test: 0.656346

Epoch: 5
Loss: 0.3195468238184645
train: 0.928735	val: 0.916190	test: 0.665027

Epoch: 6
Loss: 0.2901044510958623
train: 0.937137	val: 0.906052	test: 0.689815

Epoch: 7
Loss: 0.28025918558206103
train: 0.934691	val: 0.910770	test: 0.690297

Epoch: 8
Loss: 0.25863797802438176
train: 0.947847	val: 0.920305	test: 0.690586

Epoch: 9
Loss: 0.2407583798881053
train: 0.952676	val: 0.926930	test: 0.702836

Epoch: 10
Loss: 0.24395879694667397
train: 0.956744	val: 0.923417	test: 0.710745

Epoch: 11
Loss: 0.23672936579406154
train: 0.960918	val: 0.919904	test: 0.711034

Epoch: 12
Loss: 0.21597443606641756
train: 0.958733	val: 0.926428	test: 0.695505

Epoch: 13
Loss: 0.21204502816392282
train: 0.964898	val: 0.922814	test: 0.693769

Epoch: 14
Loss: 0.2087507581831898
train: 0.968127	val: 0.911974	test: 0.691551

Epoch: 15
Loss: 0.20187588108676455
train: 0.965573	val: 0.921409	test: 0.704475

Epoch: 16
Loss: 0.19447666064679317
train: 0.972495	val: 0.927833	test: 0.695891

Epoch: 17
Loss: 0.18484420761078965
train: 0.968208	val: 0.915086	test: 0.702932

Epoch: 18
Loss: 0.1934462636945268
train: 0.972691	val: 0.910669	test: 0.696470

Epoch: 19
Loss: 0.1828880214512181
train: 0.972986	val: 0.923216	test: 0.710262

Epoch: 20
Loss: 0.18993804951499574
train: 0.974581	val: 0.921710	test: 0.711709

Epoch: 21
Loss: 0.17933477202523945
train: 0.975877	val: 0.910669	test: 0.712288

Epoch: 22
Loss: 0.17121257090687267
train: 0.976129	val: 0.912275	test: 0.700714

Epoch: 23
Loss: 0.16499571208476502
train: 0.980364	val: 0.911673	test: 0.695698

Epoch: 24
Loss: 0.17324320942647367
train: 0.981851	val: 0.911673	test: 0.686825

Epoch: 25
Loss: 0.16452976318784
train: 0.981998	val: 0.914985	test: 0.699749

Epoch: 26
Loss: 0.17600621532217592
train: 0.982657	val: 0.918197	test: 0.697434

Epoch: 27
Loss: 0.16899455256244147
train: 0.985390	val: 0.911472	test: 0.692130

Epoch: 28
Loss: 0.15346825520953322
train: 0.982737	val: 0.910368	test: 0.713156

Epoch: 29
Loss: 0.15241924607220972
train: 0.986847	val: 0.919101	test: 0.716242

Epoch: 30
Loss: 0.13321862053233766
train: 0.986380	val: 0.903142	test: 0.726659

Epoch: 31
Loss: 0.142349671557153
train: 0.986841	val: 0.914082	test: 0.707079

Epoch: 32
Loss: 0.16254304885820803
train: 0.989783	val: 0.904346	test: 0.701582

Epoch: 33
Loss: 0.14360887161469846
train: 0.988285	val: 0.903041	test: 0.722704

Epoch: 34
Loss: 0.15233812009135414
train: 0.987297	val: 0.907859	test: 0.711902

Epoch: 35
Loss: 0.13509728114159028
train: 0.990776	val: 0.911774	test: 0.711806

Epoch: 36
Loss: 0.14517424304187027
train: 0.990362	val: 0.899930	test: 0.716049

Epoch: 37
Loss: 0.1411819828817754
train: 0.991386	val: 0.894108	test: 0.704572

Epoch: 38
Loss: 0.13584569810784372
train: 0.991503	val: 0.906354	test: 0.707272

Epoch: 39
Loss: 0.13222904014513187
train: 0.990852	val: 0.905149	test: 0.711709

Epoch: 40
Loss: 0.14085912077615242
train: 0.990797	val: 0.893907	test: 0.698592

Epoch: 41
Loss: 0.13354671682859337
train: 0.990657	val: 0.898525	test: 0.709684

Epoch: 42
Loss: 0.1341527443313739
train: 0.993308	val: 0.901134	test: 0.712867

Epoch: 43
Loss: 0.11918131900604638
train: 0.991615	val: 0.900231	test: 0.705536

Epoch: 44
Loss: 0.1205996904687221
train: 0.993992	val: 0.897621	test: 0.684896

Epoch: 45
Loss: 0.12081993166641018
train: 0.993904	val: 0.893807	test: 0.706211

Epoch: 46
Loss: 0.13549994224996179
train: 0.992594	val: 0.903242	test: 0.725694

Epoch: 47
Loss: 0.11857805178269196
train: 0.991749	val: 0.893807	test: 0.733218

Epoch: 48
Loss: 0.11659952953554728
train: 0.994206	val: 0.897220	test: 0.722704

Epoch: 49
Loss: 0.11971511113414124
train: 0.994576	val: 0.902740	test: 0.707658

Epoch: 50
Loss: 0.12303496749884323
train: 0.994424	val: 0.903844	test: 0.708430

Epoch: 51
Loss: 0.11586360593328494
train: 0.993031	val: 0.908963	test: 0.731867

Epoch: 52
Loss: 0.11931390243734184
train: 0.994977	val: 0.908762	test: 0.720968

Epoch: 53
Loss: 0.11856635538356405
train: 0.995438	val: 0.903443	test: 0.696952

Epoch: 54
Loss: 0.11681829598138868
train: 0.995959	val: 0.908060	test: 0.722415

Epoch: 55
Loss: 0.11794242750886674
train: 0.995868	val: 0.908261	test: 0.724344

Epoch: 56
Loss: 0.11942139965311617
train: 0.996307	val: 0.903844	test: 0.706019

Epoch: 57
Loss: 0.09761429114549446
train: 0.995399	val: 0.900432	test: 0.696373

Epoch: 58
Loss: 0.11684165392522476
train: 0.995934	val: 0.902740	test: 0.701968

Epoch: 59
Loss: 0.09896549434219175
train: 0.995361	val: 0.907257	test: 0.714313

Epoch: 60
Loss: 0.10420053010968454
train: 0.996793	val: 0.904246	test: 0.711323

Epoch: 61
Loss: 0.10549325744720613
train: 0.996237	val: 0.890896	test: 0.721933

Epoch: 62
Loss: 0.1092534327408219
train: 0.996280	val: 0.882064	test: 0.714313

Epoch: 63
Loss: 0.10380163779815361
train: 0.997436	val: 0.886380	test: 0.687596

Epoch: 64
Loss: 0.09999293879284583
train: 0.996029	val: 0.894409	test: 0.731192

Epoch: 65
Loss: 0.10259436133300161
train: 0.996806	val: 0.901335	test: 0.718557

Epoch: 66
Loss: 0.10948858405406565
train: 0.997036	val: 0.910870	test: 0.707369

Epoch: 67
Loss: 0.10411993836146274
train: 0.997032	val: 0.890495	test: 0.715856

Epoch: 68
Loss: 0.0987689990106113
train: 0.997043	val: 0.889290	test: 0.703125

Epoch: 69
Loss: 0.09484065418591203
train: 0.997064	val: 0.890194	test: 0.693769

Epoch: 70
Loss: 0.1056625247034982
train: 0.997514	val: 0.893707	test: 0.698688

Epoch: 71
Loss: 0.09317686360893061
train: 0.997945	val: 0.895714	test: 0.703414

Epoch: 72
Loss: 0.08175556583532072
train: 0.997721	val: 0.890294	test: 0.687789

Epoch: 73
Loss: 0.08501359207684223
train: 0.997952	val: 0.896116	test: 0.712674

Epoch: 74
Loss: 0.0925940680377164
train: 0.997370	val: 0.890896	test: 0.709973

Epoch: 75
Loss: 0.08214005857722602
train: 0.998113	val: 0.877447	test: 0.693383

Epoch: 76
Loss: 0.08388474177605308
train: 0.998076	val: 0.879153	test: 0.695409

Epoch: 77
Loss: 0.08851018322595085
train: 0.998234	val: 0.882967	test: 0.713252

Epoch: 78
Loss: 0.08159339634187197
train: 0.998315	val: 0.884974	test: 0.686728

Epoch: 79
Loss: 0.08545269150689185
train: 0.998310	val: 0.887484	test: 0.690683

Epoch: 80
Loss: 0.09450944503538317
train: 0.998168	val: 0.887484	test: 0.707948

Epoch: 81
Loss: 0.09270689639447446
train: 0.998249	val: 0.882565	test: 0.702160

Epoch: 82
Loss: 0.09252877180984322
train: 0.998054	val: 0.889491	test: 0.709008

Epoch: 83
Loss: 0.07616691743457726
train: 0.998725	val: 0.892402	test: 0.704379

Epoch: 84
Loss: 0.08498438550727463
train: 0.998718	val: 0.888789	test: 0.705826

Epoch: 85
Loss: 0.08380681212461707
train: 0.998681	val: 0.890294	test: 0.714988

Epoch: 86
Loss: 0.0813406522677603
train: 0.998733	val: 0.892603	test: 0.704090

Epoch: 87
Loss: 0.10671795204577118
train: 0.998644	val: 0.887785	test: 0.695023

Epoch: 88
Loss: 0.08493134005040935
train: 0.998704	val: 0.891298	test: 0.701196

Epoch: 89
Loss: 0.08582204201108302
train: 0.998858	val: 0.891097	test: 0.703607

Epoch: 90
Loss: 0.07364685865378362
train: 0.998283	val: 0.883670	test: 0.684221

Epoch: 91
Loss: 0.0883662467834468
train: 0.998854	val: 0.896919	test: 0.698688

Epoch: 92
Loss: 0.08305677315241074
train: 0.998583	val: 0.894710	test: 0.693094

Epoch: 93
Loss: 0.09378165573634502
train: 0.998805	val: 0.884874	test: 0.690490

Epoch: 94
Loss: 0.07835926107305229
train: 0.998494	val: 0.893506	test: 0.687307

Epoch: 95
Loss: 0.08723422121303681
train: 0.998346	val: 0.877346	test: 0.688079

Epoch: 96
Loss: 0.07653338306073086
train: 0.999081	val: 0.886580	test: 0.684414

Epoch: 97
Loss: 0.07308874420869674
train: 0.998864	val: 0.884473	test: 0.700907

Epoch: 98
Loss: 0.0686904271026821
train: 0.998774	val: 0.887183	test: 0.707369

Epoch: 99
Loss: 0.08128459473967806
train: 0.999038	val: 0.874335	test: 0.698302

Epoch: 100
Loss: 0.06460815971767618
train: 0.998850	val: 0.875238	test: 0.687596

best train: 0.972495	val: 0.927833	test: 0.695891
end
