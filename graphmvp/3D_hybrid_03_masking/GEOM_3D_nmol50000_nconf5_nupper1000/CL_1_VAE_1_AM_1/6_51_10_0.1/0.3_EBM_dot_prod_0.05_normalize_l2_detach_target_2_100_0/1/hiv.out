11558877_1
--dataset=hiv --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.24481166646796115
train: 0.775539	val: 0.761458	test: 0.738334

Epoch: 2
Loss: 0.13520706788740977
train: 0.797881	val: 0.776758	test: 0.729309

Epoch: 3
Loss: 0.1311308846737836
train: 0.812490	val: 0.796521	test: 0.733758

Epoch: 4
Loss: 0.12753515531982304
train: 0.827541	val: 0.773289	test: 0.732805

Epoch: 5
Loss: 0.12498008931149064
train: 0.830419	val: 0.813961	test: 0.760754

Epoch: 6
Loss: 0.12179086894419654
train: 0.843595	val: 0.803446	test: 0.732836

Epoch: 7
Loss: 0.12091853288251844
train: 0.845010	val: 0.819625	test: 0.746569

Epoch: 8
Loss: 0.11892332394479645
train: 0.853401	val: 0.785846	test: 0.762158

Epoch: 9
Loss: 0.11721931892349305
train: 0.868469	val: 0.818762	test: 0.753792

Epoch: 10
Loss: 0.11680419488886404
train: 0.862070	val: 0.792864	test: 0.737280

Epoch: 11
Loss: 0.11528437078081852
train: 0.874080	val: 0.817751	test: 0.750024

Epoch: 12
Loss: 0.11570034411486999
train: 0.872495	val: 0.800886	test: 0.751465

Epoch: 13
Loss: 0.11346894153878176
train: 0.875071	val: 0.801569	test: 0.774783

Epoch: 14
Loss: 0.11178249980742168
train: 0.877417	val: 0.791795	test: 0.741233

Epoch: 15
Loss: 0.1109709467050086
train: 0.883442	val: 0.802916	test: 0.752467

Epoch: 16
Loss: 0.11093886949141192
train: 0.884445	val: 0.795733	test: 0.746878

Epoch: 17
Loss: 0.11032603866567757
train: 0.890262	val: 0.826839	test: 0.746727

Epoch: 18
Loss: 0.10772570037560056
train: 0.880364	val: 0.791354	test: 0.738255

Epoch: 19
Loss: 0.10830465340541011
train: 0.889533	val: 0.794787	test: 0.760351

Epoch: 20
Loss: 0.10606294545129588
train: 0.890370	val: 0.796232	test: 0.752666

Epoch: 21
Loss: 0.10718420404027813
train: 0.901764	val: 0.803118	test: 0.746895

Epoch: 22
Loss: 0.10599672346887361
train: 0.898149	val: 0.802512	test: 0.747778

Epoch: 23
Loss: 0.1063630757533098
train: 0.905880	val: 0.809043	test: 0.757187

Epoch: 24
Loss: 0.10505871489428033
train: 0.906796	val: 0.816808	test: 0.760830

Epoch: 25
Loss: 0.1040742130396102
train: 0.907941	val: 0.804300	test: 0.770588

Epoch: 26
Loss: 0.10399351266371389
train: 0.912690	val: 0.808734	test: 0.751700

Epoch: 27
Loss: 0.10216867422047805
train: 0.914406	val: 0.816147	test: 0.764404

Epoch: 28
Loss: 0.10132578424940934
train: 0.920562	val: 0.820975	test: 0.752037

Epoch: 29
Loss: 0.10042103117677033
train: 0.912537	val: 0.802683	test: 0.770658

Epoch: 30
Loss: 0.10055777637052721
train: 0.919776	val: 0.809766	test: 0.750480

Epoch: 31
Loss: 0.10135336380735993
train: 0.918384	val: 0.820982	test: 0.770025

Epoch: 32
Loss: 0.09927406237377767
train: 0.922792	val: 0.813348	test: 0.764818

Epoch: 33
Loss: 0.09988342225525608
train: 0.926672	val: 0.828465	test: 0.756212

Epoch: 34
Loss: 0.09841614381890837
train: 0.923595	val: 0.807077	test: 0.766674

Epoch: 35
Loss: 0.09944151340341893
train: 0.931230	val: 0.813241	test: 0.764296

Epoch: 36
Loss: 0.09760651458710347
train: 0.933343	val: 0.803020	test: 0.751407

Epoch: 37
Loss: 0.09615433977614499
train: 0.930109	val: 0.793672	test: 0.757703

Epoch: 38
Loss: 0.09781664466134252
train: 0.935246	val: 0.804389	test: 0.757087

Epoch: 39
Loss: 0.09658436379633485
train: 0.932017	val: 0.797910	test: 0.751453

Epoch: 40
Loss: 0.0948926198270853
train: 0.927653	val: 0.792726	test: 0.764097

Epoch: 41
Loss: 0.09626762090145646
train: 0.932943	val: 0.786853	test: 0.767176

Epoch: 42
Loss: 0.09640827916162646
train: 0.934676	val: 0.788608	test: 0.751200

Epoch: 43
Loss: 0.09370772551987633
train: 0.941175	val: 0.796700	test: 0.750482

Epoch: 44
Loss: 0.09385952121033644
train: 0.937993	val: 0.797279	test: 0.763105

Epoch: 45
Loss: 0.09326801309584584
train: 0.942196	val: 0.780096	test: 0.764379

Epoch: 46
Loss: 0.09255008870618865
train: 0.936838	val: 0.798486	test: 0.754399

Epoch: 47
Loss: 0.0922259525080517
train: 0.946107	val: 0.797702	test: 0.774125

Epoch: 48
Loss: 0.09108854096068185
train: 0.945198	val: 0.814729	test: 0.761801

Epoch: 49
Loss: 0.09099970848914929
train: 0.945154	val: 0.783357	test: 0.737594

Epoch: 50
Loss: 0.09200953823643886
train: 0.941998	val: 0.811144	test: 0.764982

Epoch: 51
Loss: 0.09156640253452648
train: 0.938178	val: 0.800479	test: 0.750219

Epoch: 52
Loss: 0.09018447727856904
train: 0.944300	val: 0.797735	test: 0.770171

Epoch: 53
Loss: 0.09127445341937096
train: 0.948630	val: 0.809156	test: 0.754005

Epoch: 54
Loss: 0.09053283683286574
train: 0.955812	val: 0.797111	test: 0.751440

Epoch: 55
Loss: 0.0885707953754956
train: 0.950960	val: 0.800531	test: 0.748164

Epoch: 56
Loss: 0.08931361996757566
train: 0.956551	val: 0.805381	test: 0.769700

Epoch: 57
Loss: 0.08819591592837724
train: 0.951424	val: 0.773914	test: 0.733933

Epoch: 58
Loss: 0.08599368802593807
train: 0.957328	val: 0.808967	test: 0.771510

Epoch: 59
Loss: 0.08857147475734832
train: 0.955808	val: 0.798011	test: 0.746809

Epoch: 60
Loss: 0.08717533688403765
train: 0.957673	val: 0.809851	test: 0.750291

Epoch: 61
Loss: 0.0881487916089126
train: 0.958647	val: 0.807978	test: 0.754735

Epoch: 62
Loss: 0.08654052563628098
train: 0.958574	val: 0.813587	test: 0.747792

Epoch: 63
Loss: 0.08673824605223375
train: 0.957118	val: 0.792037	test: 0.741073

Epoch: 64
Loss: 0.08530624602763515
train: 0.962551	val: 0.784888	test: 0.753599

Epoch: 65
Loss: 0.08567039687888925
train: 0.959829	val: 0.813642	test: 0.761370

Epoch: 66
Loss: 0.08436735072068213
train: 0.964776	val: 0.797445	test: 0.753947

Epoch: 67
Loss: 0.08415604632875048
train: 0.963461	val: 0.786011	test: 0.756403

Epoch: 68
Loss: 0.0842947114146043
train: 0.959423	val: 0.795598	test: 0.746625

Epoch: 69
Loss: 0.08300157838251762
train: 0.958039	val: 0.792588	test: 0.751608

Epoch: 70
Loss: 0.08331663983147411
train: 0.965791	val: 0.801655	test: 0.747137

Epoch: 71
Loss: 0.08162467267044052
train: 0.968819	val: 0.807426	test: 0.741664

Epoch: 72
Loss: 0.08342633202128273
train: 0.966941	val: 0.808810	test: 0.739255

Epoch: 73
Loss: 0.0808428893111129
train: 0.970068	val: 0.803519	test: 0.768022

Epoch: 74
Loss: 0.08188943819498053
train: 0.969135	val: 0.800427	test: 0.751890

Epoch: 75
Loss: 0.08003669723085169
train: 0.966927	val: 0.780846	test: 0.743506

Epoch: 76
Loss: 0.08092721385790316
train: 0.971656	val: 0.802132	test: 0.756645

Epoch: 77
Loss: 0.08043943249537316
train: 0.968880	val: 0.804380	test: 0.739931

Epoch: 78
Loss: 0.08061842320596903
train: 0.969635	val: 0.791896	test: 0.750534

Epoch: 79
Loss: 0.08126729489725684
train: 0.970488	val: 0.789239	test: 0.728226

Epoch: 80
Loss: 0.07889583426180595
train: 0.974438	val: 0.806030	test: 0.739688

Epoch: 81
Loss: 0.07835939179177746
train: 0.974323	val: 0.812867	test: 0.744292

Epoch: 82
Loss: 0.07892278273939589
train: 0.971262	val: 0.802935	test: 0.735485

Epoch: 83
Loss: 0.07783576065506097
train: 0.976024	val: 0.824840	test: 0.756757

Epoch: 84
Loss: 0.07734957814194587
train: 0.973509	val: 0.809303	test: 0.747867

Epoch: 85
Loss: 0.0761703333087556
train: 0.967122	val: 0.803262	test: 0.724489

Epoch: 86
Loss: 0.07704917935497399
train: 0.976044	val: 0.799245	test: 0.734191

Epoch: 87
Loss: 0.07653563824180007
train: 0.976705	val: 0.801131	test: 0.748728

Epoch: 88
Loss: 0.07655414140278342
train: 0.977445	val: 0.807941	test: 0.752446

Epoch: 89
Loss: 0.07677550825931137
train: 0.978258	val: 0.800623	test: 0.742712

Epoch: 90
Loss: 0.07565486275040614
train: 0.977300	val: 0.795653	test: 0.739280

Epoch: 91
Loss: 0.07637385766398519
train: 0.979192	val: 0.813440	test: 0.756399

Epoch: 92
Loss: 0.07591471957296884
train: 0.975996	val: 0.823890	test: 0.741183

Epoch: 93
Loss: 0.0750701825434817
train: 0.979599	val: 0.822776	test: 0.753562

Epoch: 94
Loss: 0.07358287875626986
train: 0.972687	val: 0.804009	test: 0.734234

Epoch: 95
Loss: 0.07581221377456808
train: 0.980809	val: 0.816977	test: 0.741416

Epoch: 96
Loss: 0.07308324772038746
train: 0.980999	val: 0.814264	test: 0.746880

Epoch: 97
Loss: 0.07347096377640217
train: 0.978676	val: 0.811389	test: 0.747990

Epoch: 98
Loss: 0.07332938559130342
train: 0.980913	val: 0.798939	test: 0.740700

Epoch: 99
Loss: 0.07350660341993567
train: 0.982118	val: 0.817359	test: 0.753732

Epoch: 100
Loss: 0.07145776928488719
train: 0.979840	val: 0.802383	test: 0.752216

best train: 0.926672	val: 0.828465	test: 0.756212
end
