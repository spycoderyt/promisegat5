11558877_1
--dataset=sider --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6945917781778346
train: 0.527689	val: 0.524158	test: 0.505749

Epoch: 2
Loss: 0.648776393471964
train: 0.556898	val: 0.516650	test: 0.514220

Epoch: 3
Loss: 0.6122440478716362
train: 0.571549	val: 0.508004	test: 0.523187

Epoch: 4
Loss: 0.5817329533924849
train: 0.592219	val: 0.512097	test: 0.537271

Epoch: 5
Loss: 0.5610086180203184
train: 0.628212	val: 0.538464	test: 0.563060

Epoch: 6
Loss: 0.5421016777025077
train: 0.650057	val: 0.566461	test: 0.585144

Epoch: 7
Loss: 0.5326693273495613
train: 0.662758	val: 0.564634	test: 0.599310

Epoch: 8
Loss: 0.5219662287715643
train: 0.674436	val: 0.568579	test: 0.601734

Epoch: 9
Loss: 0.5152179185311291
train: 0.685455	val: 0.578983	test: 0.599467

Epoch: 10
Loss: 0.5068650707043143
train: 0.692014	val: 0.591560	test: 0.595718

Epoch: 11
Loss: 0.49887596522590183
train: 0.701601	val: 0.609732	test: 0.594887

Epoch: 12
Loss: 0.49701662188655354
train: 0.714212	val: 0.608499	test: 0.608060

Epoch: 13
Loss: 0.48761468220377246
train: 0.722574	val: 0.601439	test: 0.610246

Epoch: 14
Loss: 0.48930716908899063
train: 0.726387	val: 0.592241	test: 0.607576

Epoch: 15
Loss: 0.48194775065186796
train: 0.737735	val: 0.601617	test: 0.608740

Epoch: 16
Loss: 0.4790833101098154
train: 0.742314	val: 0.617524	test: 0.603080

Epoch: 17
Loss: 0.47472594969959186
train: 0.746608	val: 0.609982	test: 0.607552

Epoch: 18
Loss: 0.47382322147891126
train: 0.750470	val: 0.601310	test: 0.610984

Epoch: 19
Loss: 0.46787209481162295
train: 0.756607	val: 0.608427	test: 0.618929

Epoch: 20
Loss: 0.46162001839929234
train: 0.761585	val: 0.618445	test: 0.618799

Epoch: 21
Loss: 0.46285271933134053
train: 0.764997	val: 0.610316	test: 0.617766

Epoch: 22
Loss: 0.46101801068731
train: 0.770177	val: 0.607465	test: 0.619172

Epoch: 23
Loss: 0.4599807334202352
train: 0.769690	val: 0.614134	test: 0.621783

Epoch: 24
Loss: 0.4553368849813653
train: 0.775054	val: 0.628608	test: 0.615557

Epoch: 25
Loss: 0.4581642542839665
train: 0.779374	val: 0.623331	test: 0.618010

Epoch: 26
Loss: 0.452231110003048
train: 0.785242	val: 0.629199	test: 0.622761

Epoch: 27
Loss: 0.4473219422357487
train: 0.781996	val: 0.612314	test: 0.618573

Epoch: 28
Loss: 0.450696108261652
train: 0.789632	val: 0.618419	test: 0.609394

Epoch: 29
Loss: 0.4473606521251769
train: 0.788510	val: 0.623484	test: 0.608809

Epoch: 30
Loss: 0.4476155320451934
train: 0.797532	val: 0.616879	test: 0.622628

Epoch: 31
Loss: 0.44156646981454417
train: 0.794991	val: 0.613436	test: 0.618714

Epoch: 32
Loss: 0.4434429754326999
train: 0.796920	val: 0.622416	test: 0.614467

Epoch: 33
Loss: 0.4403537905919023
train: 0.795746	val: 0.617547	test: 0.607245

Epoch: 34
Loss: 0.44026305051381903
train: 0.803532	val: 0.615567	test: 0.603717

Epoch: 35
Loss: 0.4389604960635868
train: 0.806194	val: 0.618847	test: 0.611143

Epoch: 36
Loss: 0.43564383073838364
train: 0.816122	val: 0.623711	test: 0.627025

Epoch: 37
Loss: 0.432553793159624
train: 0.816586	val: 0.628919	test: 0.624152

Epoch: 38
Loss: 0.4354928076125562
train: 0.816057	val: 0.613685	test: 0.614891

Epoch: 39
Loss: 0.4287644432611035
train: 0.824203	val: 0.608589	test: 0.622895

Epoch: 40
Loss: 0.4295867119273984
train: 0.824061	val: 0.615464	test: 0.628518

Epoch: 41
Loss: 0.42536844291627957
train: 0.824173	val: 0.618698	test: 0.623764

Epoch: 42
Loss: 0.432820264981983
train: 0.830198	val: 0.621055	test: 0.617627

Epoch: 43
Loss: 0.42476035111633637
train: 0.832187	val: 0.611522	test: 0.608811

Epoch: 44
Loss: 0.4278369373313701
train: 0.834008	val: 0.616358	test: 0.615523

Epoch: 45
Loss: 0.4210274438016439
train: 0.833129	val: 0.619384	test: 0.627833

Epoch: 46
Loss: 0.42227646566254406
train: 0.838997	val: 0.603341	test: 0.625817

Epoch: 47
Loss: 0.4169166263144294
train: 0.840184	val: 0.599493	test: 0.617606

Epoch: 48
Loss: 0.41501159178764907
train: 0.842006	val: 0.602228	test: 0.623493

Epoch: 49
Loss: 0.4167277154790406
train: 0.841378	val: 0.618437	test: 0.615288

Epoch: 50
Loss: 0.41945429153781666
train: 0.842752	val: 0.627292	test: 0.616816

Epoch: 51
Loss: 0.4093334507977039
train: 0.846096	val: 0.621577	test: 0.623584

Epoch: 52
Loss: 0.4068744158879903
train: 0.850666	val: 0.616932	test: 0.611641

Epoch: 53
Loss: 0.4120482621351618
train: 0.850038	val: 0.613339	test: 0.607006

Epoch: 54
Loss: 0.4037008994703625
train: 0.850583	val: 0.614818	test: 0.607157

Epoch: 55
Loss: 0.4098584002389331
train: 0.852729	val: 0.611868	test: 0.601405

Epoch: 56
Loss: 0.40509528192838556
train: 0.855439	val: 0.604347	test: 0.605634

Epoch: 57
Loss: 0.4071000770380052
train: 0.854949	val: 0.601366	test: 0.600584

Epoch: 58
Loss: 0.40738550504267906
train: 0.856534	val: 0.619127	test: 0.604771

Epoch: 59
Loss: 0.40652488454800784
train: 0.859235	val: 0.619150	test: 0.613096

Epoch: 60
Loss: 0.40127067672978045
train: 0.861125	val: 0.615045	test: 0.606619

Epoch: 61
Loss: 0.4043912859973887
train: 0.862010	val: 0.613139	test: 0.614743

Epoch: 62
Loss: 0.39338952701995733
train: 0.863116	val: 0.618528	test: 0.619691

Epoch: 63
Loss: 0.39745616696230623
train: 0.863015	val: 0.623063	test: 0.611193

Epoch: 64
Loss: 0.3976481849814119
train: 0.866833	val: 0.626572	test: 0.605530

Epoch: 65
Loss: 0.40177529748557317
train: 0.862984	val: 0.622181	test: 0.615971

Epoch: 66
Loss: 0.39186548546768446
train: 0.869598	val: 0.624467	test: 0.620172

Epoch: 67
Loss: 0.3920110957930609
train: 0.872578	val: 0.616989	test: 0.609431

Epoch: 68
Loss: 0.39194508107314435
train: 0.871995	val: 0.602645	test: 0.611932

Epoch: 69
Loss: 0.393819513782499
train: 0.872349	val: 0.595223	test: 0.613111

Epoch: 70
Loss: 0.3928574892111786
train: 0.875527	val: 0.603540	test: 0.611263

Epoch: 71
Loss: 0.38977292491898397
train: 0.876483	val: 0.615157	test: 0.613369

Epoch: 72
Loss: 0.384469426039169
train: 0.874172	val: 0.612223	test: 0.609529

Epoch: 73
Loss: 0.3868343393653597
train: 0.877581	val: 0.607502	test: 0.604756

Epoch: 74
Loss: 0.3890134413736992
train: 0.879234	val: 0.608406	test: 0.608935

Epoch: 75
Loss: 0.3837142663663033
train: 0.879485	val: 0.611880	test: 0.605955

Epoch: 76
Loss: 0.3845409306643088
train: 0.880807	val: 0.614192	test: 0.616501

Epoch: 77
Loss: 0.3864149354637204
train: 0.882501	val: 0.606295	test: 0.623080

Epoch: 78
Loss: 0.38446375924704984
train: 0.880260	val: 0.597427	test: 0.619104

Epoch: 79
Loss: 0.3792674717857737
train: 0.884841	val: 0.606592	test: 0.608149

Epoch: 80
Loss: 0.37904875405583477
train: 0.887232	val: 0.611665	test: 0.604673

Epoch: 81
Loss: 0.3806053060874987
train: 0.887283	val: 0.599054	test: 0.610707

Epoch: 82
Loss: 0.37711096899851315
train: 0.887772	val: 0.597560	test: 0.613908

Epoch: 83
Loss: 0.3747144305338211
train: 0.891921	val: 0.610631	test: 0.606549

Epoch: 84
Loss: 0.3726394166680543
train: 0.893523	val: 0.612478	test: 0.608250

Epoch: 85
Loss: 0.3714515097987867
train: 0.893441	val: 0.615530	test: 0.620672

Epoch: 86
Loss: 0.37556187683484205
train: 0.891747	val: 0.610876	test: 0.626877

Epoch: 87
Loss: 0.37082693622402935
train: 0.890268	val: 0.616314	test: 0.626932

Epoch: 88
Loss: 0.36893358052961744
train: 0.893701	val: 0.627471	test: 0.625390

Epoch: 89
Loss: 0.37164616502753595
train: 0.897789	val: 0.625743	test: 0.620529

Epoch: 90
Loss: 0.3696738041970889
train: 0.897118	val: 0.628887	test: 0.613564

Epoch: 91
Loss: 0.3661223866321654
train: 0.896897	val: 0.622435	test: 0.607448

Epoch: 92
Loss: 0.3616465141958443
train: 0.898032	val: 0.612304	test: 0.613387

Epoch: 93
Loss: 0.37031018851482234
train: 0.895568	val: 0.601274	test: 0.600521

Epoch: 94
Loss: 0.3658701506030317
train: 0.900840	val: 0.608264	test: 0.604679

Epoch: 95
Loss: 0.3646765243298363
train: 0.901901	val: 0.620139	test: 0.620233

Epoch: 96
Loss: 0.3607542260730976
train: 0.903327	val: 0.615623	test: 0.612614

Epoch: 97
Loss: 0.3684970626925999
train: 0.903422	val: 0.606075	test: 0.598476

Epoch: 98
Loss: 0.36363798245876994
train: 0.905943	val: 0.619206	test: 0.605244

Epoch: 99
Loss: 0.3596870948230525
train: 0.904944	val: 0.635374	test: 0.629937

Epoch: 100
Loss: 0.3558507853646948
train: 0.905911	val: 0.637034	test: 0.623978

best train: 0.905911	val: 0.637034	test: 0.623978
end
