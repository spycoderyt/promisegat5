11558877_1
--dataset=tox21 --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_03_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1_AM_1/6_51_10_0.1/0.3_EBM_dot_prod_0.05_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5112530494584875
train: 0.726775	val: 0.630274	test: 0.605923

Epoch: 2
Loss: 0.3258176295419816
train: 0.772646	val: 0.698427	test: 0.662978

Epoch: 3
Loss: 0.23643305505462184
train: 0.801189	val: 0.735521	test: 0.700655

Epoch: 4
Loss: 0.20676755773171793
train: 0.826124	val: 0.763697	test: 0.716444

Epoch: 5
Loss: 0.19544099862129033
train: 0.838274	val: 0.757638	test: 0.719522

Epoch: 6
Loss: 0.19026621324472115
train: 0.843654	val: 0.761690	test: 0.717149

Epoch: 7
Loss: 0.18543475624957156
train: 0.853143	val: 0.772556	test: 0.728229

Epoch: 8
Loss: 0.18531518724021537
train: 0.856345	val: 0.761997	test: 0.715005

Epoch: 9
Loss: 0.18170388491248887
train: 0.861945	val: 0.767075	test: 0.728324

Epoch: 10
Loss: 0.1793004998960366
train: 0.863596	val: 0.759635	test: 0.722096

Epoch: 11
Loss: 0.17659456668378617
train: 0.866492	val: 0.765380	test: 0.732707

Epoch: 12
Loss: 0.1748714615207832
train: 0.877417	val: 0.773178	test: 0.732727

Epoch: 13
Loss: 0.17247045883051515
train: 0.880278	val: 0.764698	test: 0.736744

Epoch: 14
Loss: 0.17114668879744932
train: 0.884586	val: 0.781509	test: 0.738994

Epoch: 15
Loss: 0.16797312778203097
train: 0.888983	val: 0.768953	test: 0.737964

Epoch: 16
Loss: 0.16586733612208934
train: 0.891361	val: 0.768279	test: 0.747535

Epoch: 17
Loss: 0.16598563809158282
train: 0.891838	val: 0.774948	test: 0.743863

Epoch: 18
Loss: 0.16473668036765576
train: 0.893101	val: 0.774829	test: 0.742349

Epoch: 19
Loss: 0.16283791698738345
train: 0.892961	val: 0.768777	test: 0.739502

Epoch: 20
Loss: 0.16245268857169834
train: 0.898165	val: 0.777062	test: 0.735936

Epoch: 21
Loss: 0.16289889006644348
train: 0.903554	val: 0.758529	test: 0.739019

Epoch: 22
Loss: 0.15921290194122034
train: 0.905705	val: 0.775957	test: 0.741692

Epoch: 23
Loss: 0.15744825028550513
train: 0.906831	val: 0.772545	test: 0.735163

Epoch: 24
Loss: 0.15723759626159262
train: 0.908835	val: 0.775412	test: 0.743144

Epoch: 25
Loss: 0.15824127005226413
train: 0.908794	val: 0.757990	test: 0.739555

Epoch: 26
Loss: 0.15503090552492313
train: 0.914453	val: 0.772866	test: 0.742384

Epoch: 27
Loss: 0.1539074952969827
train: 0.914418	val: 0.775354	test: 0.745754

Epoch: 28
Loss: 0.15395841554256393
train: 0.914782	val: 0.766284	test: 0.745913

Epoch: 29
Loss: 0.15301620237508007
train: 0.919147	val: 0.776790	test: 0.754157

Epoch: 30
Loss: 0.1505327011892817
train: 0.921593	val: 0.775483	test: 0.744403

Epoch: 31
Loss: 0.15062754216673915
train: 0.921977	val: 0.771334	test: 0.743475

Epoch: 32
Loss: 0.15010471242894663
train: 0.922777	val: 0.768503	test: 0.747001

Epoch: 33
Loss: 0.14785914174361228
train: 0.927456	val: 0.763284	test: 0.743283

Epoch: 34
Loss: 0.14716469403553314
train: 0.926001	val: 0.760362	test: 0.739524

Epoch: 35
Loss: 0.14820332645394813
train: 0.929325	val: 0.766640	test: 0.747904

Epoch: 36
Loss: 0.1465324030002594
train: 0.930385	val: 0.768854	test: 0.739642

Epoch: 37
Loss: 0.14587235941756094
train: 0.933417	val: 0.766713	test: 0.747215

Epoch: 38
Loss: 0.14520395242298448
train: 0.932550	val: 0.756082	test: 0.738245

Epoch: 39
Loss: 0.14376068936788075
train: 0.934812	val: 0.770372	test: 0.747039

Epoch: 40
Loss: 0.14183568892409892
train: 0.936399	val: 0.774115	test: 0.747486

Epoch: 41
Loss: 0.14194005925511718
train: 0.936668	val: 0.773526	test: 0.750457

Epoch: 42
Loss: 0.14190674467294875
train: 0.939337	val: 0.762457	test: 0.748308

Epoch: 43
Loss: 0.140686385409601
train: 0.941167	val: 0.772964	test: 0.746000

Epoch: 44
Loss: 0.14062959220831547
train: 0.939358	val: 0.762218	test: 0.746528

Epoch: 45
Loss: 0.1394891688469246
train: 0.938566	val: 0.771757	test: 0.746417

Epoch: 46
Loss: 0.1389460210192242
train: 0.943703	val: 0.772058	test: 0.750013

Epoch: 47
Loss: 0.1355444247890786
train: 0.942289	val: 0.763310	test: 0.753318

Epoch: 48
Loss: 0.13742911104257297
train: 0.945748	val: 0.774706	test: 0.742889

Epoch: 49
Loss: 0.13629143710860653
train: 0.946720	val: 0.766521	test: 0.742384

Epoch: 50
Loss: 0.13515540065206727
train: 0.945974	val: 0.763738	test: 0.750304

Epoch: 51
Loss: 0.13429044039532761
train: 0.948975	val: 0.765885	test: 0.745526

Epoch: 52
Loss: 0.13229823601005986
train: 0.949891	val: 0.773277	test: 0.744093

Epoch: 53
Loss: 0.13330378825513933
train: 0.950709	val: 0.761072	test: 0.739006

Epoch: 54
Loss: 0.13067790031144566
train: 0.952343	val: 0.759699	test: 0.743301

Epoch: 55
Loss: 0.13046758061757927
train: 0.951566	val: 0.761655	test: 0.745893

Epoch: 56
Loss: 0.13297733274707857
train: 0.953858	val: 0.770323	test: 0.754153

Epoch: 57
Loss: 0.13188559189520577
train: 0.954916	val: 0.771001	test: 0.746870

Epoch: 58
Loss: 0.1299695125069302
train: 0.955533	val: 0.768325	test: 0.749507

Epoch: 59
Loss: 0.12866652323048383
train: 0.956078	val: 0.769648	test: 0.749238

Epoch: 60
Loss: 0.12899288328866346
train: 0.956435	val: 0.771931	test: 0.749788

Epoch: 61
Loss: 0.1274043095088983
train: 0.956786	val: 0.779847	test: 0.745378

Epoch: 62
Loss: 0.12746825757719685
train: 0.954215	val: 0.774008	test: 0.747961

Epoch: 63
Loss: 0.1253462530196952
train: 0.957889	val: 0.773093	test: 0.736184

Epoch: 64
Loss: 0.12629710674990188
train: 0.959697	val: 0.768966	test: 0.739558

Epoch: 65
Loss: 0.12560567987397317
train: 0.959746	val: 0.770634	test: 0.733431

Epoch: 66
Loss: 0.12463646591858116
train: 0.962097	val: 0.763989	test: 0.738061

Epoch: 67
Loss: 0.12683763630238146
train: 0.963050	val: 0.768317	test: 0.745754

Epoch: 68
Loss: 0.12471696277694327
train: 0.963761	val: 0.771086	test: 0.731312

Epoch: 69
Loss: 0.12181611132709151
train: 0.964018	val: 0.770535	test: 0.740940

Epoch: 70
Loss: 0.1228743508981898
train: 0.964510	val: 0.766920	test: 0.735612

Epoch: 71
Loss: 0.12032951921079964
train: 0.966847	val: 0.773224	test: 0.742597

Epoch: 72
Loss: 0.12071770420911677
train: 0.967303	val: 0.772193	test: 0.728109

Epoch: 73
Loss: 0.11892085215388642
train: 0.967918	val: 0.765539	test: 0.731887

Epoch: 74
Loss: 0.11740958919220608
train: 0.968556	val: 0.777684	test: 0.725242

Epoch: 75
Loss: 0.11841225170993563
train: 0.968807	val: 0.772661	test: 0.733841

Epoch: 76
Loss: 0.11811989762929281
train: 0.969101	val: 0.771157	test: 0.735304

Epoch: 77
Loss: 0.11762011330215677
train: 0.968239	val: 0.778043	test: 0.741694

Epoch: 78
Loss: 0.11727039945979546
train: 0.971118	val: 0.766468	test: 0.735065

Epoch: 79
Loss: 0.11380245934976645
train: 0.972189	val: 0.778050	test: 0.742074

Epoch: 80
Loss: 0.11426091125651704
train: 0.971394	val: 0.777199	test: 0.741448

Epoch: 81
Loss: 0.11359419021567385
train: 0.972549	val: 0.759488	test: 0.732909

Epoch: 82
Loss: 0.11470384108494645
train: 0.972891	val: 0.764434	test: 0.730841

Epoch: 83
Loss: 0.11429567000266591
train: 0.970958	val: 0.775796	test: 0.728535

Epoch: 84
Loss: 0.11328056665705724
train: 0.973403	val: 0.778141	test: 0.728324

Epoch: 85
Loss: 0.11383467480025093
train: 0.974191	val: 0.770799	test: 0.728896

Epoch: 86
Loss: 0.1117748495725098
train: 0.973870	val: 0.770432	test: 0.737795

Epoch: 87
Loss: 0.11301295357203346
train: 0.974530	val: 0.776314	test: 0.740334

Epoch: 88
Loss: 0.11069119668245143
train: 0.975976	val: 0.775588	test: 0.732386

Epoch: 89
Loss: 0.11119036358642281
train: 0.976060	val: 0.767230	test: 0.729032

Epoch: 90
Loss: 0.1098874791561256
train: 0.976068	val: 0.774035	test: 0.740193

Epoch: 91
Loss: 0.10913814491275374
train: 0.975782	val: 0.761877	test: 0.737443

Epoch: 92
Loss: 0.10750841631187004
train: 0.977675	val: 0.774091	test: 0.731454

Epoch: 93
Loss: 0.10759204780939413
train: 0.977548	val: 0.774058	test: 0.734438

Epoch: 94
Loss: 0.10709593542607194
train: 0.977295	val: 0.757377	test: 0.736626

Epoch: 95
Loss: 0.10852173102688205
train: 0.978991	val: 0.775429	test: 0.728144

Epoch: 96
Loss: 0.10347831425421536
train: 0.979605	val: 0.771449	test: 0.736374

Epoch: 97
Loss: 0.10492425673649804
train: 0.980197	val: 0.772229	test: 0.737215

Epoch: 98
Loss: 0.10606647043346268
train: 0.979406	val: 0.761173	test: 0.734170

Epoch: 99
Loss: 0.10600898363234226
train: 0.980220	val: 0.766823	test: 0.736524

Epoch: 100
Loss: 0.10491386940630548
train: 0.980547	val: 0.760737	test: 0.731925

best train: 0.884586	val: 0.781509	test: 0.738994
end
