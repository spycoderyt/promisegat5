11288244_2
--dataset=bace --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.662895883101496
train: 0.684215	val: 0.501832	test: 0.580421

Epoch: 2
Loss: 0.6077594647039481
train: 0.764600	val: 0.535897	test: 0.708572

Epoch: 3
Loss: 0.5634679667547188
train: 0.807671	val: 0.540293	test: 0.732220

Epoch: 4
Loss: 0.5240994261608749
train: 0.837126	val: 0.601832	test: 0.763519

Epoch: 5
Loss: 0.5022806063368522
train: 0.856284	val: 0.678022	test: 0.794123

Epoch: 6
Loss: 0.4889036863988018
train: 0.864780	val: 0.663736	test: 0.780038

Epoch: 7
Loss: 0.4644874201848694
train: 0.864697	val: 0.655311	test: 0.766823

Epoch: 8
Loss: 0.455653241951343
train: 0.878896	val: 0.682051	test: 0.796731

Epoch: 9
Loss: 0.43913298053843197
train: 0.883687	val: 0.675092	test: 0.791515

Epoch: 10
Loss: 0.4287273037681201
train: 0.894971	val: 0.675458	test: 0.802991

Epoch: 11
Loss: 0.4397973558919753
train: 0.905537	val: 0.672894	test: 0.814989

Epoch: 12
Loss: 0.4245630154621862
train: 0.909115	val: 0.683883	test: 0.816206

Epoch: 13
Loss: 0.4189600445407834
train: 0.909495	val: 0.679487	test: 0.822813

Epoch: 14
Loss: 0.3965423573764934
train: 0.916436	val: 0.688278	test: 0.831681

Epoch: 15
Loss: 0.407871997980296
train: 0.918630	val: 0.676557	test: 0.820901

Epoch: 16
Loss: 0.3899710377593441
train: 0.921147	val: 0.686447	test: 0.810816

Epoch: 17
Loss: 0.3997202435158164
train: 0.924295	val: 0.694872	test: 0.818292

Epoch: 18
Loss: 0.38992361919597074
train: 0.922069	val: 0.671062	test: 0.807164

Epoch: 19
Loss: 0.37488877298495205
train: 0.925243	val: 0.666667	test: 0.806816

Epoch: 20
Loss: 0.3625575125050208
train: 0.927006	val: 0.667033	test: 0.792384

Epoch: 21
Loss: 0.37053773079413255
train: 0.929940	val: 0.669963	test: 0.788211

Epoch: 22
Loss: 0.36585921443135244
train: 0.927240	val: 0.632601	test: 0.791167

Epoch: 23
Loss: 0.3759948501618801
train: 0.933390	val: 0.689377	test: 0.810989

Epoch: 24
Loss: 0.3757295806807084
train: 0.934475	val: 0.697070	test: 0.811685

Epoch: 25
Loss: 0.3625295783063661
train: 0.936484	val: 0.671429	test: 0.825248

Epoch: 26
Loss: 0.35311911957925374
train: 0.937172	val: 0.676557	test: 0.829421

Epoch: 27
Loss: 0.3545095914414836
train: 0.938704	val: 0.684249	test: 0.828030

Epoch: 28
Loss: 0.35425880313641034
train: 0.940679	val: 0.693773	test: 0.813250

Epoch: 29
Loss: 0.35192744614098687
train: 0.941313	val: 0.692308	test: 0.798296

Epoch: 30
Loss: 0.35023062301456953
train: 0.942885	val: 0.691941	test: 0.802817

Epoch: 31
Loss: 0.34421719316430294
train: 0.943382	val: 0.663370	test: 0.788385

Epoch: 32
Loss: 0.34366913847759195
train: 0.943330	val: 0.685714	test: 0.805251

Epoch: 33
Loss: 0.3313416006127451
train: 0.945325	val: 0.672527	test: 0.822640

Epoch: 34
Loss: 0.33847768576595605
train: 0.945765	val: 0.648718	test: 0.790645

Epoch: 35
Loss: 0.3259424782975125
train: 0.949469	val: 0.653114	test: 0.787341

Epoch: 36
Loss: 0.3379276443174314
train: 0.948807	val: 0.655311	test: 0.785081

Epoch: 37
Loss: 0.3393210763792423
train: 0.947934	val: 0.653480	test: 0.790123

Epoch: 38
Loss: 0.3338105361544576
train: 0.949118	val: 0.665568	test: 0.807338

Epoch: 39
Loss: 0.3208731201737607
train: 0.949361	val: 0.648352	test: 0.809772

Epoch: 40
Loss: 0.33374320123893125
train: 0.943910	val: 0.672161	test: 0.775343

Epoch: 41
Loss: 0.3261206889136431
train: 0.950848	val: 0.674725	test: 0.801600

Epoch: 42
Loss: 0.3211968588911388
train: 0.950108	val: 0.678388	test: 0.788559

Epoch: 43
Loss: 0.3232167671671215
train: 0.953185	val: 0.679121	test: 0.800904

Epoch: 44
Loss: 0.3198191949123764
train: 0.954124	val: 0.673993	test: 0.793079

Epoch: 45
Loss: 0.3116087962849019
train: 0.954309	val: 0.654212	test: 0.777082

Epoch: 46
Loss: 0.3426334390407507
train: 0.957854	val: 0.655678	test: 0.792732

Epoch: 47
Loss: 0.3219400146852459
train: 0.958856	val: 0.647619	test: 0.800904

Epoch: 48
Loss: 0.309759441750706
train: 0.957745	val: 0.659707	test: 0.795514

Epoch: 49
Loss: 0.3146249623886975
train: 0.958111	val: 0.662637	test: 0.780734

Epoch: 50
Loss: 0.3082679824162683
train: 0.956376	val: 0.671795	test: 0.793949

Epoch: 51
Loss: 0.29448888159242054
train: 0.961450	val: 0.665934	test: 0.804034

Epoch: 52
Loss: 0.3040917199401966
train: 0.960728	val: 0.652747	test: 0.788559

Epoch: 53
Loss: 0.3040643290423975
train: 0.962477	val: 0.647619	test: 0.796035

Epoch: 54
Loss: 0.30573947692598924
train: 0.962560	val: 0.655311	test: 0.792732

Epoch: 55
Loss: 0.294015221453194
train: 0.962574	val: 0.649451	test: 0.780386

Epoch: 56
Loss: 0.3028632640215192
train: 0.965331	val: 0.660806	test: 0.792906

Epoch: 57
Loss: 0.2935367208547703
train: 0.964937	val: 0.651282	test: 0.798818

Epoch: 58
Loss: 0.2950782455613279
train: 0.963011	val: 0.644689	test: 0.799165

Epoch: 59
Loss: 0.2977174751669568
train: 0.967754	val: 0.627106	test: 0.795340

Epoch: 60
Loss: 0.2854908731201532
train: 0.964866	val: 0.649451	test: 0.780908

Epoch: 61
Loss: 0.2885848975671291
train: 0.965930	val: 0.676557	test: 0.793079

Epoch: 62
Loss: 0.2730931735240566
train: 0.967683	val: 0.675458	test: 0.788906

Epoch: 63
Loss: 0.2790468391906823
train: 0.967908	val: 0.657875	test: 0.779690

Epoch: 64
Loss: 0.28228227472418144
train: 0.967765	val: 0.671429	test: 0.765084

Epoch: 65
Loss: 0.27725525270062956
train: 0.969275	val: 0.682784	test: 0.776561

Epoch: 66
Loss: 0.2677408058113638
train: 0.967280	val: 0.689011	test: 0.784733

Epoch: 67
Loss: 0.27285182604853475
train: 0.970568	val: 0.662271	test: 0.784733

Epoch: 68
Loss: 0.28226115801617685
train: 0.965037	val: 0.665201	test: 0.778299

Epoch: 69
Loss: 0.29545742163845484
train: 0.972240	val: 0.655311	test: 0.795862

Epoch: 70
Loss: 0.2772494232060842
train: 0.972917	val: 0.663370	test: 0.800209

Epoch: 71
Loss: 0.2958804435737614
train: 0.973413	val: 0.653846	test: 0.790123

Epoch: 72
Loss: 0.273109000097904
train: 0.971844	val: 0.644322	test: 0.787167

Epoch: 73
Loss: 0.26676426881647786
train: 0.972009	val: 0.647985	test: 0.786646

Epoch: 74
Loss: 0.27551182695623355
train: 0.972440	val: 0.649817	test: 0.763172

Epoch: 75
Loss: 0.2652449125796428
train: 0.963385	val: 0.650916	test: 0.735524

Epoch: 76
Loss: 0.28191096846295877
train: 0.973530	val: 0.670330	test: 0.784385

Epoch: 77
Loss: 0.25295574615899097
train: 0.974384	val: 0.667033	test: 0.800730

Epoch: 78
Loss: 0.2604516545586014
train: 0.976204	val: 0.667033	test: 0.795514

Epoch: 79
Loss: 0.2808266157726663
train: 0.976033	val: 0.678388	test: 0.789602

Epoch: 80
Loss: 0.26554014166516987
train: 0.971889	val: 0.683516	test: 0.778473

Epoch: 81
Loss: 0.27758771551538686
train: 0.970765	val: 0.682784	test: 0.779343

Epoch: 82
Loss: 0.2576315158960895
train: 0.974509	val: 0.678388	test: 0.784907

Epoch: 83
Loss: 0.26071128920961495
train: 0.970442	val: 0.643956	test: 0.741784

Epoch: 84
Loss: 0.26526707996682236
train: 0.967626	val: 0.644689	test: 0.743175

Epoch: 85
Loss: 0.26248901943086933
train: 0.970933	val: 0.635897	test: 0.750478

Epoch: 86
Loss: 0.2600697754140435
train: 0.976678	val: 0.634432	test: 0.782299

Epoch: 87
Loss: 0.23744869839255847
train: 0.978933	val: 0.644689	test: 0.792036

Epoch: 88
Loss: 0.22815660383349373
train: 0.975702	val: 0.638095	test: 0.763346

Epoch: 89
Loss: 0.24947964231074976
train: 0.964001	val: 0.641758	test: 0.714137

Epoch: 90
Loss: 0.25367738416935726
train: 0.972306	val: 0.661538	test: 0.726482

Epoch: 91
Loss: 0.2542424193200824
train: 0.978682	val: 0.656410	test: 0.758651

Epoch: 92
Loss: 0.265846894053766
train: 0.980651	val: 0.654945	test: 0.778473

Epoch: 93
Loss: 0.2373347076038354
train: 0.981378	val: 0.658242	test: 0.783516

Epoch: 94
Loss: 0.246239455011577
train: 0.981318	val: 0.646154	test: 0.773778

Epoch: 95
Loss: 0.22981487909562665
train: 0.978610	val: 0.660440	test: 0.768562

Epoch: 96
Loss: 0.2510182687120327
train: 0.979994	val: 0.659341	test: 0.774996

Epoch: 97
Loss: 0.2406061104467751
train: 0.981358	val: 0.667399	test: 0.768040

Epoch: 98
Loss: 0.23199205219085503
train: 0.981224	val: 0.658242	test: 0.771692

Epoch: 99
Loss: 0.22872493574812364
train: 0.982329	val: 0.656777	test: 0.760563

Epoch: 100
Loss: 0.23668810642147645
train: 0.979846	val: 0.663370	test: 0.748565

best train: 0.934475	val: 0.697070	test: 0.811685
end
