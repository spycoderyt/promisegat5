11288244_2
--dataset=tox21 --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5272198843190213
train: 0.691578	val: 0.613533	test: 0.574636

Epoch: 2
Loss: 0.3357241954336393
train: 0.763199	val: 0.712509	test: 0.673485

Epoch: 3
Loss: 0.24142021321256632
train: 0.790402	val: 0.716488	test: 0.672640

Epoch: 4
Loss: 0.20737153602589814
train: 0.808552	val: 0.749462	test: 0.714807

Epoch: 5
Loss: 0.1965227368727943
train: 0.828607	val: 0.755144	test: 0.718527

Epoch: 6
Loss: 0.19103157593874115
train: 0.837942	val: 0.760159	test: 0.720684

Epoch: 7
Loss: 0.18700731831281148
train: 0.849481	val: 0.758549	test: 0.713049

Epoch: 8
Loss: 0.18509157458289155
train: 0.851250	val: 0.762640	test: 0.726324

Epoch: 9
Loss: 0.18332958026625662
train: 0.858251	val: 0.750758	test: 0.708812

Epoch: 10
Loss: 0.1796064849667326
train: 0.865629	val: 0.767986	test: 0.730955

Epoch: 11
Loss: 0.1768598737514388
train: 0.864708	val: 0.764334	test: 0.730821

Epoch: 12
Loss: 0.1787599190699905
train: 0.866525	val: 0.767456	test: 0.723225

Epoch: 13
Loss: 0.17367995592707575
train: 0.869923	val: 0.767395	test: 0.726950

Epoch: 14
Loss: 0.17207133947276085
train: 0.875855	val: 0.767881	test: 0.729068

Epoch: 15
Loss: 0.1710327255832111
train: 0.879116	val: 0.776175	test: 0.735326

Epoch: 16
Loss: 0.16935179828776767
train: 0.880841	val: 0.775632	test: 0.728839

Epoch: 17
Loss: 0.1677339320326825
train: 0.878376	val: 0.768666	test: 0.729474

Epoch: 18
Loss: 0.16696151128826867
train: 0.885594	val: 0.778582	test: 0.735984

Epoch: 19
Loss: 0.16637554976146737
train: 0.890904	val: 0.777982	test: 0.742603

Epoch: 20
Loss: 0.16444077882392436
train: 0.892049	val: 0.778911	test: 0.739640

Epoch: 21
Loss: 0.16422128467520253
train: 0.893449	val: 0.781242	test: 0.747970

Epoch: 22
Loss: 0.1636594006011483
train: 0.898590	val: 0.772653	test: 0.741219

Epoch: 23
Loss: 0.1599301159345052
train: 0.899293	val: 0.774921	test: 0.745272

Epoch: 24
Loss: 0.1611604636116578
train: 0.902243	val: 0.775264	test: 0.740749

Epoch: 25
Loss: 0.16144198931251266
train: 0.900022	val: 0.783405	test: 0.746076

Epoch: 26
Loss: 0.16273180636284656
train: 0.904366	val: 0.784657	test: 0.746529

Epoch: 27
Loss: 0.1602895002410255
train: 0.903115	val: 0.782528	test: 0.752637

Epoch: 28
Loss: 0.15839398328468068
train: 0.904073	val: 0.782096	test: 0.751418

Epoch: 29
Loss: 0.15822652646773686
train: 0.897671	val: 0.787513	test: 0.747955

Epoch: 30
Loss: 0.15643308906663347
train: 0.908558	val: 0.778522	test: 0.741532

Epoch: 31
Loss: 0.15542122255941723
train: 0.912083	val: 0.773342	test: 0.756853

Epoch: 32
Loss: 0.15327084581334613
train: 0.911839	val: 0.768169	test: 0.745920

Epoch: 33
Loss: 0.15436678911784513
train: 0.915195	val: 0.774864	test: 0.747134

Epoch: 34
Loss: 0.15526551372899583
train: 0.915583	val: 0.788920	test: 0.761982

Epoch: 35
Loss: 0.15221980493572376
train: 0.917977	val: 0.782380	test: 0.753339

Epoch: 36
Loss: 0.15083480305728972
train: 0.919708	val: 0.785198	test: 0.750345

Epoch: 37
Loss: 0.14868720963232354
train: 0.921828	val: 0.784913	test: 0.761039

Epoch: 38
Loss: 0.14913629253573413
train: 0.924013	val: 0.784975	test: 0.746881

Epoch: 39
Loss: 0.14811111901661694
train: 0.924548	val: 0.789282	test: 0.764684

Epoch: 40
Loss: 0.1500660945734587
train: 0.924769	val: 0.775747	test: 0.754129

Epoch: 41
Loss: 0.14592168350313403
train: 0.929079	val: 0.777829	test: 0.747322

Epoch: 42
Loss: 0.14505964244205324
train: 0.926556	val: 0.779544	test: 0.762681

Epoch: 43
Loss: 0.14537468519721666
train: 0.929042	val: 0.792290	test: 0.749801

Epoch: 44
Loss: 0.14511039396967862
train: 0.932064	val: 0.777187	test: 0.748493

Epoch: 45
Loss: 0.1441459752351516
train: 0.932752	val: 0.771415	test: 0.747110

Epoch: 46
Loss: 0.1427338427679069
train: 0.934915	val: 0.786259	test: 0.751207

Epoch: 47
Loss: 0.14159903330303147
train: 0.936167	val: 0.787950	test: 0.756834

Epoch: 48
Loss: 0.14115103306147356
train: 0.937952	val: 0.783257	test: 0.757856

Epoch: 49
Loss: 0.14033975478721708
train: 0.938396	val: 0.788076	test: 0.746461

Epoch: 50
Loss: 0.14075713876855517
train: 0.939203	val: 0.783068	test: 0.750184

Epoch: 51
Loss: 0.13960453913312054
train: 0.939709	val: 0.772918	test: 0.741730

Epoch: 52
Loss: 0.13898144008044366
train: 0.941521	val: 0.794572	test: 0.749859

Epoch: 53
Loss: 0.13861381627496566
train: 0.938130	val: 0.773016	test: 0.744397

Epoch: 54
Loss: 0.14048161663681571
train: 0.943241	val: 0.780154	test: 0.750534

Epoch: 55
Loss: 0.13902509471504074
train: 0.945097	val: 0.781465	test: 0.751923

Epoch: 56
Loss: 0.13539190151181166
train: 0.946483	val: 0.784440	test: 0.743982

Epoch: 57
Loss: 0.13504813566397653
train: 0.945604	val: 0.767978	test: 0.749398

Epoch: 58
Loss: 0.13487952715216672
train: 0.948139	val: 0.783046	test: 0.755397

Epoch: 59
Loss: 0.13347242736480752
train: 0.947463	val: 0.770465	test: 0.743132

Epoch: 60
Loss: 0.13208615981563296
train: 0.950222	val: 0.780024	test: 0.740816

Epoch: 61
Loss: 0.13406903357656427
train: 0.951462	val: 0.778881	test: 0.747075

Epoch: 62
Loss: 0.13268031454946788
train: 0.951160	val: 0.783368	test: 0.749063

Epoch: 63
Loss: 0.1307532181089291
train: 0.953172	val: 0.782630	test: 0.753218

Epoch: 64
Loss: 0.13090456234881398
train: 0.950920	val: 0.785029	test: 0.738335

Epoch: 65
Loss: 0.13131876908003629
train: 0.953528	val: 0.782125	test: 0.743006

Epoch: 66
Loss: 0.13095060065636832
train: 0.956564	val: 0.776917	test: 0.738705

Epoch: 67
Loss: 0.12754373129049676
train: 0.958652	val: 0.790462	test: 0.749919

Epoch: 68
Loss: 0.12771948709326683
train: 0.956896	val: 0.782454	test: 0.748128

Epoch: 69
Loss: 0.1264612434678999
train: 0.959276	val: 0.783078	test: 0.744007

Epoch: 70
Loss: 0.12588707468554483
train: 0.957810	val: 0.775902	test: 0.755027

Epoch: 71
Loss: 0.12694291516481843
train: 0.959805	val: 0.782517	test: 0.745479

Epoch: 72
Loss: 0.12301964953416791
train: 0.959131	val: 0.791225	test: 0.752906

Epoch: 73
Loss: 0.12477102111313008
train: 0.962575	val: 0.780186	test: 0.742965

Epoch: 74
Loss: 0.12350798816194251
train: 0.962272	val: 0.772912	test: 0.742915

Epoch: 75
Loss: 0.12452276877622646
train: 0.963629	val: 0.778213	test: 0.748811

Epoch: 76
Loss: 0.12258182773375548
train: 0.963200	val: 0.776969	test: 0.742741

Epoch: 77
Loss: 0.12311971240469
train: 0.964913	val: 0.778439	test: 0.744809

Epoch: 78
Loss: 0.12112223587270782
train: 0.964686	val: 0.780958	test: 0.757731

Epoch: 79
Loss: 0.11913778161361321
train: 0.966186	val: 0.779842	test: 0.739042

Epoch: 80
Loss: 0.1201496310531335
train: 0.964536	val: 0.782165	test: 0.745675

Epoch: 81
Loss: 0.12103038378822596
train: 0.965758	val: 0.776656	test: 0.743665

Epoch: 82
Loss: 0.12023696035676784
train: 0.967165	val: 0.785234	test: 0.743267

Epoch: 83
Loss: 0.11940026365720512
train: 0.968130	val: 0.780859	test: 0.750167

Epoch: 84
Loss: 0.11632417934407513
train: 0.968401	val: 0.779018	test: 0.745184

Epoch: 85
Loss: 0.11763113957878608
train: 0.968729	val: 0.778652	test: 0.747269

Epoch: 86
Loss: 0.11644807199089822
train: 0.970006	val: 0.778473	test: 0.742945

Epoch: 87
Loss: 0.11645317289935175
train: 0.970814	val: 0.779948	test: 0.743970

Epoch: 88
Loss: 0.11515768400513655
train: 0.970401	val: 0.776509	test: 0.741686

Epoch: 89
Loss: 0.11767856474572723
train: 0.971188	val: 0.787484	test: 0.741291

Epoch: 90
Loss: 0.11436486282401419
train: 0.970899	val: 0.787422	test: 0.742085

Epoch: 91
Loss: 0.11408098886948279
train: 0.972901	val: 0.769633	test: 0.745819

Epoch: 92
Loss: 0.1117711997808495
train: 0.972691	val: 0.777331	test: 0.740131

Epoch: 93
Loss: 0.1129388541590284
train: 0.972651	val: 0.768514	test: 0.748714

Epoch: 94
Loss: 0.1124790060514335
train: 0.972713	val: 0.778778	test: 0.748593

Epoch: 95
Loss: 0.11092283335529075
train: 0.974685	val: 0.781447	test: 0.744531

Epoch: 96
Loss: 0.11097561768380748
train: 0.973372	val: 0.775807	test: 0.745694

Epoch: 97
Loss: 0.11043779982852006
train: 0.974440	val: 0.787725	test: 0.739346

Epoch: 98
Loss: 0.10994171194024291
train: 0.975087	val: 0.775502	test: 0.740873

Epoch: 99
Loss: 0.10751728840321223
train: 0.976773	val: 0.784093	test: 0.743572

Epoch: 100
Loss: 0.10915851078680888
train: 0.974002	val: 0.772814	test: 0.747872

best train: 0.941521	val: 0.794572	test: 0.749859
end
