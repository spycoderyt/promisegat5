11288244_2
--dataset=clintox --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='clintox', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6470601744733191
train: 0.645722	val: 0.709761	test: 0.387853

Epoch: 2
Loss: 0.564481829479958
train: 0.723161	val: 0.794869	test: 0.480782

Epoch: 3
Loss: 0.5044494193942202
train: 0.769881	val: 0.849759	test: 0.563909

Epoch: 4
Loss: 0.4540065198598822
train: 0.779683	val: 0.827957	test: 0.543292

Epoch: 5
Loss: 0.41115702913096
train: 0.810249	val: 0.862308	test: 0.571650

Epoch: 6
Loss: 0.37524775952196865
train: 0.808105	val: 0.870088	test: 0.562112

Epoch: 7
Loss: 0.3483902925245192
train: 0.823316	val: 0.891864	test: 0.578101

Epoch: 8
Loss: 0.31568693146192356
train: 0.838233	val: 0.879565	test: 0.588926

Epoch: 9
Loss: 0.2916486149599945
train: 0.845136	val: 0.871261	test: 0.607289

Epoch: 10
Loss: 0.27637631880484204
train: 0.845428	val: 0.856390	test: 0.604851

Epoch: 11
Loss: 0.2586908532987674
train: 0.869495	val: 0.848135	test: 0.622448

Epoch: 12
Loss: 0.2528904199351822
train: 0.867940	val: 0.869638	test: 0.614791

Epoch: 13
Loss: 0.23740365672427788
train: 0.875114	val: 0.875619	test: 0.637864

Epoch: 14
Loss: 0.22830118307606875
train: 0.867940	val: 0.895635	test: 0.638062

Epoch: 15
Loss: 0.2200745362745696
train: 0.888408	val: 0.885096	test: 0.663402

Epoch: 16
Loss: 0.20484695944728282
train: 0.900812	val: 0.854815	test: 0.669623

Epoch: 17
Loss: 0.2061502716800459
train: 0.907415	val: 0.841367	test: 0.671366

Epoch: 18
Loss: 0.20352321803629203
train: 0.904398	val: 0.859349	test: 0.677286

Epoch: 19
Loss: 0.1948060521301313
train: 0.916208	val: 0.876455	test: 0.679673

Epoch: 20
Loss: 0.19706404684152917
train: 0.924699	val: 0.841068	test: 0.681384

Epoch: 21
Loss: 0.19329436543557146
train: 0.921591	val: 0.808827	test: 0.669892

Epoch: 22
Loss: 0.18116890853789475
train: 0.927364	val: 0.831703	test: 0.672645

Epoch: 23
Loss: 0.17413330110492944
train: 0.939335	val: 0.793305	test: 0.682644

Epoch: 24
Loss: 0.1700688512068343
train: 0.941715	val: 0.786400	test: 0.711387

Epoch: 25
Loss: 0.17493396387027488
train: 0.945010	val: 0.827395	test: 0.717203

Epoch: 26
Loss: 0.17509258430711497
train: 0.942149	val: 0.862845	test: 0.720294

Epoch: 27
Loss: 0.1683666843393083
train: 0.944045	val: 0.851220	test: 0.733041

Epoch: 28
Loss: 0.1739918419133837
train: 0.948163	val: 0.839895	test: 0.725966

Epoch: 29
Loss: 0.176972858256535
train: 0.938985	val: 0.826133	test: 0.745354

Epoch: 30
Loss: 0.16476553651912101
train: 0.945143	val: 0.886172	test: 0.686391

Epoch: 31
Loss: 0.1654599726185279
train: 0.954417	val: 0.873398	test: 0.711513

Epoch: 32
Loss: 0.16088449229517204
train: 0.956253	val: 0.830617	test: 0.659926

Epoch: 33
Loss: 0.16358112464973507
train: 0.959813	val: 0.760641	test: 0.672737

Epoch: 34
Loss: 0.15958526635963147
train: 0.957205	val: 0.795505	test: 0.669026

Epoch: 35
Loss: 0.16333773304745644
train: 0.960233	val: 0.820940	test: 0.687252

Epoch: 36
Loss: 0.15473958826740397
train: 0.963447	val: 0.779083	test: 0.719357

Epoch: 37
Loss: 0.16080432182658477
train: 0.964152	val: 0.781255	test: 0.745009

Epoch: 38
Loss: 0.16507187545615892
train: 0.963526	val: 0.790258	test: 0.733710

Epoch: 39
Loss: 0.14709541654429578
train: 0.967340	val: 0.790184	test: 0.714491

Epoch: 40
Loss: 0.1495752983905839
train: 0.967621	val: 0.774663	test: 0.727214

Epoch: 41
Loss: 0.14624434272099046
train: 0.968246	val: 0.798625	test: 0.733715

Epoch: 42
Loss: 0.14543815241783592
train: 0.971758	val: 0.810900	test: 0.743677

Epoch: 43
Loss: 0.1378678972200331
train: 0.973524	val: 0.829967	test: 0.773664

Epoch: 44
Loss: 0.14801187652880807
train: 0.968960	val: 0.839058	test: 0.738642

Epoch: 45
Loss: 0.14964472857632422
train: 0.973154	val: 0.844365	test: 0.770571

Epoch: 46
Loss: 0.14345429974761936
train: 0.967714	val: 0.825722	test: 0.734518

Epoch: 47
Loss: 0.14236667601417907
train: 0.971370	val: 0.836261	test: 0.740133

Epoch: 48
Loss: 0.13904941621725847
train: 0.970171	val: 0.858351	test: 0.772581

Epoch: 49
Loss: 0.14980260618188504
train: 0.972231	val: 0.837347	test: 0.795504

Epoch: 50
Loss: 0.12947678756143838
train: 0.974278	val: 0.840007	test: 0.773300

Epoch: 51
Loss: 0.13930431010204822
train: 0.973919	val: 0.840046	test: 0.755199

Epoch: 52
Loss: 0.12887819831712072
train: 0.970651	val: 0.829106	test: 0.771319

Epoch: 53
Loss: 0.12839322590255978
train: 0.974491	val: 0.818293	test: 0.752999

Epoch: 54
Loss: 0.12625523299448332
train: 0.975211	val: 0.818705	test: 0.789457

Epoch: 55
Loss: 0.12246039766253072
train: 0.975844	val: 0.791471	test: 0.778346

Epoch: 56
Loss: 0.13405783864513962
train: 0.976302	val: 0.775499	test: 0.795525

Epoch: 57
Loss: 0.13607778047651142
train: 0.977633	val: 0.815008	test: 0.776619

Epoch: 58
Loss: 0.1364583512809019
train: 0.979555	val: 0.815820	test: 0.758193

Epoch: 59
Loss: 0.14130873118105214
train: 0.978308	val: 0.818279	test: 0.771671

Epoch: 60
Loss: 0.11995845982811873
train: 0.977718	val: 0.830280	test: 0.740728

Epoch: 61
Loss: 0.133275245854701
train: 0.975717	val: 0.823062	test: 0.770502

Epoch: 62
Loss: 0.128823738239003
train: 0.976701	val: 0.756881	test: 0.805035

Epoch: 63
Loss: 0.13343085569595226
train: 0.979948	val: 0.780433	test: 0.743113

Epoch: 64
Loss: 0.13614685247812025
train: 0.976599	val: 0.820103	test: 0.738454

Epoch: 65
Loss: 0.12711407542096737
train: 0.978615	val: 0.806592	test: 0.794705

Epoch: 66
Loss: 0.1373222122268349
train: 0.981267	val: 0.806342	test: 0.773294

Epoch: 67
Loss: 0.13083076765464843
train: 0.981410	val: 0.809389	test: 0.765099

Epoch: 68
Loss: 0.11606053589301228
train: 0.982765	val: 0.811462	test: 0.785662

Epoch: 69
Loss: 0.11877179789761896
train: 0.981766	val: 0.813061	test: 0.770934

Epoch: 70
Loss: 0.11189916941250033
train: 0.980658	val: 0.785303	test: 0.777254

Epoch: 71
Loss: 0.12138442665796108
train: 0.983850	val: 0.797603	test: 0.799688

Epoch: 72
Loss: 0.12473940598830727
train: 0.984019	val: 0.797402	test: 0.799976

Epoch: 73
Loss: 0.13698620043376836
train: 0.983789	val: 0.794268	test: 0.795329

Epoch: 74
Loss: 0.1224852237977285
train: 0.979045	val: 0.812885	test: 0.792313

Epoch: 75
Loss: 0.11201678186135629
train: 0.981971	val: 0.836099	test: 0.766816

Epoch: 76
Loss: 0.11749966734985243
train: 0.982774	val: 0.823937	test: 0.777590

Epoch: 77
Loss: 0.12352066320089068
train: 0.983276	val: 0.785490	test: 0.796971

Epoch: 78
Loss: 0.12979728515703065
train: 0.983061	val: 0.760241	test: 0.771382

Epoch: 79
Loss: 0.11356968672170482
train: 0.983588	val: 0.768520	test: 0.781101

Epoch: 80
Loss: 0.12173846261637378
train: 0.983963	val: 0.810826	test: 0.766037

Epoch: 81
Loss: 0.11583351956900219
train: 0.981385	val: 0.839733	test: 0.769634

Epoch: 82
Loss: 0.12297385920211638
train: 0.981429	val: 0.825448	test: 0.780883

Epoch: 83
Loss: 0.11378996696459179
train: 0.983328	val: 0.826035	test: 0.810744

Epoch: 84
Loss: 0.12084962375611419
train: 0.980931	val: 0.810401	test: 0.796166

Epoch: 85
Loss: 0.11180241029986977
train: 0.985407	val: 0.794356	test: 0.792692

Epoch: 86
Loss: 0.11338742169330115
train: 0.985138	val: 0.765523	test: 0.773812

Epoch: 87
Loss: 0.11559298286387978
train: 0.984786	val: 0.752324	test: 0.764356

Epoch: 88
Loss: 0.10561038370715532
train: 0.984538	val: 0.773738	test: 0.780728

Epoch: 89
Loss: 0.11114588302468509
train: 0.984587	val: 0.803608	test: 0.796778

Epoch: 90
Loss: 0.10827084320508415
train: 0.985121	val: 0.828158	test: 0.812298

Epoch: 91
Loss: 0.1075002530530009
train: 0.985318	val: 0.794967	test: 0.799912

Epoch: 92
Loss: 0.10486085468995827
train: 0.985689	val: 0.778785	test: 0.795116

Epoch: 93
Loss: 0.10894839080555763
train: 0.986469	val: 0.747330	test: 0.819112

Epoch: 94
Loss: 0.1072405619638821
train: 0.986023	val: 0.757732	test: 0.832678

Epoch: 95
Loss: 0.09486101414794554
train: 0.986573	val: 0.773327	test: 0.814129

Epoch: 96
Loss: 0.10057428836938571
train: 0.984357	val: 0.734180	test: 0.780489

Epoch: 97
Loss: 0.1129937856972312
train: 0.984702	val: 0.718609	test: 0.784037

Epoch: 98
Loss: 0.11571834176711797
train: 0.986485	val: 0.749115	test: 0.787646

Epoch: 99
Loss: 0.10568002300271293
train: 0.985996	val: 0.758480	test: 0.780463

Epoch: 100
Loss: 0.1019930031843577
train: 0.985909	val: 0.764598	test: 0.786202

best train: 0.867940	val: 0.895635	test: 0.638062
end
