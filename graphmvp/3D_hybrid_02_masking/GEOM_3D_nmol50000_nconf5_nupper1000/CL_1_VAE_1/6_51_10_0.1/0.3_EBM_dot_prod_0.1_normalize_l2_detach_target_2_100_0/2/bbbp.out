11288244_2
--dataset=bbbp --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bbbp', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bbbp
Data: Data(edge_attr=[105842, 2], edge_index=[2, 105842], id=[2039], x=[49068, 2], y=[2039])
MoleculeDataset(2039)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6461968676664627
train: 0.816128	val: 0.904748	test: 0.628858

Epoch: 2
Loss: 0.5123720381832804
train: 0.862185	val: 0.904346	test: 0.633970

Epoch: 3
Loss: 0.43185321502395757
train: 0.885858	val: 0.899629	test: 0.625772

Epoch: 4
Loss: 0.36536314531281816
train: 0.891883	val: 0.891699	test: 0.649595

Epoch: 5
Loss: 0.31902890778334364
train: 0.909656	val: 0.905952	test: 0.665413

Epoch: 6
Loss: 0.2869772145466042
train: 0.930036	val: 0.916993	test: 0.675733

Epoch: 7
Loss: 0.27235164513512544
train: 0.931717	val: 0.919603	test: 0.679012

Epoch: 8
Loss: 0.27386640676929586
train: 0.942117	val: 0.923015	test: 0.683063

Epoch: 9
Loss: 0.2547632381549748
train: 0.941090	val: 0.924420	test: 0.680073

Epoch: 10
Loss: 0.23683707761805017
train: 0.946493	val: 0.938773	test: 0.690876

Epoch: 11
Loss: 0.23649388505083171
train: 0.947527	val: 0.932049	test: 0.682099

Epoch: 12
Loss: 0.21586207140122418
train: 0.951118	val: 0.931647	test: 0.686246

Epoch: 13
Loss: 0.2180706936571893
train: 0.949551	val: 0.925926	test: 0.683160

Epoch: 14
Loss: 0.21970429260836805
train: 0.958244	val: 0.935662	test: 0.680845

Epoch: 15
Loss: 0.20854661363771587
train: 0.955099	val: 0.925625	test: 0.677083

Epoch: 16
Loss: 0.20147470760732558
train: 0.957424	val: 0.938573	test: 0.679495

Epoch: 17
Loss: 0.19807069520861637
train: 0.958878	val: 0.934558	test: 0.695505

Epoch: 18
Loss: 0.2030485272852421
train: 0.961168	val: 0.932450	test: 0.705922

Epoch: 19
Loss: 0.19026495199039667
train: 0.966500	val: 0.933353	test: 0.695988

Epoch: 20
Loss: 0.17660489472494972
train: 0.964937	val: 0.929238	test: 0.698110

Epoch: 21
Loss: 0.1769993590908996
train: 0.965989	val: 0.925524	test: 0.702257

Epoch: 22
Loss: 0.17649862810959974
train: 0.970538	val: 0.927833	test: 0.696759

Epoch: 23
Loss: 0.18272779023197105
train: 0.973909	val: 0.926829	test: 0.676890

Epoch: 24
Loss: 0.16798554894923415
train: 0.975267	val: 0.932852	test: 0.681520

Epoch: 25
Loss: 0.16785123269577953
train: 0.973700	val: 0.932049	test: 0.692323

Epoch: 26
Loss: 0.16762133117060832
train: 0.974783	val: 0.929138	test: 0.707465

Epoch: 27
Loss: 0.1703231922871947
train: 0.979277	val: 0.931647	test: 0.706597

Epoch: 28
Loss: 0.15547059127952328
train: 0.979368	val: 0.931246	test: 0.714024

Epoch: 29
Loss: 0.17276887671919755
train: 0.982102	val: 0.930443	test: 0.704475

Epoch: 30
Loss: 0.14967210297187197
train: 0.980932	val: 0.933956	test: 0.696663

Epoch: 31
Loss: 0.15548024393202242
train: 0.985101	val: 0.931045	test: 0.682774

Epoch: 32
Loss: 0.14590343120156038
train: 0.977385	val: 0.928335	test: 0.674576

Epoch: 33
Loss: 0.15051839988910784
train: 0.981528	val: 0.935461	test: 0.712384

Epoch: 34
Loss: 0.1523372786871581
train: 0.984868	val: 0.929640	test: 0.698110

Epoch: 35
Loss: 0.16133026085809252
train: 0.986331	val: 0.926428	test: 0.694444

Epoch: 36
Loss: 0.14843225408029806
train: 0.986891	val: 0.930944	test: 0.702546

Epoch: 37
Loss: 0.1414015825914925
train: 0.982952	val: 0.928435	test: 0.709105

Epoch: 38
Loss: 0.15400538594518312
train: 0.986725	val: 0.930744	test: 0.706501

Epoch: 39
Loss: 0.1482910361202349
train: 0.986113	val: 0.921008	test: 0.706790

Epoch: 40
Loss: 0.14341282758327828
train: 0.986170	val: 0.926829	test: 0.701871

Epoch: 41
Loss: 0.13005169165380312
train: 0.984430	val: 0.930844	test: 0.706308

Epoch: 42
Loss: 0.15141410100937644
train: 0.988679	val: 0.930844	test: 0.702643

Epoch: 43
Loss: 0.13311034876277475
train: 0.989481	val: 0.913380	test: 0.696566

Epoch: 44
Loss: 0.14564378246452722
train: 0.990239	val: 0.919703	test: 0.705054

Epoch: 45
Loss: 0.10711116636955743
train: 0.990775	val: 0.929037	test: 0.703029

Epoch: 46
Loss: 0.12801273342236472
train: 0.991165	val: 0.934959	test: 0.702836

Epoch: 47
Loss: 0.13345658692936044
train: 0.992132	val: 0.925023	test: 0.695698

Epoch: 48
Loss: 0.12654751142581466
train: 0.988875	val: 0.914985	test: 0.683931

Epoch: 49
Loss: 0.13636721061642304
train: 0.989462	val: 0.933454	test: 0.697531

Epoch: 50
Loss: 0.13592354440300042
train: 0.990427	val: 0.914885	test: 0.675058

Epoch: 51
Loss: 0.12878744887809862
train: 0.991936	val: 0.913881	test: 0.692805

Epoch: 52
Loss: 0.1310045405999652
train: 0.992456	val: 0.914283	test: 0.691840

Epoch: 53
Loss: 0.1256629277388349
train: 0.993691	val: 0.918699	test: 0.691069

Epoch: 54
Loss: 0.10536272545875387
train: 0.992080	val: 0.925926	test: 0.688368

Epoch: 55
Loss: 0.1238839940588572
train: 0.993138	val: 0.923918	test: 0.691358

Epoch: 56
Loss: 0.11382525415086553
train: 0.994388	val: 0.919301	test: 0.697145

Epoch: 57
Loss: 0.13010767581244953
train: 0.993680	val: 0.910870	test: 0.690104

Epoch: 58
Loss: 0.13130131502264905
train: 0.993586	val: 0.911272	test: 0.695216

Epoch: 59
Loss: 0.12129502697084885
train: 0.993642	val: 0.915588	test: 0.683256

Epoch: 60
Loss: 0.11908065068152662
train: 0.994072	val: 0.919904	test: 0.693962

Epoch: 61
Loss: 0.11409409023586008
train: 0.993159	val: 0.896818	test: 0.684799

Epoch: 62
Loss: 0.10062070225662657
train: 0.993262	val: 0.905149	test: 0.668306

Epoch: 63
Loss: 0.11232032229701824
train: 0.995607	val: 0.919904	test: 0.697338

Epoch: 64
Loss: 0.10907810086104155
train: 0.993005	val: 0.906956	test: 0.681713

Epoch: 65
Loss: 0.12754441110546
train: 0.994842	val: 0.914283	test: 0.677566

Epoch: 66
Loss: 0.1226382165926748
train: 0.988658	val: 0.884573	test: 0.667728

Epoch: 67
Loss: 0.09991169143394041
train: 0.994814	val: 0.906354	test: 0.669271

Epoch: 68
Loss: 0.11797056713741923
train: 0.991228	val: 0.924119	test: 0.666281

Epoch: 69
Loss: 0.10429514453382026
train: 0.995608	val: 0.910268	test: 0.685378

Epoch: 70
Loss: 0.11498441154834886
train: 0.996380	val: 0.909867	test: 0.677951

Epoch: 71
Loss: 0.11094431519890965
train: 0.996496	val: 0.912878	test: 0.699942

Epoch: 72
Loss: 0.08804550405233712
train: 0.996983	val: 0.918298	test: 0.695988

Epoch: 73
Loss: 0.0946080884925187
train: 0.997493	val: 0.913982	test: 0.705343

Epoch: 74
Loss: 0.10105300680668157
train: 0.997426	val: 0.915688	test: 0.692998

Epoch: 75
Loss: 0.09223192912054431
train: 0.996263	val: 0.910971	test: 0.691647

Epoch: 76
Loss: 0.09196392636041752
train: 0.997332	val: 0.905450	test: 0.680170

Epoch: 77
Loss: 0.09002656166342694
train: 0.997248	val: 0.908662	test: 0.669946

Epoch: 78
Loss: 0.08767070443379435
train: 0.996577	val: 0.896818	test: 0.697531

Epoch: 79
Loss: 0.10423309675369928
train: 0.997143	val: 0.916090	test: 0.696856

Epoch: 80
Loss: 0.10208259175849885
train: 0.995807	val: 0.910168	test: 0.682099

Epoch: 81
Loss: 0.10180390052960196
train: 0.995305	val: 0.908562	test: 0.685667

Epoch: 82
Loss: 0.09699320944264016
train: 0.997799	val: 0.913982	test: 0.699653

Epoch: 83
Loss: 0.08763620572576311
train: 0.997144	val: 0.913580	test: 0.701003

Epoch: 84
Loss: 0.10139837465122817
train: 0.994796	val: 0.905350	test: 0.692226

Epoch: 85
Loss: 0.09647064372402527
train: 0.997694	val: 0.912577	test: 0.679495

Epoch: 86
Loss: 0.08767998877806245
train: 0.997681	val: 0.915688	test: 0.692226

Epoch: 87
Loss: 0.08131365927760302
train: 0.998409	val: 0.916491	test: 0.699942

Epoch: 88
Loss: 0.08423716890185544
train: 0.996673	val: 0.916190	test: 0.673900

Epoch: 89
Loss: 0.08961801556151859
train: 0.997370	val: 0.916391	test: 0.673611

Epoch: 90
Loss: 0.09120893064628362
train: 0.996165	val: 0.902941	test: 0.690683

Epoch: 91
Loss: 0.08785825794765492
train: 0.998063	val: 0.908562	test: 0.696952

Epoch: 92
Loss: 0.09587547131739504
train: 0.997941	val: 0.903142	test: 0.700521

Epoch: 93
Loss: 0.0874270586786325
train: 0.997789	val: 0.915387	test: 0.698399

Epoch: 94
Loss: 0.09526319386532411
train: 0.995994	val: 0.917595	test: 0.699749

Epoch: 95
Loss: 0.0875117006977095
train: 0.997561	val: 0.907056	test: 0.688272

Epoch: 96
Loss: 0.08527280445779982
train: 0.998056	val: 0.913781	test: 0.689718

Epoch: 97
Loss: 0.08998744171760802
train: 0.998508	val: 0.906253	test: 0.682195

Epoch: 98
Loss: 0.09673468206879934
train: 0.997959	val: 0.911874	test: 0.683063

Epoch: 99
Loss: 0.10574611985500257
train: 0.997998	val: 0.909666	test: 0.692805

Epoch: 100
Loss: 0.08010167365930979
train: 0.998370	val: 0.899127	test: 0.710648

best train: 0.946493	val: 0.938773	test: 0.690876
end
