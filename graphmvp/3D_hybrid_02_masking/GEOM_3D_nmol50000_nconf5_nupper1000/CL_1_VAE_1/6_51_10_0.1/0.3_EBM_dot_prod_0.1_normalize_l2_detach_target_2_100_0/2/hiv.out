11288244_2
--dataset=hiv --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25517693768417077
train: 0.766624	val: 0.743184	test: 0.752243

Epoch: 2
Loss: 0.1386523866348164
train: 0.788606	val: 0.757566	test: 0.762199

Epoch: 3
Loss: 0.13286828937733566
train: 0.796459	val: 0.745623	test: 0.757056

Epoch: 4
Loss: 0.1290026268218687
train: 0.821205	val: 0.782466	test: 0.777727

Epoch: 5
Loss: 0.1273524469551523
train: 0.828525	val: 0.789490	test: 0.763972

Epoch: 6
Loss: 0.1240548659378599
train: 0.834418	val: 0.802233	test: 0.773319

Epoch: 7
Loss: 0.12183884488585481
train: 0.847095	val: 0.780408	test: 0.758570

Epoch: 8
Loss: 0.12096147990799848
train: 0.857073	val: 0.769731	test: 0.758655

Epoch: 9
Loss: 0.11816788180353971
train: 0.859127	val: 0.804147	test: 0.743861

Epoch: 10
Loss: 0.11651285790940373
train: 0.850243	val: 0.759734	test: 0.747984

Epoch: 11
Loss: 0.11588724214171049
train: 0.854386	val: 0.819285	test: 0.751656

Epoch: 12
Loss: 0.11484716121266393
train: 0.863274	val: 0.790809	test: 0.766666

Epoch: 13
Loss: 0.11333348288338649
train: 0.870644	val: 0.786688	test: 0.735366

Epoch: 14
Loss: 0.11239165760226066
train: 0.869124	val: 0.810638	test: 0.753833

Epoch: 15
Loss: 0.11334152513625473
train: 0.881315	val: 0.780635	test: 0.753271

Epoch: 16
Loss: 0.11218092657526021
train: 0.886257	val: 0.798091	test: 0.750536

Epoch: 17
Loss: 0.11042924845234835
train: 0.883406	val: 0.831793	test: 0.762925

Epoch: 18
Loss: 0.10879557565078078
train: 0.887416	val: 0.787643	test: 0.763468

Epoch: 19
Loss: 0.10999698181683902
train: 0.891547	val: 0.801082	test: 0.760552

Epoch: 20
Loss: 0.10808291121719087
train: 0.896547	val: 0.804796	test: 0.761210

Epoch: 21
Loss: 0.10745888282692398
train: 0.896404	val: 0.799361	test: 0.763814

Epoch: 22
Loss: 0.10584511461179277
train: 0.903626	val: 0.816190	test: 0.761786

Epoch: 23
Loss: 0.10778957249041352
train: 0.903101	val: 0.808630	test: 0.744091

Epoch: 24
Loss: 0.10482357924532983
train: 0.902697	val: 0.807491	test: 0.739817

Epoch: 25
Loss: 0.10412740823790373
train: 0.911339	val: 0.792509	test: 0.778613

Epoch: 26
Loss: 0.10537794383886952
train: 0.909855	val: 0.804236	test: 0.764439

Epoch: 27
Loss: 0.10338988523107763
train: 0.912564	val: 0.795114	test: 0.764739

Epoch: 28
Loss: 0.10293805544175735
train: 0.909019	val: 0.800503	test: 0.759279

Epoch: 29
Loss: 0.10153392397787411
train: 0.921398	val: 0.795329	test: 0.757854

Epoch: 30
Loss: 0.10084158141513634
train: 0.907098	val: 0.773669	test: 0.768192

Epoch: 31
Loss: 0.10186773964977035
train: 0.909264	val: 0.781556	test: 0.744812

Epoch: 32
Loss: 0.10190150452841337
train: 0.916163	val: 0.784535	test: 0.763078

Epoch: 33
Loss: 0.10073701854935842
train: 0.923121	val: 0.838526	test: 0.766778

Epoch: 34
Loss: 0.09960788302785474
train: 0.921306	val: 0.782019	test: 0.746407

Epoch: 35
Loss: 0.10016181027187958
train: 0.928053	val: 0.791394	test: 0.760722

Epoch: 36
Loss: 0.09856595149185626
train: 0.912249	val: 0.775389	test: 0.759690

Epoch: 37
Loss: 0.09816899406673787
train: 0.928893	val: 0.804934	test: 0.745090

Epoch: 38
Loss: 0.09877910370770765
train: 0.923083	val: 0.806342	test: 0.772126

Epoch: 39
Loss: 0.09606358611665859
train: 0.931209	val: 0.790678	test: 0.757197

Epoch: 40
Loss: 0.09767577751034638
train: 0.929559	val: 0.793985	test: 0.768715

Epoch: 41
Loss: 0.09574447009533044
train: 0.922536	val: 0.783464	test: 0.759887

Epoch: 42
Loss: 0.09641685335132343
train: 0.935212	val: 0.803082	test: 0.759887

Epoch: 43
Loss: 0.095879266944443
train: 0.934742	val: 0.818210	test: 0.735416

Epoch: 44
Loss: 0.09573693391006809
train: 0.932752	val: 0.769930	test: 0.742627

Epoch: 45
Loss: 0.09486626804145412
train: 0.937613	val: 0.778338	test: 0.753205

Epoch: 46
Loss: 0.0935811438983039
train: 0.937388	val: 0.803829	test: 0.763501

Epoch: 47
Loss: 0.09385585619626763
train: 0.933922	val: 0.776529	test: 0.752674

Epoch: 48
Loss: 0.09271249367078169
train: 0.943409	val: 0.796263	test: 0.765731

Epoch: 49
Loss: 0.09256030632323296
train: 0.943430	val: 0.797083	test: 0.742944

Epoch: 50
Loss: 0.09322129768003302
train: 0.941768	val: 0.792095	test: 0.760123

Epoch: 51
Loss: 0.09137348723203916
train: 0.943684	val: 0.804211	test: 0.764113

Epoch: 52
Loss: 0.09028935207555187
train: 0.941478	val: 0.782202	test: 0.751457

Epoch: 53
Loss: 0.09085964737775096
train: 0.949373	val: 0.804459	test: 0.776292

Epoch: 54
Loss: 0.09076016640053805
train: 0.949794	val: 0.809163	test: 0.752701

Epoch: 55
Loss: 0.09026667007341237
train: 0.951165	val: 0.798939	test: 0.759476

Epoch: 56
Loss: 0.08917957096897525
train: 0.949235	val: 0.806361	test: 0.761616

Epoch: 57
Loss: 0.08997136364719993
train: 0.954198	val: 0.801792	test: 0.765318

Epoch: 58
Loss: 0.08848509625896908
train: 0.954908	val: 0.803620	test: 0.756745

Epoch: 59
Loss: 0.08930994242788184
train: 0.955648	val: 0.799472	test: 0.756569

Epoch: 60
Loss: 0.0902803255644597
train: 0.945824	val: 0.789627	test: 0.750215

Epoch: 61
Loss: 0.08859989392254598
train: 0.954034	val: 0.808373	test: 0.768026

Epoch: 62
Loss: 0.08646819779540511
train: 0.959002	val: 0.797034	test: 0.752993

Epoch: 63
Loss: 0.08638540755789116
train: 0.955984	val: 0.777248	test: 0.753753

Epoch: 64
Loss: 0.08842738168011834
train: 0.960293	val: 0.827301	test: 0.770718

Epoch: 65
Loss: 0.0876346581565117
train: 0.956611	val: 0.825069	test: 0.761065

Epoch: 66
Loss: 0.08799147876173814
train: 0.960594	val: 0.802295	test: 0.741399

Epoch: 67
Loss: 0.08477433711254583
train: 0.953466	val: 0.803584	test: 0.778358

Epoch: 68
Loss: 0.08698176847337992
train: 0.944530	val: 0.796890	test: 0.741751

Epoch: 69
Loss: 0.0844164883587658
train: 0.966758	val: 0.811422	test: 0.762993

Epoch: 70
Loss: 0.0844412368758781
train: 0.961257	val: 0.786746	test: 0.756689

Epoch: 71
Loss: 0.08450038116685166
train: 0.965133	val: 0.805816	test: 0.766784

Epoch: 72
Loss: 0.08489894492609916
train: 0.965091	val: 0.806967	test: 0.780380

Epoch: 73
Loss: 0.08371886218339143
train: 0.966228	val: 0.801468	test: 0.747280

Epoch: 74
Loss: 0.08181571192858599
train: 0.965973	val: 0.792891	test: 0.773435

Epoch: 75
Loss: 0.0827466420724831
train: 0.967335	val: 0.807448	test: 0.756301

Epoch: 76
Loss: 0.08248081690321551
train: 0.967078	val: 0.805014	test: 0.744638

Epoch: 77
Loss: 0.08298219788021535
train: 0.967052	val: 0.805363	test: 0.754101

Epoch: 78
Loss: 0.0818342242778213
train: 0.969375	val: 0.807586	test: 0.768014

Epoch: 79
Loss: 0.08001985911432706
train: 0.969344	val: 0.807512	test: 0.737314

Epoch: 80
Loss: 0.08034412321535751
train: 0.971029	val: 0.788966	test: 0.754972

Epoch: 81
Loss: 0.08008988062205782
train: 0.967778	val: 0.787600	test: 0.745175

Epoch: 82
Loss: 0.07974227608989039
train: 0.964865	val: 0.785659	test: 0.755606

Epoch: 83
Loss: 0.07895349323035822
train: 0.971471	val: 0.810452	test: 0.750942

Epoch: 84
Loss: 0.08073664918845101
train: 0.963500	val: 0.784098	test: 0.751890

Epoch: 85
Loss: 0.07881817665504258
train: 0.972708	val: 0.792673	test: 0.761363

Epoch: 86
Loss: 0.07882733824471692
train: 0.974534	val: 0.796899	test: 0.754493

Epoch: 87
Loss: 0.0775025162033157
train: 0.975268	val: 0.797873	test: 0.759543

Epoch: 88
Loss: 0.07727408931607196
train: 0.973402	val: 0.775717	test: 0.760306

Epoch: 89
Loss: 0.07655890898661448
train: 0.972865	val: 0.787285	test: 0.750652

Epoch: 90
Loss: 0.07728070353285679
train: 0.976378	val: 0.806052	test: 0.745128

Epoch: 91
Loss: 0.07695871047585938
train: 0.977440	val: 0.797399	test: 0.758655

Epoch: 92
Loss: 0.07758943320958603
train: 0.975956	val: 0.811141	test: 0.756992

Epoch: 93
Loss: 0.07515286648965222
train: 0.975885	val: 0.796544	test: 0.741841

Epoch: 94
Loss: 0.07812990863365965
train: 0.978953	val: 0.776072	test: 0.765729

Epoch: 95
Loss: 0.07519129486943253
train: 0.967367	val: 0.789597	test: 0.724145

Epoch: 96
Loss: 0.07588595240422455
train: 0.979224	val: 0.800035	test: 0.767213

Epoch: 97
Loss: 0.07496705591808882
train: 0.977817	val: 0.779281	test: 0.749578

Epoch: 98
Loss: 0.07479162117979622
train: 0.973505	val: 0.785638	test: 0.753645

Epoch: 99
Loss: 0.07464704150400521
train: 0.976382	val: 0.784177	test: 0.767836

Epoch: 100
Loss: 0.07370156326045167
train: 0.980800	val: 0.797723	test: 0.757504

best train: 0.923121	val: 0.838526	test: 0.766778
end
