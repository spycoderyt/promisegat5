11288244_2
--dataset=sider --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6857104044284454
train: 0.541400	val: 0.540026	test: 0.512159

Epoch: 2
Loss: 0.6445564975787544
train: 0.573706	val: 0.537268	test: 0.528239

Epoch: 3
Loss: 0.6088128719919604
train: 0.585998	val: 0.531340	test: 0.535593

Epoch: 4
Loss: 0.5819509738217608
train: 0.600938	val: 0.536355	test: 0.540868

Epoch: 5
Loss: 0.5549370841372003
train: 0.627074	val: 0.550434	test: 0.551155

Epoch: 6
Loss: 0.5422321401392922
train: 0.648696	val: 0.555153	test: 0.573969

Epoch: 7
Loss: 0.5293221728816752
train: 0.664530	val: 0.561382	test: 0.589897

Epoch: 8
Loss: 0.5182521211255597
train: 0.677683	val: 0.573852	test: 0.593666

Epoch: 9
Loss: 0.5116273601848776
train: 0.693181	val: 0.581989	test: 0.599881

Epoch: 10
Loss: 0.5020097608317686
train: 0.694638	val: 0.579289	test: 0.601306

Epoch: 11
Loss: 0.4993173006675389
train: 0.706292	val: 0.588822	test: 0.601825

Epoch: 12
Loss: 0.4917229646630316
train: 0.714055	val: 0.597335	test: 0.594165

Epoch: 13
Loss: 0.48770608859327486
train: 0.720915	val: 0.587101	test: 0.598765

Epoch: 14
Loss: 0.48685444911399306
train: 0.725242	val: 0.593921	test: 0.603088

Epoch: 15
Loss: 0.4776720724243416
train: 0.733280	val: 0.599702	test: 0.604544

Epoch: 16
Loss: 0.4737104363287762
train: 0.735617	val: 0.587811	test: 0.609173

Epoch: 17
Loss: 0.4750599679160354
train: 0.741873	val: 0.601727	test: 0.603814

Epoch: 18
Loss: 0.4752130811749429
train: 0.740278	val: 0.610740	test: 0.595629

Epoch: 19
Loss: 0.468610437493048
train: 0.744001	val: 0.615974	test: 0.589033

Epoch: 20
Loss: 0.4678745384301915
train: 0.751722	val: 0.606562	test: 0.604843

Epoch: 21
Loss: 0.46529823417713195
train: 0.754198	val: 0.597557	test: 0.610774

Epoch: 22
Loss: 0.46123335890771316
train: 0.762262	val: 0.605394	test: 0.604752

Epoch: 23
Loss: 0.45900490272141836
train: 0.763362	val: 0.616536	test: 0.602792

Epoch: 24
Loss: 0.4597608564622905
train: 0.766969	val: 0.617114	test: 0.615104

Epoch: 25
Loss: 0.4544067023113031
train: 0.767194	val: 0.610251	test: 0.611792

Epoch: 26
Loss: 0.45766016659961756
train: 0.774021	val: 0.608398	test: 0.603498

Epoch: 27
Loss: 0.45361226617972966
train: 0.774727	val: 0.607392	test: 0.596426

Epoch: 28
Loss: 0.44981231506482083
train: 0.779405	val: 0.614237	test: 0.594909

Epoch: 29
Loss: 0.4473162186001674
train: 0.783690	val: 0.626404	test: 0.596623

Epoch: 30
Loss: 0.44841108973104526
train: 0.785061	val: 0.621219	test: 0.598340

Epoch: 31
Loss: 0.44656322181480307
train: 0.788952	val: 0.613291	test: 0.599858

Epoch: 32
Loss: 0.44913744578358833
train: 0.791336	val: 0.616655	test: 0.604172

Epoch: 33
Loss: 0.4477171021312736
train: 0.791152	val: 0.609654	test: 0.608068

Epoch: 34
Loss: 0.44798680067811725
train: 0.795033	val: 0.612344	test: 0.607387

Epoch: 35
Loss: 0.43956460928131796
train: 0.797055	val: 0.614345	test: 0.616061

Epoch: 36
Loss: 0.4435411944195879
train: 0.799944	val: 0.618055	test: 0.609835

Epoch: 37
Loss: 0.4372413315621094
train: 0.802976	val: 0.609892	test: 0.603987

Epoch: 38
Loss: 0.4382892689650221
train: 0.803163	val: 0.603582	test: 0.595379

Epoch: 39
Loss: 0.435442378382474
train: 0.808247	val: 0.617897	test: 0.586146

Epoch: 40
Loss: 0.43615963158040705
train: 0.809560	val: 0.609467	test: 0.594969

Epoch: 41
Loss: 0.4329958269991113
train: 0.813588	val: 0.606280	test: 0.591324

Epoch: 42
Loss: 0.43070102542647104
train: 0.814465	val: 0.607388	test: 0.579953

Epoch: 43
Loss: 0.4277937287911898
train: 0.814954	val: 0.600600	test: 0.587388

Epoch: 44
Loss: 0.4294941225597653
train: 0.816699	val: 0.602598	test: 0.604645

Epoch: 45
Loss: 0.4267625906243747
train: 0.818076	val: 0.617927	test: 0.597644

Epoch: 46
Loss: 0.4261024068066231
train: 0.824272	val: 0.612563	test: 0.599774

Epoch: 47
Loss: 0.4237715332829122
train: 0.827133	val: 0.604829	test: 0.591883

Epoch: 48
Loss: 0.42022929495116657
train: 0.826578	val: 0.602295	test: 0.612055

Epoch: 49
Loss: 0.4188773911070923
train: 0.823949	val: 0.616495	test: 0.605929

Epoch: 50
Loss: 0.42292223071696544
train: 0.831095	val: 0.616015	test: 0.608965

Epoch: 51
Loss: 0.419282381144611
train: 0.831877	val: 0.614069	test: 0.602760

Epoch: 52
Loss: 0.41901718591404624
train: 0.832005	val: 0.630684	test: 0.601714

Epoch: 53
Loss: 0.42168111117342033
train: 0.833463	val: 0.633975	test: 0.601269

Epoch: 54
Loss: 0.41762853176715
train: 0.838052	val: 0.624059	test: 0.608596

Epoch: 55
Loss: 0.41472223895706034
train: 0.839139	val: 0.620784	test: 0.593729

Epoch: 56
Loss: 0.4152468651549519
train: 0.839731	val: 0.630178	test: 0.584625

Epoch: 57
Loss: 0.4159812387107758
train: 0.842616	val: 0.632913	test: 0.589904

Epoch: 58
Loss: 0.4122810008185942
train: 0.844159	val: 0.612754	test: 0.581873

Epoch: 59
Loss: 0.41678748239213287
train: 0.840928	val: 0.588990	test: 0.573793

Epoch: 60
Loss: 0.4130352922077615
train: 0.841319	val: 0.591515	test: 0.588314

Epoch: 61
Loss: 0.4147126692859401
train: 0.845956	val: 0.610435	test: 0.603538

Epoch: 62
Loss: 0.41034339769539424
train: 0.851602	val: 0.613791	test: 0.599277

Epoch: 63
Loss: 0.4069827568819079
train: 0.850612	val: 0.616165	test: 0.602610

Epoch: 64
Loss: 0.4007659946317541
train: 0.854217	val: 0.624368	test: 0.591054

Epoch: 65
Loss: 0.39841825246314977
train: 0.850825	val: 0.615628	test: 0.590429

Epoch: 66
Loss: 0.4020881105345212
train: 0.858805	val: 0.616844	test: 0.599475

Epoch: 67
Loss: 0.4086902077316751
train: 0.859348	val: 0.617511	test: 0.596399

Epoch: 68
Loss: 0.40022784910522596
train: 0.856962	val: 0.618241	test: 0.584170

Epoch: 69
Loss: 0.40090155443027903
train: 0.857056	val: 0.629333	test: 0.606678

Epoch: 70
Loss: 0.3995642018503367
train: 0.858029	val: 0.628113	test: 0.591539

Epoch: 71
Loss: 0.4039988413204286
train: 0.861917	val: 0.620348	test: 0.585299

Epoch: 72
Loss: 0.39964101460301565
train: 0.864513	val: 0.621856	test: 0.594929

Epoch: 73
Loss: 0.3970536923891589
train: 0.864727	val: 0.635623	test: 0.599195

Epoch: 74
Loss: 0.4016614493289721
train: 0.865958	val: 0.625458	test: 0.585899

Epoch: 75
Loss: 0.39899539275886375
train: 0.868275	val: 0.616031	test: 0.585877

Epoch: 76
Loss: 0.39724797625728187
train: 0.867148	val: 0.620283	test: 0.608404

Epoch: 77
Loss: 0.3905758693782384
train: 0.866534	val: 0.628567	test: 0.603135

Epoch: 78
Loss: 0.3944668002998622
train: 0.869096	val: 0.633856	test: 0.608331

Epoch: 79
Loss: 0.39629471488653256
train: 0.865012	val: 0.627044	test: 0.603993

Epoch: 80
Loss: 0.3895756076161464
train: 0.870199	val: 0.626044	test: 0.606576

Epoch: 81
Loss: 0.3952828060921829
train: 0.872565	val: 0.625749	test: 0.605355

Epoch: 82
Loss: 0.3902137751312428
train: 0.873852	val: 0.617564	test: 0.588265

Epoch: 83
Loss: 0.3843727965115074
train: 0.876373	val: 0.614262	test: 0.588993

Epoch: 84
Loss: 0.3861252732495098
train: 0.876903	val: 0.614006	test: 0.599812

Epoch: 85
Loss: 0.38223406197282406
train: 0.874757	val: 0.621853	test: 0.580934

Epoch: 86
Loss: 0.3847916317980973
train: 0.877382	val: 0.621061	test: 0.609275

Epoch: 87
Loss: 0.3826778884103412
train: 0.879519	val: 0.627916	test: 0.592719

Epoch: 88
Loss: 0.38174600519487495
train: 0.882336	val: 0.629292	test: 0.586128

Epoch: 89
Loss: 0.38412498832643627
train: 0.881544	val: 0.628364	test: 0.601011

Epoch: 90
Loss: 0.37878007852972295
train: 0.883266	val: 0.622383	test: 0.577356

Epoch: 91
Loss: 0.3835099616153675
train: 0.880569	val: 0.621662	test: 0.577799

Epoch: 92
Loss: 0.3765516090634149
train: 0.884009	val: 0.632306	test: 0.584041

Epoch: 93
Loss: 0.3737036765445225
train: 0.887701	val: 0.633191	test: 0.595008

Epoch: 94
Loss: 0.3759454363782019
train: 0.884270	val: 0.621322	test: 0.570257

Epoch: 95
Loss: 0.37884564447995966
train: 0.889613	val: 0.620402	test: 0.577939

Epoch: 96
Loss: 0.37178929037349484
train: 0.890789	val: 0.632919	test: 0.589211

Epoch: 97
Loss: 0.37184762302634483
train: 0.891735	val: 0.634236	test: 0.580649

Epoch: 98
Loss: 0.37555359663595933
train: 0.894030	val: 0.628135	test: 0.578451

Epoch: 99
Loss: 0.3701939007018285
train: 0.893531	val: 0.632863	test: 0.580712

Epoch: 100
Loss: 0.36753715506141643
train: 0.894318	val: 0.631994	test: 0.595232

best train: 0.864727	val: 0.635623	test: 0.599195
end
