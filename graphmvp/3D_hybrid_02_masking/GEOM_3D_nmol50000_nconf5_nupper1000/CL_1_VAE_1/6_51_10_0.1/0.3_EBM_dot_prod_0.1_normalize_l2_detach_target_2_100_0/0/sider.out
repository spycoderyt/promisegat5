13650977_0
--dataset=sider --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6780985531909507
train: 0.544499	val: 0.522970	test: 0.512898

Epoch: 2
Loss: 0.6358971695856164
train: 0.570907	val: 0.542153	test: 0.517328

Epoch: 3
Loss: 0.6003944519193579
train: 0.595319	val: 0.534410	test: 0.532685

Epoch: 4
Loss: 0.5750590164879934
train: 0.616437	val: 0.543990	test: 0.552952

Epoch: 5
Loss: 0.5507658497594758
train: 0.640844	val: 0.561915	test: 0.571793

Epoch: 6
Loss: 0.5374625322024935
train: 0.647563	val: 0.565253	test: 0.579645

Epoch: 7
Loss: 0.5229495101169305
train: 0.667424	val: 0.575518	test: 0.584838

Epoch: 8
Loss: 0.5181201764448844
train: 0.681992	val: 0.568082	test: 0.584826

Epoch: 9
Loss: 0.5073963672854609
train: 0.694849	val: 0.576182	test: 0.588957

Epoch: 10
Loss: 0.49985464863437523
train: 0.705598	val: 0.584331	test: 0.583249

Epoch: 11
Loss: 0.4900466140731618
train: 0.711994	val: 0.580163	test: 0.583625

Epoch: 12
Loss: 0.4872555142587546
train: 0.718548	val: 0.590160	test: 0.579117

Epoch: 13
Loss: 0.48006797371906484
train: 0.723367	val: 0.606814	test: 0.584281

Epoch: 14
Loss: 0.4782473184885799
train: 0.729882	val: 0.608510	test: 0.591997

Epoch: 15
Loss: 0.47704766807306714
train: 0.733698	val: 0.598060	test: 0.588559

Epoch: 16
Loss: 0.47574297520416
train: 0.741170	val: 0.606155	test: 0.580252

Epoch: 17
Loss: 0.46620000263561695
train: 0.744404	val: 0.597902	test: 0.596866

Epoch: 18
Loss: 0.46524895923407944
train: 0.747136	val: 0.603168	test: 0.596268

Epoch: 19
Loss: 0.46604640111149803
train: 0.750531	val: 0.621027	test: 0.578664

Epoch: 20
Loss: 0.46423496698129413
train: 0.756449	val: 0.618311	test: 0.583416

Epoch: 21
Loss: 0.45900868380518534
train: 0.760608	val: 0.607359	test: 0.587272

Epoch: 22
Loss: 0.4575393082862293
train: 0.761717	val: 0.605431	test: 0.597795

Epoch: 23
Loss: 0.45889311017134704
train: 0.756787	val: 0.613520	test: 0.590198

Epoch: 24
Loss: 0.45630622333848325
train: 0.766999	val: 0.610089	test: 0.598593

Epoch: 25
Loss: 0.45551088903311265
train: 0.768160	val: 0.604649	test: 0.604181

Epoch: 26
Loss: 0.45143590353506724
train: 0.776125	val: 0.611417	test: 0.602916

Epoch: 27
Loss: 0.4537643422712053
train: 0.781498	val: 0.616844	test: 0.597156

Epoch: 28
Loss: 0.44969011658146824
train: 0.780913	val: 0.604601	test: 0.606923

Epoch: 29
Loss: 0.4474758659642791
train: 0.783721	val: 0.606715	test: 0.604404

Epoch: 30
Loss: 0.4460077896189799
train: 0.785160	val: 0.605575	test: 0.590099

Epoch: 31
Loss: 0.4459969059770703
train: 0.789968	val: 0.608584	test: 0.595939

Epoch: 32
Loss: 0.4410534285403557
train: 0.788856	val: 0.602634	test: 0.595757

Epoch: 33
Loss: 0.44179717746022157
train: 0.795496	val: 0.600127	test: 0.604953

Epoch: 34
Loss: 0.4425395294445312
train: 0.799785	val: 0.610426	test: 0.600417

Epoch: 35
Loss: 0.4417722493168871
train: 0.798517	val: 0.603665	test: 0.597636

Epoch: 36
Loss: 0.43832786018961895
train: 0.800892	val: 0.607908	test: 0.601622

Epoch: 37
Loss: 0.4408913949633818
train: 0.802085	val: 0.615586	test: 0.599131

Epoch: 38
Loss: 0.437813890926923
train: 0.804072	val: 0.618572	test: 0.600294

Epoch: 39
Loss: 0.43667680870194864
train: 0.809120	val: 0.615857	test: 0.599420

Epoch: 40
Loss: 0.4305970402897815
train: 0.814353	val: 0.615579	test: 0.594417

Epoch: 41
Loss: 0.42585318258049726
train: 0.816503	val: 0.604645	test: 0.585884

Epoch: 42
Loss: 0.42857153249375307
train: 0.818256	val: 0.612543	test: 0.592803

Epoch: 43
Loss: 0.4321794869707599
train: 0.817226	val: 0.613729	test: 0.608927

Epoch: 44
Loss: 0.42719279618325745
train: 0.824578	val: 0.627021	test: 0.603199

Epoch: 45
Loss: 0.42427232379583213
train: 0.825093	val: 0.625051	test: 0.614322

Epoch: 46
Loss: 0.4241486118279349
train: 0.828003	val: 0.619341	test: 0.605470

Epoch: 47
Loss: 0.42375167593162527
train: 0.830347	val: 0.623615	test: 0.602638

Epoch: 48
Loss: 0.4169266954815584
train: 0.829517	val: 0.606756	test: 0.609690

Epoch: 49
Loss: 0.42454898889519244
train: 0.832600	val: 0.605993	test: 0.611014

Epoch: 50
Loss: 0.4145705869376359
train: 0.835902	val: 0.617541	test: 0.604905

Epoch: 51
Loss: 0.42119847885658646
train: 0.837641	val: 0.611071	test: 0.598884

Epoch: 52
Loss: 0.4151283549070438
train: 0.833895	val: 0.614798	test: 0.616532

Epoch: 53
Loss: 0.41740523376386796
train: 0.833589	val: 0.623389	test: 0.622622

Epoch: 54
Loss: 0.41333850508540937
train: 0.842264	val: 0.618048	test: 0.610330

Epoch: 55
Loss: 0.4155865492194234
train: 0.842637	val: 0.603807	test: 0.594616

Epoch: 56
Loss: 0.4084003289013077
train: 0.842928	val: 0.590619	test: 0.601208

Epoch: 57
Loss: 0.4124910817937596
train: 0.824148	val: 0.592889	test: 0.585719

Epoch: 58
Loss: 0.4046122719508755
train: 0.846072	val: 0.588822	test: 0.591302

Epoch: 59
Loss: 0.4072725697804788
train: 0.846467	val: 0.599157	test: 0.581999

Epoch: 60
Loss: 0.40773081262554955
train: 0.849706	val: 0.608253	test: 0.575359

Epoch: 61
Loss: 0.40566499601113487
train: 0.855321	val: 0.612762	test: 0.593561

Epoch: 62
Loss: 0.40796728000494625
train: 0.849411	val: 0.615744	test: 0.610261

Epoch: 63
Loss: 0.40694987563590096
train: 0.853764	val: 0.629543	test: 0.607454

Epoch: 64
Loss: 0.40400592682277603
train: 0.852095	val: 0.619740	test: 0.598133

Epoch: 65
Loss: 0.3995154218725502
train: 0.857584	val: 0.609407	test: 0.598145

Epoch: 66
Loss: 0.4052590298463839
train: 0.859400	val: 0.608564	test: 0.612018

Epoch: 67
Loss: 0.3932518875274454
train: 0.857130	val: 0.611028	test: 0.600057

Epoch: 68
Loss: 0.3983092921404096
train: 0.855929	val: 0.607800	test: 0.589922

Epoch: 69
Loss: 0.3997497411325929
train: 0.862557	val: 0.611247	test: 0.582225

Epoch: 70
Loss: 0.3987251107436747
train: 0.863647	val: 0.616281	test: 0.587283

Epoch: 71
Loss: 0.3977532288817526
train: 0.860904	val: 0.627579	test: 0.592220

Epoch: 72
Loss: 0.3942939868585791
train: 0.862966	val: 0.614231	test: 0.596961

Epoch: 73
Loss: 0.401932136765505
train: 0.866473	val: 0.615780	test: 0.604955

Epoch: 74
Loss: 0.39440705406723675
train: 0.868368	val: 0.622027	test: 0.606287

Epoch: 75
Loss: 0.38772777173926054
train: 0.866668	val: 0.611632	test: 0.607794

Epoch: 76
Loss: 0.3884902429216884
train: 0.870738	val: 0.606740	test: 0.600999

Epoch: 77
Loss: 0.3889188733640451
train: 0.873745	val: 0.608659	test: 0.602173

Epoch: 78
Loss: 0.38760817939406167
train: 0.872840	val: 0.613620	test: 0.594816

Epoch: 79
Loss: 0.3875126046272669
train: 0.869249	val: 0.595338	test: 0.619470

Epoch: 80
Loss: 0.3867761141687061
train: 0.876183	val: 0.607678	test: 0.596025

Epoch: 81
Loss: 0.38718601864910707
train: 0.878358	val: 0.615465	test: 0.575891

Epoch: 82
Loss: 0.38147510881742774
train: 0.877678	val: 0.612740	test: 0.593163

Epoch: 83
Loss: 0.38221770559588986
train: 0.876852	val: 0.612350	test: 0.614144

Epoch: 84
Loss: 0.3807471210409047
train: 0.881362	val: 0.621139	test: 0.597452

Epoch: 85
Loss: 0.3811144537797643
train: 0.878408	val: 0.609410	test: 0.598123

Epoch: 86
Loss: 0.3789995936188188
train: 0.883610	val: 0.627686	test: 0.596166

Epoch: 87
Loss: 0.37652808657474485
train: 0.881522	val: 0.612247	test: 0.605822

Epoch: 88
Loss: 0.38043356873818013
train: 0.881630	val: 0.603102	test: 0.603755

Epoch: 89
Loss: 0.3798197220377541
train: 0.886552	val: 0.611214	test: 0.593716

Epoch: 90
Loss: 0.37654555217549873
train: 0.887576	val: 0.615279	test: 0.611290

Epoch: 91
Loss: 0.37352078847819425
train: 0.880911	val: 0.610397	test: 0.613857

Epoch: 92
Loss: 0.3739295826958543
train: 0.881087	val: 0.613254	test: 0.595820

Epoch: 93
Loss: 0.3727556933130859
train: 0.889984	val: 0.615009	test: 0.592931

Epoch: 94
Loss: 0.375153210136942
train: 0.889005	val: 0.611011	test: 0.594384

Epoch: 95
Loss: 0.37768225546231504
train: 0.891766	val: 0.596712	test: 0.594775

Epoch: 96
Loss: 0.37106690758103783
train: 0.890999	val: 0.594179	test: 0.601329

Epoch: 97
Loss: 0.36725153222699236
train: 0.893200	val: 0.609598	test: 0.604063

Epoch: 98
Loss: 0.3666282515683307
train: 0.894964	val: 0.628634	test: 0.609409

Epoch: 99
Loss: 0.36355044707080253
train: 0.896886	val: 0.618087	test: 0.603132

Epoch: 100
Loss: 0.36104709631222204
train: 0.898442	val: 0.614106	test: 0.602402

best train: 0.853764	val: 0.629543	test: 0.607454
end
