13650977_0
--dataset=hiv --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.27120570274153893
train: 0.783512	val: 0.748922	test: 0.752711

Epoch: 2
Loss: 0.13782606334231376
train: 0.763347	val: 0.747223	test: 0.658201

Epoch: 3
Loss: 0.13216663510777987
train: 0.809732	val: 0.767998	test: 0.746913

Epoch: 4
Loss: 0.1287049811372944
train: 0.807622	val: 0.744605	test: 0.757224

Epoch: 5
Loss: 0.12570215796273704
train: 0.830640	val: 0.779275	test: 0.752559

Epoch: 6
Loss: 0.12271381643162367
train: 0.837753	val: 0.789891	test: 0.770115

Epoch: 7
Loss: 0.12276237033460985
train: 0.836521	val: 0.796786	test: 0.748649

Epoch: 8
Loss: 0.12170806660035766
train: 0.846813	val: 0.819610	test: 0.753056

Epoch: 9
Loss: 0.1188883103232163
train: 0.864509	val: 0.818535	test: 0.744367

Epoch: 10
Loss: 0.11680108717783988
train: 0.864945	val: 0.821119	test: 0.765718

Epoch: 11
Loss: 0.11712348296960365
train: 0.865835	val: 0.793317	test: 0.757263

Epoch: 12
Loss: 0.11444234452019261
train: 0.881581	val: 0.822356	test: 0.751667

Epoch: 13
Loss: 0.11338443555973393
train: 0.884710	val: 0.792732	test: 0.748098

Epoch: 14
Loss: 0.11253126663500729
train: 0.882359	val: 0.804441	test: 0.746791

Epoch: 15
Loss: 0.11062271664388255
train: 0.885744	val: 0.813719	test: 0.727341

Epoch: 16
Loss: 0.11087320998153777
train: 0.894290	val: 0.812151	test: 0.759516

Epoch: 17
Loss: 0.10972455406449408
train: 0.889899	val: 0.820825	test: 0.733745

Epoch: 18
Loss: 0.10869300473914024
train: 0.892710	val: 0.818526	test: 0.745729

Epoch: 19
Loss: 0.10883970403511924
train: 0.886387	val: 0.807947	test: 0.721478

Epoch: 20
Loss: 0.1080484292362199
train: 0.892146	val: 0.826536	test: 0.747400

Epoch: 21
Loss: 0.1072897914839323
train: 0.896190	val: 0.808431	test: 0.765349

Epoch: 22
Loss: 0.10453420623123538
train: 0.897093	val: 0.800843	test: 0.774575

Epoch: 23
Loss: 0.10618072236427924
train: 0.905710	val: 0.800901	test: 0.762742

Epoch: 24
Loss: 0.10623664309303482
train: 0.886193	val: 0.816732	test: 0.700351

Epoch: 25
Loss: 0.10418549543842148
train: 0.912104	val: 0.813042	test: 0.746592

Epoch: 26
Loss: 0.10413428760765228
train: 0.912444	val: 0.823391	test: 0.746718

Epoch: 27
Loss: 0.1038132242919939
train: 0.910440	val: 0.813866	test: 0.738392

Epoch: 28
Loss: 0.10312218334544858
train: 0.912901	val: 0.821781	test: 0.743502

Epoch: 29
Loss: 0.102661806608031
train: 0.916928	val: 0.817629	test: 0.756525

Epoch: 30
Loss: 0.10215664092933983
train: 0.910497	val: 0.813700	test: 0.741857

Epoch: 31
Loss: 0.10120512850138107
train: 0.916385	val: 0.799046	test: 0.733614

Epoch: 32
Loss: 0.10123686986476513
train: 0.921441	val: 0.811450	test: 0.744306

Epoch: 33
Loss: 0.10151284954998903
train: 0.922644	val: 0.803440	test: 0.757546

Epoch: 34
Loss: 0.10101531577618876
train: 0.919506	val: 0.824043	test: 0.736424

Epoch: 35
Loss: 0.09909431065940433
train: 0.926249	val: 0.812439	test: 0.736300

Epoch: 36
Loss: 0.1014469727402024
train: 0.924472	val: 0.825927	test: 0.748562

Epoch: 37
Loss: 0.09823865894310496
train: 0.927513	val: 0.822953	test: 0.757798

Epoch: 38
Loss: 0.09855390198880905
train: 0.919223	val: 0.829362	test: 0.729356

Epoch: 39
Loss: 0.09834183602629841
train: 0.930787	val: 0.811297	test: 0.738697

Epoch: 40
Loss: 0.09830549482869809
train: 0.933872	val: 0.799018	test: 0.748953

Epoch: 41
Loss: 0.09748907855182186
train: 0.933439	val: 0.815366	test: 0.750138

Epoch: 42
Loss: 0.09789178193864184
train: 0.937540	val: 0.809398	test: 0.721267

Epoch: 43
Loss: 0.09593005077608466
train: 0.939006	val: 0.804481	test: 0.750117

Epoch: 44
Loss: 0.09564342361585268
train: 0.939693	val: 0.813970	test: 0.730312

Epoch: 45
Loss: 0.09628568877080111
train: 0.937179	val: 0.802656	test: 0.730993

Epoch: 46
Loss: 0.09418502430998077
train: 0.935141	val: 0.811924	test: 0.725866

Epoch: 47
Loss: 0.09663216737210746
train: 0.939890	val: 0.804591	test: 0.754787

Epoch: 48
Loss: 0.09319080727956451
train: 0.944860	val: 0.817990	test: 0.743311

Epoch: 49
Loss: 0.09497257649798037
train: 0.941692	val: 0.811303	test: 0.733546

Epoch: 50
Loss: 0.09367815947850852
train: 0.944084	val: 0.803330	test: 0.745818

Epoch: 51
Loss: 0.09356736947273965
train: 0.947668	val: 0.812402	test: 0.768524

Epoch: 52
Loss: 0.09165369021492122
train: 0.942553	val: 0.825758	test: 0.766143

Epoch: 53
Loss: 0.09184404432740632
train: 0.949182	val: 0.823140	test: 0.763199

Epoch: 54
Loss: 0.0914684116068897
train: 0.951120	val: 0.804692	test: 0.748012

Epoch: 55
Loss: 0.09167599050640277
train: 0.945390	val: 0.826986	test: 0.735916

Epoch: 56
Loss: 0.08982628377970545
train: 0.945705	val: 0.819738	test: 0.742061

Epoch: 57
Loss: 0.09018972933045194
train: 0.950453	val: 0.806878	test: 0.747405

Epoch: 58
Loss: 0.08986331654028117
train: 0.942426	val: 0.824598	test: 0.747241

Epoch: 59
Loss: 0.08961122477771935
train: 0.954884	val: 0.797907	test: 0.761878

Epoch: 60
Loss: 0.08943934652347292
train: 0.952471	val: 0.803767	test: 0.749924

Epoch: 61
Loss: 0.08855954701000944
train: 0.950705	val: 0.815519	test: 0.763657

Epoch: 62
Loss: 0.08984835648870095
train: 0.958317	val: 0.821236	test: 0.752662

Epoch: 63
Loss: 0.08737784219973507
train: 0.954983	val: 0.818195	test: 0.754680

Epoch: 64
Loss: 0.08732064390224777
train: 0.959746	val: 0.824797	test: 0.762610

Epoch: 65
Loss: 0.08709872826904531
train: 0.956270	val: 0.824775	test: 0.765540

Epoch: 66
Loss: 0.08635827911549318
train: 0.956193	val: 0.829861	test: 0.745304

Epoch: 67
Loss: 0.08757480138437694
train: 0.955811	val: 0.790047	test: 0.743230

Epoch: 68
Loss: 0.08600952538882022
train: 0.962478	val: 0.805595	test: 0.750499

Epoch: 69
Loss: 0.0858614232247363
train: 0.960178	val: 0.829517	test: 0.758043

Epoch: 70
Loss: 0.08502345912818378
train: 0.962295	val: 0.795868	test: 0.736837

Epoch: 71
Loss: 0.08492985788590258
train: 0.960023	val: 0.811474	test: 0.753446

Epoch: 72
Loss: 0.08500726667415547
train: 0.965776	val: 0.800323	test: 0.740290

Epoch: 73
Loss: 0.084610293549347
train: 0.962987	val: 0.819224	test: 0.740022

Epoch: 74
Loss: 0.08412709594781542
train: 0.965718	val: 0.834188	test: 0.737708

Epoch: 75
Loss: 0.08343643856716601
train: 0.965061	val: 0.838780	test: 0.761133

Epoch: 76
Loss: 0.08127482857450972
train: 0.961538	val: 0.821839	test: 0.742154

Epoch: 77
Loss: 0.08426885028899604
train: 0.963892	val: 0.804401	test: 0.749045

Epoch: 78
Loss: 0.08206287976433518
train: 0.965237	val: 0.822852	test: 0.753574

Epoch: 79
Loss: 0.08105991112253466
train: 0.965049	val: 0.812509	test: 0.725815

Epoch: 80
Loss: 0.0817113720023786
train: 0.966180	val: 0.825363	test: 0.761538

Epoch: 81
Loss: 0.08032566745382883
train: 0.969229	val: 0.815455	test: 0.770675

Epoch: 82
Loss: 0.07953331596296936
train: 0.972216	val: 0.816229	test: 0.764167

Epoch: 83
Loss: 0.0799333410271231
train: 0.964242	val: 0.840400	test: 0.767970

Epoch: 84
Loss: 0.0797683411345612
train: 0.974547	val: 0.838652	test: 0.761846

Epoch: 85
Loss: 0.0800821931951179
train: 0.972702	val: 0.812476	test: 0.752386

Epoch: 86
Loss: 0.07915711972023874
train: 0.973989	val: 0.827828	test: 0.777226

Epoch: 87
Loss: 0.07759371192148275
train: 0.973923	val: 0.830434	test: 0.749510

Epoch: 88
Loss: 0.07978707920731161
train: 0.975410	val: 0.822509	test: 0.733064

Epoch: 89
Loss: 0.07821005345528881
train: 0.972884	val: 0.823211	test: 0.728264

Epoch: 90
Loss: 0.07799732183758662
train: 0.971231	val: 0.824690	test: 0.749621

Epoch: 91
Loss: 0.0768224875393353
train: 0.975361	val: 0.820241	test: 0.740746

Epoch: 92
Loss: 0.07849208197932772
train: 0.972393	val: 0.835562	test: 0.767292

Epoch: 93
Loss: 0.07681089135171182
train: 0.979201	val: 0.826070	test: 0.738207

Epoch: 94
Loss: 0.07706718725834318
train: 0.978871	val: 0.836677	test: 0.762523

Epoch: 95
Loss: 0.07444409102629937
train: 0.978031	val: 0.829451	test: 0.754966

Epoch: 96
Loss: 0.07520830524713117
train: 0.971528	val: 0.830103	test: 0.733313

Epoch: 97
Loss: 0.07386821737322104
train: 0.979178	val: 0.805338	test: 0.749893

Epoch: 98
Loss: 0.07508632088583037
train: 0.979092	val: 0.815149	test: 0.749323

Epoch: 99
Loss: 0.07640287612588768
train: 0.978582	val: 0.820643	test: 0.759586

Epoch: 100
Loss: 0.07413868932554092
train: 0.979687	val: 0.841230	test: 0.756907

best train: 0.979687	val: 0.841230	test: 0.756907
end
