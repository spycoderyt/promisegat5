13617833_1
--dataset=sider --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6965682311746878
train: 0.532859	val: 0.525766	test: 0.497098

Epoch: 2
Loss: 0.6517882134096966
train: 0.571782	val: 0.535359	test: 0.512514

Epoch: 3
Loss: 0.6182655174358915
train: 0.586133	val: 0.531864	test: 0.533011

Epoch: 4
Loss: 0.5842369439867997
train: 0.602500	val: 0.531578	test: 0.546023

Epoch: 5
Loss: 0.5612858735397047
train: 0.632076	val: 0.550234	test: 0.565314

Epoch: 6
Loss: 0.5444057376605725
train: 0.651361	val: 0.565886	test: 0.586624

Epoch: 7
Loss: 0.5292995150476669
train: 0.661361	val: 0.557605	test: 0.601780

Epoch: 8
Loss: 0.5229301089323769
train: 0.670890	val: 0.559725	test: 0.596806

Epoch: 9
Loss: 0.5140748347950929
train: 0.678935	val: 0.570800	test: 0.588456

Epoch: 10
Loss: 0.5030086848520858
train: 0.688060	val: 0.583944	test: 0.593882

Epoch: 11
Loss: 0.49808918913522426
train: 0.691297	val: 0.599157	test: 0.598868

Epoch: 12
Loss: 0.49651423362895664
train: 0.707222	val: 0.579044	test: 0.607357

Epoch: 13
Loss: 0.4897711965246304
train: 0.716445	val: 0.579898	test: 0.605898

Epoch: 14
Loss: 0.4866725143315677
train: 0.723689	val: 0.587713	test: 0.605398

Epoch: 15
Loss: 0.48090090953094966
train: 0.730831	val: 0.593188	test: 0.605504

Epoch: 16
Loss: 0.47829584425131555
train: 0.736626	val: 0.592963	test: 0.608259

Epoch: 17
Loss: 0.4751175007718813
train: 0.741084	val: 0.588796	test: 0.607715

Epoch: 18
Loss: 0.47182945952080874
train: 0.743625	val: 0.584757	test: 0.597769

Epoch: 19
Loss: 0.4708720027132318
train: 0.746491	val: 0.595638	test: 0.598261

Epoch: 20
Loss: 0.4666644985364897
train: 0.751356	val: 0.596043	test: 0.601726

Epoch: 21
Loss: 0.46492647446173957
train: 0.753601	val: 0.595362	test: 0.598623

Epoch: 22
Loss: 0.4671231174587371
train: 0.758220	val: 0.595684	test: 0.597208

Epoch: 23
Loss: 0.4605620486832568
train: 0.756707	val: 0.576075	test: 0.614015

Epoch: 24
Loss: 0.4596323308348332
train: 0.759686	val: 0.584674	test: 0.621546

Epoch: 25
Loss: 0.4565153087485248
train: 0.767868	val: 0.596336	test: 0.617770

Epoch: 26
Loss: 0.4553161220457331
train: 0.770336	val: 0.600354	test: 0.608688

Epoch: 27
Loss: 0.45690948263045783
train: 0.774057	val: 0.596443	test: 0.607998

Epoch: 28
Loss: 0.45006145839929346
train: 0.779071	val: 0.610785	test: 0.603418

Epoch: 29
Loss: 0.4525182572116215
train: 0.775425	val: 0.602058	test: 0.615426

Epoch: 30
Loss: 0.45434981044169775
train: 0.780008	val: 0.588128	test: 0.622571

Epoch: 31
Loss: 0.4515547329067081
train: 0.786196	val: 0.602220	test: 0.616592

Epoch: 32
Loss: 0.4474260185641839
train: 0.790749	val: 0.606680	test: 0.612736

Epoch: 33
Loss: 0.44185264419971243
train: 0.788331	val: 0.605556	test: 0.600216

Epoch: 34
Loss: 0.4456370314801448
train: 0.792476	val: 0.616564	test: 0.596267

Epoch: 35
Loss: 0.44409705776518527
train: 0.795387	val: 0.606111	test: 0.613751

Epoch: 36
Loss: 0.4398831458114622
train: 0.798426	val: 0.605533	test: 0.620362

Epoch: 37
Loss: 0.4406852505761895
train: 0.796080	val: 0.620775	test: 0.612718

Epoch: 38
Loss: 0.4429632419868453
train: 0.800370	val: 0.611598	test: 0.610019

Epoch: 39
Loss: 0.43828700982793156
train: 0.806695	val: 0.611267	test: 0.613396

Epoch: 40
Loss: 0.43572471677935204
train: 0.809693	val: 0.614955	test: 0.614324

Epoch: 41
Loss: 0.4303698957670924
train: 0.805101	val: 0.606650	test: 0.606558

Epoch: 42
Loss: 0.43850379134800566
train: 0.809434	val: 0.608322	test: 0.610121

Epoch: 43
Loss: 0.4354797698735151
train: 0.812809	val: 0.615375	test: 0.606064

Epoch: 44
Loss: 0.4308277398183318
train: 0.814575	val: 0.613793	test: 0.620836

Epoch: 45
Loss: 0.430287117604891
train: 0.815928	val: 0.615170	test: 0.629812

Epoch: 46
Loss: 0.43058964789452664
train: 0.820715	val: 0.628351	test: 0.621870

Epoch: 47
Loss: 0.43014746299708584
train: 0.823126	val: 0.623668	test: 0.618762

Epoch: 48
Loss: 0.4236717569359727
train: 0.804729	val: 0.590939	test: 0.620702

Epoch: 49
Loss: 0.4247708495702957
train: 0.828630	val: 0.617829	test: 0.612451

Epoch: 50
Loss: 0.4200199504890597
train: 0.827151	val: 0.624336	test: 0.603731

Epoch: 51
Loss: 0.4192783969456218
train: 0.825218	val: 0.599827	test: 0.616068

Epoch: 52
Loss: 0.4228360741319978
train: 0.828797	val: 0.618342	test: 0.599638

Epoch: 53
Loss: 0.41813814350263445
train: 0.833153	val: 0.624119	test: 0.608956

Epoch: 54
Loss: 0.4083722256784835
train: 0.836900	val: 0.626880	test: 0.612224

Epoch: 55
Loss: 0.41142382929786336
train: 0.833489	val: 0.614902	test: 0.615838

Epoch: 56
Loss: 0.41608106986347126
train: 0.836735	val: 0.625651	test: 0.616151

Epoch: 57
Loss: 0.4093439876817439
train: 0.841935	val: 0.621781	test: 0.607648

Epoch: 58
Loss: 0.41141622310472564
train: 0.841260	val: 0.607007	test: 0.611930

Epoch: 59
Loss: 0.4126789115172708
train: 0.844529	val: 0.614925	test: 0.605702

Epoch: 60
Loss: 0.408618218200524
train: 0.842555	val: 0.631411	test: 0.591357

Epoch: 61
Loss: 0.412497258134316
train: 0.844792	val: 0.611626	test: 0.609090

Epoch: 62
Loss: 0.40691651044051735
train: 0.847495	val: 0.619997	test: 0.615575

Epoch: 63
Loss: 0.4077284347955767
train: 0.853227	val: 0.626617	test: 0.617776

Epoch: 64
Loss: 0.4051225780769593
train: 0.850855	val: 0.622560	test: 0.616382

Epoch: 65
Loss: 0.4089863322197555
train: 0.853124	val: 0.626941	test: 0.609242

Epoch: 66
Loss: 0.4068202156623282
train: 0.853994	val: 0.626991	test: 0.610780

Epoch: 67
Loss: 0.40289058675760997
train: 0.855254	val: 0.622298	test: 0.607173

Epoch: 68
Loss: 0.4019453497961031
train: 0.855874	val: 0.625660	test: 0.613228

Epoch: 69
Loss: 0.40178320507573695
train: 0.857662	val: 0.625858	test: 0.615857

Epoch: 70
Loss: 0.4048369711835851
train: 0.858673	val: 0.618839	test: 0.621534

Epoch: 71
Loss: 0.39909462894751
train: 0.859975	val: 0.617166	test: 0.603611

Epoch: 72
Loss: 0.39450860022900436
train: 0.860354	val: 0.610984	test: 0.609610

Epoch: 73
Loss: 0.39564696526881893
train: 0.862089	val: 0.619915	test: 0.601701

Epoch: 74
Loss: 0.39285127964739414
train: 0.863264	val: 0.620432	test: 0.614112

Epoch: 75
Loss: 0.39246277542146796
train: 0.863910	val: 0.622144	test: 0.615833

Epoch: 76
Loss: 0.3946182411949583
train: 0.865486	val: 0.625872	test: 0.620984

Epoch: 77
Loss: 0.3937941708485615
train: 0.863545	val: 0.626348	test: 0.616206

Epoch: 78
Loss: 0.3969131268559055
train: 0.866078	val: 0.617343	test: 0.605036

Epoch: 79
Loss: 0.39245201047577677
train: 0.870428	val: 0.634929	test: 0.585170

Epoch: 80
Loss: 0.3935460417466687
train: 0.872151	val: 0.643478	test: 0.593517

Epoch: 81
Loss: 0.39474476974871214
train: 0.866432	val: 0.635795	test: 0.601445

Epoch: 82
Loss: 0.3870275569790886
train: 0.871151	val: 0.623500	test: 0.609036

Epoch: 83
Loss: 0.39130998886118984
train: 0.873552	val: 0.628964	test: 0.601611

Epoch: 84
Loss: 0.38748091928961303
train: 0.876237	val: 0.627961	test: 0.592030

Epoch: 85
Loss: 0.3823275390387891
train: 0.876879	val: 0.621533	test: 0.595350

Epoch: 86
Loss: 0.3822985669099005
train: 0.879748	val: 0.628093	test: 0.617851

Epoch: 87
Loss: 0.38221201885116046
train: 0.878514	val: 0.639465	test: 0.604742

Epoch: 88
Loss: 0.37941901707033254
train: 0.880871	val: 0.637513	test: 0.594773

Epoch: 89
Loss: 0.3801184380249149
train: 0.881286	val: 0.625878	test: 0.611503

Epoch: 90
Loss: 0.3805376903353518
train: 0.884206	val: 0.628939	test: 0.599169

Epoch: 91
Loss: 0.37738947748658547
train: 0.887590	val: 0.630648	test: 0.602062

Epoch: 92
Loss: 0.3735881048770294
train: 0.883949	val: 0.612719	test: 0.616399

Epoch: 93
Loss: 0.37778828665698316
train: 0.886250	val: 0.625592	test: 0.592690

Epoch: 94
Loss: 0.3807119816486734
train: 0.886026	val: 0.618946	test: 0.589484

Epoch: 95
Loss: 0.37495742694780654
train: 0.883689	val: 0.614032	test: 0.596670

Epoch: 96
Loss: 0.3740429960273116
train: 0.889227	val: 0.623945	test: 0.615980

Epoch: 97
Loss: 0.37508017236381963
train: 0.890025	val: 0.629884	test: 0.590669

Epoch: 98
Loss: 0.3742775839711204
train: 0.893287	val: 0.627282	test: 0.597733

Epoch: 99
Loss: 0.36876936917399494
train: 0.891104	val: 0.622302	test: 0.616355

Epoch: 100
Loss: 0.3689899600614076
train: 0.891047	val: 0.623756	test: 0.613404

best train: 0.872151	val: 0.643478	test: 0.593517
end
