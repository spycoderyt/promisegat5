13617833_1
--dataset=bace --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6588015914404404
train: 0.704583	val: 0.510256	test: 0.631195

Epoch: 2
Loss: 0.6055070130268968
train: 0.779886	val: 0.612821	test: 0.711876

Epoch: 3
Loss: 0.5533214838019002
train: 0.822614	val: 0.671062	test: 0.765258

Epoch: 4
Loss: 0.51186397872706
train: 0.858388	val: 0.667033	test: 0.806295

Epoch: 5
Loss: 0.4872829108531806
train: 0.867511	val: 0.680952	test: 0.803686

Epoch: 6
Loss: 0.45908193580088136
train: 0.872848	val: 0.667399	test: 0.807164

Epoch: 7
Loss: 0.4514099690082462
train: 0.869934	val: 0.604396	test: 0.800383

Epoch: 8
Loss: 0.43654902407578977
train: 0.893804	val: 0.625641	test: 0.822292

Epoch: 9
Loss: 0.42435239089303367
train: 0.903864	val: 0.666667	test: 0.815163

Epoch: 10
Loss: 0.41989437273651536
train: 0.907862	val: 0.673993	test: 0.827160

Epoch: 11
Loss: 0.43145089974807105
train: 0.904366	val: 0.633700	test: 0.825596

Epoch: 12
Loss: 0.40005162005581385
train: 0.916082	val: 0.689377	test: 0.833246

Epoch: 13
Loss: 0.4108646583034091
train: 0.916658	val: 0.705861	test: 0.814293

Epoch: 14
Loss: 0.40144916630407995
train: 0.922474	val: 0.691941	test: 0.828725

Epoch: 15
Loss: 0.39176489162606054
train: 0.923225	val: 0.684615	test: 0.835681

Epoch: 16
Loss: 0.3983502611122116
train: 0.925768	val: 0.694139	test: 0.822118

Epoch: 17
Loss: 0.37381931961053694
train: 0.928054	val: 0.690476	test: 0.821596

Epoch: 18
Loss: 0.37742247506242355
train: 0.919775	val: 0.703297	test: 0.806816

Epoch: 19
Loss: 0.37744449383266276
train: 0.927026	val: 0.686081	test: 0.809598

Epoch: 20
Loss: 0.3853475477458721
train: 0.925985	val: 0.677289	test: 0.804034

Epoch: 21
Loss: 0.3681616546506912
train: 0.933941	val: 0.711722	test: 0.823857

Epoch: 22
Loss: 0.3665442557111513
train: 0.935828	val: 0.705861	test: 0.826117

Epoch: 23
Loss: 0.3626593756637392
train: 0.937908	val: 0.709158	test: 0.818119

Epoch: 24
Loss: 0.3561366463221941
train: 0.938593	val: 0.708425	test: 0.818466

Epoch: 25
Loss: 0.3617407776390804
train: 0.939675	val: 0.706593	test: 0.820205

Epoch: 26
Loss: 0.3526645095961577
train: 0.937597	val: 0.668132	test: 0.808729

Epoch: 27
Loss: 0.351043174560692
train: 0.937289	val: 0.683516	test: 0.809946

Epoch: 28
Loss: 0.35717564668838464
train: 0.940034	val: 0.677289	test: 0.811163

Epoch: 29
Loss: 0.35262293254421334
train: 0.944917	val: 0.689377	test: 0.810294

Epoch: 30
Loss: 0.3532302592125644
train: 0.944883	val: 0.703297	test: 0.791515

Epoch: 31
Loss: 0.35731249599268605
train: 0.945477	val: 0.702198	test: 0.797427

Epoch: 32
Loss: 0.35105762607868246
train: 0.945348	val: 0.700000	test: 0.800904

Epoch: 33
Loss: 0.32965962909035706
train: 0.946447	val: 0.681685	test: 0.805599

Epoch: 34
Loss: 0.3381885029362751
train: 0.946769	val: 0.685348	test: 0.800556

Epoch: 35
Loss: 0.33213853134785715
train: 0.948550	val: 0.695971	test: 0.796905

Epoch: 36
Loss: 0.32550729389591143
train: 0.948813	val: 0.702198	test: 0.803512

Epoch: 37
Loss: 0.3184236010508661
train: 0.948924	val: 0.695604	test: 0.805251

Epoch: 38
Loss: 0.336350402498646
train: 0.953059	val: 0.679121	test: 0.810120

Epoch: 39
Loss: 0.3291056192290374
train: 0.953579	val: 0.684615	test: 0.813945

Epoch: 40
Loss: 0.32836252728784215
train: 0.954629	val: 0.681685	test: 0.820901

Epoch: 41
Loss: 0.31285019324983376
train: 0.956367	val: 0.686081	test: 0.810294

Epoch: 42
Loss: 0.32708213720942403
train: 0.957235	val: 0.672894	test: 0.818988

Epoch: 43
Loss: 0.34187089559369166
train: 0.957948	val: 0.680586	test: 0.810642

Epoch: 44
Loss: 0.3158262065192677
train: 0.954252	val: 0.704762	test: 0.792732

Epoch: 45
Loss: 0.31541825366531134
train: 0.955054	val: 0.709158	test: 0.797774

Epoch: 46
Loss: 0.3150992686117609
train: 0.959195	val: 0.702930	test: 0.807686

Epoch: 47
Loss: 0.3185555247530371
train: 0.961293	val: 0.690476	test: 0.804208

Epoch: 48
Loss: 0.3180721837748013
train: 0.957640	val: 0.683883	test: 0.791167

Epoch: 49
Loss: 0.2971578620577947
train: 0.951712	val: 0.660073	test: 0.795862

Epoch: 50
Loss: 0.30503371983095984
train: 0.959886	val: 0.663004	test: 0.792210

Epoch: 51
Loss: 0.29860751081431924
train: 0.960956	val: 0.682784	test: 0.776734

Epoch: 52
Loss: 0.30026290549023527
train: 0.958653	val: 0.666300	test: 0.753608

Epoch: 53
Loss: 0.29700842000567823
train: 0.959623	val: 0.685348	test: 0.785081

Epoch: 54
Loss: 0.29664664096174875
train: 0.962877	val: 0.657509	test: 0.800556

Epoch: 55
Loss: 0.27732264012429136
train: 0.963279	val: 0.675824	test: 0.802643

Epoch: 56
Loss: 0.28614947099432164
train: 0.965454	val: 0.691209	test: 0.772040

Epoch: 57
Loss: 0.2911210872741868
train: 0.960063	val: 0.664469	test: 0.746653

Epoch: 58
Loss: 0.2961449580647855
train: 0.966955	val: 0.667033	test: 0.783168

Epoch: 59
Loss: 0.28109985268707444
train: 0.962306	val: 0.657875	test: 0.790123

Epoch: 60
Loss: 0.29289758828825807
train: 0.962280	val: 0.651282	test: 0.789428

Epoch: 61
Loss: 0.29045363127328383
train: 0.966136	val: 0.673993	test: 0.800904

Epoch: 62
Loss: 0.3071531649724496
train: 0.967928	val: 0.679487	test: 0.793775

Epoch: 63
Loss: 0.2829769423892918
train: 0.970171	val: 0.677656	test: 0.787167

Epoch: 64
Loss: 0.28597182741681315
train: 0.969649	val: 0.698901	test: 0.783864

Epoch: 65
Loss: 0.28319760561511187
train: 0.967783	val: 0.680952	test: 0.799339

Epoch: 66
Loss: 0.2881562697148783
train: 0.970108	val: 0.687179	test: 0.784385

Epoch: 67
Loss: 0.2764862400259839
train: 0.964706	val: 0.679853	test: 0.764041

Epoch: 68
Loss: 0.2645622174662507
train: 0.970194	val: 0.655311	test: 0.764563

Epoch: 69
Loss: 0.28371837063552874
train: 0.971025	val: 0.653114	test: 0.774126

Epoch: 70
Loss: 0.2841183991312494
train: 0.968065	val: 0.676190	test: 0.775691

Epoch: 71
Loss: 0.27836675693788193
train: 0.970848	val: 0.665201	test: 0.761433

Epoch: 72
Loss: 0.2837985405082064
train: 0.972731	val: 0.677656	test: 0.777082

Epoch: 73
Loss: 0.2625958571587991
train: 0.972486	val: 0.648352	test: 0.767866

Epoch: 74
Loss: 0.2647788336602392
train: 0.974027	val: 0.681685	test: 0.772387

Epoch: 75
Loss: 0.26870855380468867
train: 0.970713	val: 0.699267	test: 0.773083

Epoch: 76
Loss: 0.2590374648565388
train: 0.973462	val: 0.673260	test: 0.749261

Epoch: 77
Loss: 0.2608757816678341
train: 0.971635	val: 0.650916	test: 0.737785

Epoch: 78
Loss: 0.26064574746645264
train: 0.977394	val: 0.651282	test: 0.754651

Epoch: 79
Loss: 0.2580482673435637
train: 0.973770	val: 0.648718	test: 0.763867

Epoch: 80
Loss: 0.25688147827662655
train: 0.977423	val: 0.670330	test: 0.768214

Epoch: 81
Loss: 0.23636205818189984
train: 0.976327	val: 0.686081	test: 0.753956

Epoch: 82
Loss: 0.2321802441050611
train: 0.976164	val: 0.675092	test: 0.750826

Epoch: 83
Loss: 0.26064842453105436
train: 0.977914	val: 0.676557	test: 0.768910

Epoch: 84
Loss: 0.25790169754662423
train: 0.978747	val: 0.658974	test: 0.764389

Epoch: 85
Loss: 0.23960517622758742
train: 0.973188	val: 0.687912	test: 0.741610

Epoch: 86
Loss: 0.25254177072122547
train: 0.979044	val: 0.691941	test: 0.773778

Epoch: 87
Loss: 0.2452276403537541
train: 0.980705	val: 0.658242	test: 0.757086

Epoch: 88
Loss: 0.2381060558809282
train: 0.977900	val: 0.658608	test: 0.749609

Epoch: 89
Loss: 0.24886887833580068
train: 0.978308	val: 0.704396	test: 0.763172

Epoch: 90
Loss: 0.25717138145430063
train: 0.977999	val: 0.687179	test: 0.740915

Epoch: 91
Loss: 0.2436225569578129
train: 0.979269	val: 0.696703	test: 0.723005

Epoch: 92
Loss: 0.23308191904006983
train: 0.976313	val: 0.693407	test: 0.734307

Epoch: 93
Loss: 0.23726047393740934
train: 0.977728	val: 0.678388	test: 0.746305

Epoch: 94
Loss: 0.22623704027887315
train: 0.981535	val: 0.672894	test: 0.745436

Epoch: 95
Loss: 0.22160887799834397
train: 0.982466	val: 0.682784	test: 0.747001

Epoch: 96
Loss: 0.23488048217132432
train: 0.981926	val: 0.694505	test: 0.749609

Epoch: 97
Loss: 0.219200265556589
train: 0.980985	val: 0.690110	test: 0.741784

Epoch: 98
Loss: 0.22091549984897885
train: 0.981550	val: 0.695238	test: 0.742827

Epoch: 99
Loss: 0.24040771258083976
train: 0.984697	val: 0.672894	test: 0.740567

Epoch: 100
Loss: 0.23057929736927188
train: 0.984806	val: 0.679487	test: 0.729612

best train: 0.933941	val: 0.711722	test: 0.823857
end
