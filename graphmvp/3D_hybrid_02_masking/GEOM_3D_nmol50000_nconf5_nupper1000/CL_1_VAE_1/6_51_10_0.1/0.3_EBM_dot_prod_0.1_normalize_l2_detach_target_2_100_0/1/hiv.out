13617833_1
--dataset=hiv --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.3_EBM_dot_prod_0.1_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.25025794861514683
train: 0.763666	val: 0.747875	test: 0.717426

Epoch: 2
Loss: 0.1367798987841256
train: 0.800569	val: 0.766069	test: 0.765272

Epoch: 3
Loss: 0.1325949261235506
train: 0.802882	val: 0.759498	test: 0.723656

Epoch: 4
Loss: 0.12854402908582965
train: 0.814423	val: 0.768188	test: 0.761475

Epoch: 5
Loss: 0.1257821008313614
train: 0.833735	val: 0.822969	test: 0.752122

Epoch: 6
Loss: 0.12340659273962497
train: 0.834147	val: 0.792316	test: 0.712706

Epoch: 7
Loss: 0.12244717191615528
train: 0.816458	val: 0.815075	test: 0.691332

Epoch: 8
Loss: 0.11996611529427581
train: 0.844909	val: 0.806346	test: 0.763954

Epoch: 9
Loss: 0.11865690117908138
train: 0.856094	val: 0.800105	test: 0.769146

Epoch: 10
Loss: 0.11783497978621252
train: 0.855514	val: 0.783871	test: 0.754035

Epoch: 11
Loss: 0.11777320579800109
train: 0.866943	val: 0.801183	test: 0.755853

Epoch: 12
Loss: 0.11659715424109707
train: 0.865523	val: 0.778014	test: 0.760469

Epoch: 13
Loss: 0.11443819815937079
train: 0.868083	val: 0.779165	test: 0.763000

Epoch: 14
Loss: 0.11289843273261117
train: 0.862241	val: 0.765356	test: 0.753404

Epoch: 15
Loss: 0.11437108870549334
train: 0.875489	val: 0.818890	test: 0.774254

Epoch: 16
Loss: 0.11271587305274777
train: 0.870749	val: 0.784073	test: 0.752149

Epoch: 17
Loss: 0.11165776429454155
train: 0.885359	val: 0.801318	test: 0.761958

Epoch: 18
Loss: 0.11069721631553407
train: 0.876087	val: 0.794309	test: 0.753460

Epoch: 19
Loss: 0.11043309653426686
train: 0.883341	val: 0.792046	test: 0.769441

Epoch: 20
Loss: 0.10857218351901886
train: 0.892698	val: 0.799867	test: 0.778349

Epoch: 21
Loss: 0.10812309999121107
train: 0.890733	val: 0.798198	test: 0.749879

Epoch: 22
Loss: 0.10714234599864776
train: 0.896530	val: 0.814855	test: 0.779841

Epoch: 23
Loss: 0.1073583313118184
train: 0.891840	val: 0.813798	test: 0.772860

Epoch: 24
Loss: 0.10629344347943159
train: 0.899918	val: 0.811784	test: 0.758510

Epoch: 25
Loss: 0.1075826373129616
train: 0.906468	val: 0.816174	test: 0.780582

Epoch: 26
Loss: 0.10427768687217762
train: 0.904776	val: 0.812540	test: 0.765140

Epoch: 27
Loss: 0.10336768502164888
train: 0.875458	val: 0.805755	test: 0.666255

Epoch: 28
Loss: 0.10449485427831434
train: 0.901137	val: 0.808902	test: 0.751938

Epoch: 29
Loss: 0.10379124990785976
train: 0.905824	val: 0.812852	test: 0.766390

Epoch: 30
Loss: 0.10391941728083467
train: 0.913240	val: 0.819527	test: 0.768881

Epoch: 31
Loss: 0.10236347205007762
train: 0.914846	val: 0.804074	test: 0.780199

Epoch: 32
Loss: 0.10180450363963062
train: 0.909103	val: 0.808287	test: 0.759346

Epoch: 33
Loss: 0.1013324929929198
train: 0.921647	val: 0.826903	test: 0.740460

Epoch: 34
Loss: 0.10148670965362165
train: 0.921643	val: 0.808324	test: 0.778445

Epoch: 35
Loss: 0.10210804738640356
train: 0.918893	val: 0.800656	test: 0.765022

Epoch: 36
Loss: 0.09819127298768088
train: 0.921588	val: 0.808149	test: 0.766699

Epoch: 37
Loss: 0.10012789788005369
train: 0.919363	val: 0.805412	test: 0.782682

Epoch: 38
Loss: 0.09809799593889845
train: 0.922826	val: 0.801642	test: 0.767460

Epoch: 39
Loss: 0.0990936032517752
train: 0.929170	val: 0.799187	test: 0.758348

Epoch: 40
Loss: 0.09814402606743337
train: 0.914297	val: 0.794692	test: 0.776906

Epoch: 41
Loss: 0.098020619913548
train: 0.924780	val: 0.806385	test: 0.783379

Epoch: 42
Loss: 0.09800440529348999
train: 0.934323	val: 0.803222	test: 0.755018

Epoch: 43
Loss: 0.09632488389467497
train: 0.927433	val: 0.809753	test: 0.742716

Epoch: 44
Loss: 0.09612645993534123
train: 0.932944	val: 0.808731	test: 0.770247

Epoch: 45
Loss: 0.09609882478692637
train: 0.928907	val: 0.793648	test: 0.757311

Epoch: 46
Loss: 0.09508372366923377
train: 0.931706	val: 0.787377	test: 0.775612

Epoch: 47
Loss: 0.09473603631204498
train: 0.935460	val: 0.815550	test: 0.772319

Epoch: 48
Loss: 0.09403167476300123
train: 0.941892	val: 0.796896	test: 0.756900

Epoch: 49
Loss: 0.09485636684044585
train: 0.939822	val: 0.784079	test: 0.753578

Epoch: 50
Loss: 0.09430839655030064
train: 0.935955	val: 0.820140	test: 0.783982

Epoch: 51
Loss: 0.09374829263664577
train: 0.940351	val: 0.808474	test: 0.763897

Epoch: 52
Loss: 0.09126357281885547
train: 0.946238	val: 0.821649	test: 0.779076

Epoch: 53
Loss: 0.09246646360295267
train: 0.941320	val: 0.791964	test: 0.755633

Epoch: 54
Loss: 0.09143144847219659
train: 0.948852	val: 0.821716	test: 0.764435

Epoch: 55
Loss: 0.09155738010028573
train: 0.939402	val: 0.812540	test: 0.766556

Epoch: 56
Loss: 0.09023971390182252
train: 0.950874	val: 0.807151	test: 0.787246

Epoch: 57
Loss: 0.09039271581756497
train: 0.949921	val: 0.783430	test: 0.761376

Epoch: 58
Loss: 0.09118469071257257
train: 0.949813	val: 0.803440	test: 0.779745

Epoch: 59
Loss: 0.08965874723324271
train: 0.953129	val: 0.789989	test: 0.762471

Epoch: 60
Loss: 0.08863376945244988
train: 0.950268	val: 0.805234	test: 0.749557

Epoch: 61
Loss: 0.08868980827775613
train: 0.956441	val: 0.811079	test: 0.760465

Epoch: 62
Loss: 0.08662172272134393
train: 0.949366	val: 0.799122	test: 0.758472

Epoch: 63
Loss: 0.08907252918473846
train: 0.951794	val: 0.826585	test: 0.758510

Epoch: 64
Loss: 0.09055985050197608
train: 0.958272	val: 0.808467	test: 0.764928

Epoch: 65
Loss: 0.08847573929589686
train: 0.960288	val: 0.821961	test: 0.760096

Epoch: 66
Loss: 0.08654335119866421
train: 0.960886	val: 0.811300	test: 0.764625

Epoch: 67
Loss: 0.08517492246256804
train: 0.958056	val: 0.792744	test: 0.766247

Epoch: 68
Loss: 0.08722104936201149
train: 0.958824	val: 0.803620	test: 0.751714

Epoch: 69
Loss: 0.08513228535373145
train: 0.956095	val: 0.776758	test: 0.777686

Epoch: 70
Loss: 0.08475809764559163
train: 0.962342	val: 0.807720	test: 0.767827

Epoch: 71
Loss: 0.08568633137623091
train: 0.962537	val: 0.805568	test: 0.753688

Epoch: 72
Loss: 0.08562152646898168
train: 0.967476	val: 0.803219	test: 0.760820

Epoch: 73
Loss: 0.08346464081145721
train: 0.965325	val: 0.807693	test: 0.768165

Epoch: 74
Loss: 0.0842999773300957
train: 0.967247	val: 0.812947	test: 0.763261

Epoch: 75
Loss: 0.08415674683438899
train: 0.964760	val: 0.808259	test: 0.763499

Epoch: 76
Loss: 0.08354565106589325
train: 0.961916	val: 0.809297	test: 0.760971

Epoch: 77
Loss: 0.0809491133660852
train: 0.967929	val: 0.807200	test: 0.770264

Epoch: 78
Loss: 0.08229260327873379
train: 0.968822	val: 0.811881	test: 0.774853

Epoch: 79
Loss: 0.08200643197177968
train: 0.969491	val: 0.807579	test: 0.753095

Epoch: 80
Loss: 0.08124918042350539
train: 0.967224	val: 0.807625	test: 0.755895

Epoch: 81
Loss: 0.08026009279837212
train: 0.964426	val: 0.817016	test: 0.761452

Epoch: 82
Loss: 0.08007232661319577
train: 0.970922	val: 0.809404	test: 0.770691

Epoch: 83
Loss: 0.08030563505255964
train: 0.970070	val: 0.824778	test: 0.756185

Epoch: 84
Loss: 0.08165888323316285
train: 0.970519	val: 0.823581	test: 0.780660

Epoch: 85
Loss: 0.07997947178460935
train: 0.969143	val: 0.802442	test: 0.749043

Epoch: 86
Loss: 0.07902796322290197
train: 0.970310	val: 0.798366	test: 0.779766

Epoch: 87
Loss: 0.07935264996935902
train: 0.971721	val: 0.815837	test: 0.748792

Epoch: 88
Loss: 0.07840809824769089
train: 0.975154	val: 0.797206	test: 0.777087

Epoch: 89
Loss: 0.07846787786123306
train: 0.973406	val: 0.801878	test: 0.765919

Epoch: 90
Loss: 0.07818158286935604
train: 0.973640	val: 0.809276	test: 0.761106

Epoch: 91
Loss: 0.07824189242087655
train: 0.976690	val: 0.814160	test: 0.773138

Epoch: 92
Loss: 0.07763736216710063
train: 0.964737	val: 0.806272	test: 0.748562

Epoch: 93
Loss: 0.07726531590917904
train: 0.976043	val: 0.814294	test: 0.766934

Epoch: 94
Loss: 0.07604310083981242
train: 0.972720	val: 0.816732	test: 0.769445

Epoch: 95
Loss: 0.07715455947371351
train: 0.979311	val: 0.799787	test: 0.748777

Epoch: 96
Loss: 0.07477935029124681
train: 0.977727	val: 0.798568	test: 0.778093

Epoch: 97
Loss: 0.07643078485732836
train: 0.977160	val: 0.815225	test: 0.740406

Epoch: 98
Loss: 0.07550464904855138
train: 0.977825	val: 0.794820	test: 0.739678

Epoch: 99
Loss: 0.07471633518283828
train: 0.979489	val: 0.808642	test: 0.754559

Epoch: 100
Loss: 0.07583904592154297
train: 0.980565	val: 0.804536	test: 0.762052

best train: 0.921647	val: 0.826903	test: 0.740460
end
