13984932_1
--dataset=bace --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='bace', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: bace
Data: Data(edge_attr=[111536, 2], edge_index=[2, 111536], fold=[1513], id=[1513], x=[51577, 2], y=[1513])
MoleculeDataset(1513)
split via scaffold
Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.6530274930847416
train: 0.706538	val: 0.529304	test: 0.653278
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 2
Loss: 0.5999408148224024
train: 0.761096	val: 0.567033	test: 0.735350
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 3
Loss: 0.5511909220187341
train: 0.808330	val: 0.600000	test: 0.788732
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 4
Loss: 0.5079554851507819
train: 0.843716	val: 0.628938	test: 0.777778
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 5
Loss: 0.4802972066542974
train: 0.866684	val: 0.651648	test: 0.790471
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 6
Loss: 0.458448653235512
train: 0.877032	val: 0.661905	test: 0.803165
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 7
Loss: 0.44000705275545293
train: 0.888536	val: 0.684982	test: 0.804556
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 8
Loss: 0.4513355224942348
train: 0.897842	val: 0.686447	test: 0.786994
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 9
Loss: 0.42051448941638797
train: 0.897209	val: 0.700366	test: 0.787863
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 10
Loss: 0.4275022063802706
train: 0.903550	val: 0.689011	test: 0.786124
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 11
Loss: 0.4207735786138887
train: 0.903405	val: 0.668864	test: 0.797427
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 12
Loss: 0.4051560313933672
train: 0.911087	val: 0.669963	test: 0.806642
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 13
Loss: 0.4009588582146317
train: 0.914512	val: 0.678388	test: 0.787863
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 14
Loss: 0.39743851527912255
train: 0.914081	val: 0.663004	test: 0.787167
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 15
Loss: 0.3942764209858766
train: 0.921130	val: 0.671795	test: 0.791341
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 16
Loss: 0.371932927153679
train: 0.922708	val: 0.689744	test: 0.786472
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 17
Loss: 0.38161723046363727
train: 0.924021	val: 0.681685	test: 0.790123
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 18
Loss: 0.3836450252689428
train: 0.925739	val: 0.668132	test: 0.783864
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 19
Loss: 0.3620756898044265
train: 0.928145	val: 0.665934	test: 0.800556
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 20
Loss: 0.37948587289885083
train: 0.929555	val: 0.666667	test: 0.796905
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 21
Loss: 0.3728337751400628
train: 0.931418	val: 0.672161	test: 0.779517
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 22
Loss: 0.36475176692490774
train: 0.932477	val: 0.664835	test: 0.767345
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 23
Loss: 0.36921506486566713
train: 0.936929	val: 0.689744	test: 0.765780
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 24
Loss: 0.36575246545376655
train: 0.938031	val: 0.694505	test: 0.784385
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 25
Loss: 0.36391896342051694
train: 0.937098	val: 0.679121	test: 0.809598
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 26
Loss: 0.35805984690395143
train: 0.936484	val: 0.673993	test: 0.807512
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 27
Loss: 0.36763416037879654
train: 0.935437	val: 0.676557	test: 0.800209
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 28
Loss: 0.348993860931347
train: 0.935839	val: 0.657875	test: 0.785776
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 29
Loss: 0.34947508181848286
train: 0.941233	val: 0.690476	test: 0.796731
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 30
Loss: 0.36173103962835185
train: 0.938236	val: 0.702564	test: 0.803860
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 31
Loss: 0.35055843170193307
train: 0.941889	val: 0.671062	test: 0.810120
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 32
Loss: 0.34619993390977405
train: 0.942192	val: 0.683883	test: 0.793253
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 33
Loss: 0.3381203818292443
train: 0.942389	val: 0.684982	test: 0.776908
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 34
Loss: 0.3475407045861952
train: 0.945277	val: 0.663736	test: 0.783342
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 35
Loss: 0.3445023193443024
train: 0.946204	val: 0.684982	test: 0.796383
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 36
Loss: 0.3280626104679444
train: 0.946687	val: 0.697070	test: 0.793775
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 37
Loss: 0.33139742102107056
train: 0.947868	val: 0.691575	test: 0.776039
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 38
Loss: 0.3374699171676853
train: 0.948579	val: 0.699267	test: 0.777604
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 39
Loss: 0.3545607321428505
train: 0.946304	val: 0.705495	test: 0.779690
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 40
Loss: 0.32517977700747663
train: 0.948256	val: 0.686813	test: 0.751869
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 41
Loss: 0.3245175420216687
train: 0.946615	val: 0.672894	test: 0.751695
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 42
Loss: 0.3211839421800969
train: 0.951133	val: 0.697070	test: 0.772561
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 43
Loss: 0.33068219402014476
train: 0.954775	val: 0.682418	test: 0.782646
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 44
Loss: 0.32364409416101886
train: 0.954658	val: 0.677656	test: 0.758129
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 45
Loss: 0.3221691503420471
train: 0.955497	val: 0.690842	test: 0.778299
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 46
Loss: 0.3097077239101185
train: 0.953893	val: 0.660440	test: 0.775343
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 47
Loss: 0.3158228634376516
train: 0.954732	val: 0.676923	test: 0.771344
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 48
Loss: 0.33048607228577936
train: 0.954743	val: 0.689377	test: 0.776387
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 49
Loss: 0.30582357103333646
train: 0.954478	val: 0.708059	test: 0.779517
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 50
Loss: 0.3046273177269409
train: 0.951153	val: 0.691209	test: 0.786994
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 51
Loss: 0.30595671883336834
train: 0.959763	val: 0.684615	test: 0.780560
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 52
Loss: 0.2999240702836675
train: 0.959238	val: 0.672894	test: 0.760216
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 53
Loss: 0.3253397684612475
train: 0.960442	val: 0.691209	test: 0.767345
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 54
Loss: 0.3034520842983529
train: 0.962003	val: 0.693407	test: 0.772735
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 55
Loss: 0.3054481841723166
train: 0.961430	val: 0.693040	test: 0.770649
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 56
Loss: 0.28887669354980594
train: 0.961204	val: 0.678388	test: 0.778995
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 57
Loss: 0.29889920344730614
train: 0.961641	val: 0.667033	test: 0.767866
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 58
Loss: 0.2852673210041622
train: 0.963453	val: 0.676557	test: 0.768736
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 59
Loss: 0.28985153131880675
train: 0.960071	val: 0.667766	test: 0.752391
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 60
Loss: 0.29896253718545474
train: 0.960645	val: 0.673626	test: 0.747174
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 61
Loss: 0.29591189030801934
train: 0.963105	val: 0.681319	test: 0.757433
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 62
Loss: 0.2998005140544745
train: 0.964426	val: 0.672527	test: 0.761954
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 63
Loss: 0.2946925454552713
train: 0.964635	val: 0.656777	test: 0.744392
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 64
Loss: 0.2876395609977547
train: 0.956695	val: 0.660440	test: 0.739697
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 65
Loss: 0.2945731360485365
train: 0.955265	val: 0.659707	test: 0.756738
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 66
Loss: 0.301853540890508
train: 0.964649	val: 0.682418	test: 0.764389
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 67
Loss: 0.28049466236664433
train: 0.968139	val: 0.694505	test: 0.771866
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 68
Loss: 0.27351274940638237
train: 0.969618	val: 0.695971	test: 0.761954
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 69
Loss: 0.28871620072336107
train: 0.968025	val: 0.694139	test: 0.742653
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 70
Loss: 0.28563360767006274
train: 0.964700	val: 0.699267	test: 0.759346
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 71
Loss: 0.2708183837245597
train: 0.969261	val: 0.684249	test: 0.749609
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 72
Loss: 0.2838110442620654
train: 0.970448	val: 0.693040	test: 0.746305
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 73
Loss: 0.27785735850110915
train: 0.970696	val: 0.685714	test: 0.751000
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 74
Loss: 0.26834866361675785
train: 0.971216	val: 0.683516	test: 0.760042
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 75
Loss: 0.2857256180700743
train: 0.970294	val: 0.670330	test: 0.744740
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 76
Loss: 0.260256018075807
train: 0.970183	val: 0.653846	test: 0.741958
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 77
Loss: 0.27753583061072723
train: 0.971341	val: 0.658974	test: 0.753260
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 78
Loss: 0.26179386293457324
train: 0.972974	val: 0.683150	test: 0.740393
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 79
Loss: 0.2638838596150553
train: 0.971898	val: 0.680952	test: 0.738132
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 80
Loss: 0.2548029612002845
train: 0.973770	val: 0.684249	test: 0.751869
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 81
Loss: 0.24750630477586605
train: 0.972149	val: 0.681685	test: 0.759346
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 82
Loss: 0.25927066472507987
train: 0.971316	val: 0.675092	test: 0.735350
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 83
Loss: 0.2731181763352827
train: 0.973154	val: 0.687179	test: 0.730308
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 84
Loss: 0.2569756352955356
train: 0.972834	val: 0.674725	test: 0.755521
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 85
Loss: 0.25527233904787644
train: 0.974489	val: 0.679853	test: 0.741610
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 86
Loss: 0.2583873774987923
train: 0.975853	val: 0.663370	test: 0.728221
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 87
Loss: 0.265593328366214
train: 0.966495	val: 0.650183	test: 0.724222
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 88
Loss: 0.25748612232895296
train: 0.975174	val: 0.641758	test: 0.758477
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 89
Loss: 0.24637338581195004
train: 0.976027	val: 0.676923	test: 0.761433
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 90
Loss: 0.2723646972190838
train: 0.978228	val: 0.677656	test: 0.753956
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 91
Loss: 0.2583302774217128
train: 0.975388	val: 0.694505	test: 0.735350
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 92
Loss: 0.24723123287485285
train: 0.976935	val: 0.675824	test: 0.729786
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 93
Loss: 0.24520495592178157
train: 0.976056	val: 0.668132	test: 0.729960
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 94
Loss: 0.23820931281902274
train: 0.979755	val: 0.661538	test: 0.744740
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 95
Loss: 0.23164878307038358
train: 0.979555	val: 0.676190	test: 0.737785
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 96
Loss: 0.23963868144741793
train: 0.979646	val: 0.685714	test: 0.728395
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 97
Loss: 0.24119383406299896
train: 0.976658	val: 0.682418	test: 0.716919
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 98
Loss: 0.23742522879831224
train: 0.980799	val: 0.674359	test: 0.741610
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 99
Loss: 0.24907176997672695
train: 0.981929	val: 0.683516	test: 0.741089
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 100
Loss: 0.2322361695698591
train: 0.980294	val: 0.692674	test: 0.720918
acc train: 0.000000	val: 0.000000	test: 0.000000

best train: 0.954478	val: 0.708059	test: 0.779517
best ACC train: 0.000000	val: 0.000000	test: 0.000000
end
