13984932_1
--dataset=sider --runseed=1 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=1, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.699152961660071
train: 0.534367	val: 0.511682	test: 0.491364
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 2
Loss: 0.6591318405436797
train: 0.551401	val: 0.517738	test: 0.505128
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 3
Loss: 0.627666695883353
train: 0.559877	val: 0.505679	test: 0.519398
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 4
Loss: 0.5947952173929323
train: 0.575918	val: 0.504642	test: 0.530407
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 5
Loss: 0.5715384454175695
train: 0.607224	val: 0.520660	test: 0.551848
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 6
Loss: 0.5541788094518743
train: 0.634125	val: 0.554413	test: 0.576648
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 7
Loss: 0.5413447558244362
train: 0.648743	val: 0.570722	test: 0.588953
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 8
Loss: 0.5325826273886018
train: 0.659387	val: 0.575158	test: 0.585749
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 9
Loss: 0.5220431140316196
train: 0.668625	val: 0.573549	test: 0.578452
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 10
Loss: 0.5129254748906351
train: 0.678400	val: 0.575061	test: 0.578613
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 11
Loss: 0.5065468602027006
train: 0.687338	val: 0.588782	test: 0.581880
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 12
Loss: 0.5041165823262042
train: 0.698103	val: 0.585618	test: 0.588479
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 13
Loss: 0.49492938653093504
train: 0.702437	val: 0.589970	test: 0.583849
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 14
Loss: 0.4906745306346945
train: 0.708632	val: 0.585911	test: 0.590633
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 15
Loss: 0.48549536315768693
train: 0.712139	val: 0.591225	test: 0.597853
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 16
Loss: 0.48559057549679574
train: 0.718337	val: 0.598714	test: 0.600716
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 17
Loss: 0.47955069102047226
train: 0.724487	val: 0.594724	test: 0.602580
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 18
Loss: 0.47736658563053397
train: 0.728360	val: 0.598847	test: 0.591241
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 19
Loss: 0.47662413273916143
train: 0.732619	val: 0.601956	test: 0.597840
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 20
Loss: 0.47365395061882437
train: 0.735676	val: 0.597294	test: 0.602325
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 21
Loss: 0.47234204786015555
train: 0.735734	val: 0.598520	test: 0.604753
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 22
Loss: 0.46870258668330467
train: 0.744524	val: 0.606696	test: 0.602413
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 23
Loss: 0.46708424150400407
train: 0.747107	val: 0.603495	test: 0.603260
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 24
Loss: 0.465016935055909
train: 0.749737	val: 0.602128	test: 0.603942
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 25
Loss: 0.4604607212083721
train: 0.756194	val: 0.606141	test: 0.598731
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 26
Loss: 0.4618134465604727
train: 0.760959	val: 0.603631	test: 0.599102
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 27
Loss: 0.4600466598920622
train: 0.760124	val: 0.601448	test: 0.599356
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 28
Loss: 0.45792449561789655
train: 0.764329	val: 0.604253	test: 0.614824
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 29
Loss: 0.4516361856451588
train: 0.760579	val: 0.597849	test: 0.620275
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 30
Loss: 0.45758336649232756
train: 0.768826	val: 0.601628	test: 0.608811
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 31
Loss: 0.4537048527478074
train: 0.771593	val: 0.603641	test: 0.613189
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 32
Loss: 0.45629540496185006
train: 0.774422	val: 0.601375	test: 0.614411
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 33
Loss: 0.45226049760647297
train: 0.770652	val: 0.604442	test: 0.602432
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 34
Loss: 0.4536830673845687
train: 0.782788	val: 0.610584	test: 0.610842
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 35
Loss: 0.4493066053996877
train: 0.785054	val: 0.603188	test: 0.612658
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 36
Loss: 0.45054368578400866
train: 0.788289	val: 0.606250	test: 0.614783
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 37
Loss: 0.4444050909430364
train: 0.790798	val: 0.616849	test: 0.613689
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 38
Loss: 0.4509693805748819
train: 0.790401	val: 0.613037	test: 0.611264
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 39
Loss: 0.4432427132070199
train: 0.791113	val: 0.609141	test: 0.610276
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 40
Loss: 0.44322361489400536
train: 0.796427	val: 0.611184	test: 0.616690
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 41
Loss: 0.4363657263528398
train: 0.797911	val: 0.604764	test: 0.621300
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 42
Loss: 0.4438588050180807
train: 0.800939	val: 0.604953	test: 0.606565
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 43
Loss: 0.44025742837576304
train: 0.801948	val: 0.612779	test: 0.604637
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 44
Loss: 0.43632793763409045
train: 0.803543	val: 0.609909	test: 0.611789
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 45
Loss: 0.43345493846319283
train: 0.805115	val: 0.610613	test: 0.620230
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 46
Loss: 0.4405574867709351
train: 0.807600	val: 0.620038	test: 0.616734
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 47
Loss: 0.43688896658501075
train: 0.811352	val: 0.624203	test: 0.614259
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 48
Loss: 0.424755558646514
train: 0.806614	val: 0.611214	test: 0.626478
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 49
Loss: 0.42736734842606017
train: 0.815529	val: 0.612373	test: 0.620793
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 50
Loss: 0.43225230826843763
train: 0.817411	val: 0.613866	test: 0.615501
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 51
Loss: 0.424108172370973
train: 0.814217	val: 0.609619	test: 0.621381
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 52
Loss: 0.4250420726355343
train: 0.819949	val: 0.620338	test: 0.611656
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 53
Loss: 0.42607464283513474
train: 0.821749	val: 0.610312	test: 0.618462
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 54
Loss: 0.4209173323814569
train: 0.821808	val: 0.608299	test: 0.606224
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 55
Loss: 0.4209863645512081
train: 0.829679	val: 0.603785	test: 0.600049
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 56
Loss: 0.4201734489045294
train: 0.829232	val: 0.597021	test: 0.611754
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 57
Loss: 0.423643437167851
train: 0.831945	val: 0.600722	test: 0.614976
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 58
Loss: 0.41883221269805493
train: 0.832684	val: 0.614903	test: 0.621728
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 59
Loss: 0.4270782860426663
train: 0.828477	val: 0.611540	test: 0.628067
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 60
Loss: 0.4186464553630554
train: 0.831845	val: 0.622148	test: 0.622227
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 61
Loss: 0.4188740323450667
train: 0.834072	val: 0.608797	test: 0.623431
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 62
Loss: 0.41738729823588716
train: 0.835191	val: 0.621544	test: 0.614946
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 63
Loss: 0.4171340008710155
train: 0.840647	val: 0.618799	test: 0.621979
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 64
Loss: 0.41243237126510496
train: 0.840278	val: 0.607464	test: 0.611686
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 65
Loss: 0.4142559020811338
train: 0.841093	val: 0.619437	test: 0.620014
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 66
Loss: 0.4091624754527551
train: 0.845621	val: 0.618454	test: 0.603869
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 67
Loss: 0.41056740862094776
train: 0.849271	val: 0.619543	test: 0.606758
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 68
Loss: 0.4068906713007223
train: 0.844560	val: 0.618956	test: 0.619508
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 69
Loss: 0.40610150804555334
train: 0.849561	val: 0.608115	test: 0.612135
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 70
Loss: 0.40867111797917016
train: 0.847505	val: 0.596732	test: 0.608135
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 71
Loss: 0.4071363166786729
train: 0.848679	val: 0.616148	test: 0.603841
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 72
Loss: 0.40444580470525315
train: 0.845709	val: 0.618385	test: 0.611663
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 73
Loss: 0.40475259184519113
train: 0.854669	val: 0.616715	test: 0.621096
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 74
Loss: 0.40432581149734775
train: 0.853651	val: 0.615822	test: 0.634013
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 75
Loss: 0.40199736039660144
train: 0.854083	val: 0.624453	test: 0.614162
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 76
Loss: 0.3990129307498839
train: 0.853622	val: 0.622792	test: 0.613593
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 77
Loss: 0.4028851579459964
train: 0.858631	val: 0.612078	test: 0.613942
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 78
Loss: 0.39869514171194387
train: 0.858735	val: 0.592319	test: 0.605065
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 79
Loss: 0.3924791338166358
train: 0.860910	val: 0.603125	test: 0.613857
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 80
Loss: 0.40108584819591264
train: 0.862348	val: 0.616969	test: 0.617349
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 81
Loss: 0.39450627673098765
train: 0.862476	val: 0.613635	test: 0.620973
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 82
Loss: 0.38878564356859835
train: 0.863281	val: 0.604874	test: 0.614124
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 83
Loss: 0.39386235188814284
train: 0.860736	val: 0.616151	test: 0.595831
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 84
Loss: 0.3925685662327089
train: 0.863822	val: 0.610440	test: 0.590191
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 85
Loss: 0.39220119125718655
train: 0.868216	val: 0.604883	test: 0.606861
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 86
Loss: 0.38832587939478935
train: 0.866391	val: 0.605758	test: 0.633190
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 87
Loss: 0.3868158407986907
train: 0.869995	val: 0.611379	test: 0.619227
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 88
Loss: 0.3901011878823609
train: 0.874386	val: 0.604609	test: 0.609601
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 89
Loss: 0.3894371514822397
train: 0.869954	val: 0.601923	test: 0.596669
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 90
Loss: 0.3868673337784323
train: 0.864502	val: 0.617101	test: 0.600700
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 91
Loss: 0.3852139850506261
train: 0.871482	val: 0.622235	test: 0.610246
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 92
Loss: 0.3818370314362693
train: 0.873178	val: 0.606535	test: 0.595200
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 93
Loss: 0.38201538272917046
train: 0.872682	val: 0.610118	test: 0.595318
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 94
Loss: 0.3859468062209437
train: 0.875192	val: 0.631492	test: 0.610910
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 95
Loss: 0.38049063149491247
train: 0.879435	val: 0.625317	test: 0.604870
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 96
Loss: 0.37807785848929953
train: 0.875235	val: 0.611719	test: 0.606845
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 97
Loss: 0.3793178991434205
train: 0.884263	val: 0.613068	test: 0.595526
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 98
Loss: 0.3878661098024941
train: 0.881206	val: 0.605517	test: 0.609297
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 99
Loss: 0.3730746533086948
train: 0.880213	val: 0.611629	test: 0.617239
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 100
Loss: 0.37245409816999475
train: 0.882806	val: 0.609344	test: 0.593103
acc train: 0.000000	val: 0.000000	test: 0.000000

best train: 0.875192	val: 0.631492	test: 0.610910
best ACC train: 0.000000	val: 0.000000	test: 0.000000
end
