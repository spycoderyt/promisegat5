9087242_2
--dataset=hiv --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=True, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2661195881687195
train: 0.771504	val: 0.757208	test: 0.741569

Epoch: 2
Loss: 0.13855385735148137
train: 0.783852	val: 0.788969	test: 0.751859

Epoch: 3
Loss: 0.13355077980088909
train: 0.791841	val: 0.781317	test: 0.737077

Epoch: 4
Loss: 0.12873369593579895
train: 0.818372	val: 0.812362	test: 0.760172

Epoch: 5
Loss: 0.1274487747161194
train: 0.825772	val: 0.794612	test: 0.748645

Epoch: 6
Loss: 0.12553055890189616
train: 0.817864	val: 0.813495	test: 0.753249

Epoch: 7
Loss: 0.12396984698539641
train: 0.825222	val: 0.799839	test: 0.742764

Epoch: 8
Loss: 0.12169006051724439
train: 0.835689	val: 0.773378	test: 0.741013

Epoch: 9
Loss: 0.11986875419713518
train: 0.845280	val: 0.840752	test: 0.742855

Epoch: 10
Loss: 0.11818791594869336
train: 0.846091	val: 0.795846	test: 0.757736

Epoch: 11
Loss: 0.11683672817981894
train: 0.855741	val: 0.815213	test: 0.763006

Epoch: 12
Loss: 0.11713341685679883
train: 0.854538	val: 0.799867	test: 0.775716

Epoch: 13
Loss: 0.11485195510663489
train: 0.866031	val: 0.800807	test: 0.756197

Epoch: 14
Loss: 0.11424711960805098
train: 0.867134	val: 0.800317	test: 0.773509

Epoch: 15
Loss: 0.11399428795125799
train: 0.876487	val: 0.820094	test: 0.759254

Epoch: 16
Loss: 0.11221756302027155
train: 0.880526	val: 0.822773	test: 0.761730

Epoch: 17
Loss: 0.11094536633096279
train: 0.875103	val: 0.818364	test: 0.763260

Epoch: 18
Loss: 0.11114800585447568
train: 0.879369	val: 0.804110	test: 0.773812

Epoch: 19
Loss: 0.1107992604631504
train: 0.877769	val: 0.799413	test: 0.767493

Epoch: 20
Loss: 0.1104833827302053
train: 0.882726	val: 0.816413	test: 0.746299

Epoch: 21
Loss: 0.10971920411996644
train: 0.883690	val: 0.805139	test: 0.761486

Epoch: 22
Loss: 0.10916330851876514
train: 0.883869	val: 0.808670	test: 0.728577

Epoch: 23
Loss: 0.10871128809713407
train: 0.894915	val: 0.798400	test: 0.757411

Epoch: 24
Loss: 0.1070839335415563
train: 0.891779	val: 0.819873	test: 0.756504

Epoch: 25
Loss: 0.10534366048320447
train: 0.899334	val: 0.788828	test: 0.769660

Epoch: 26
Loss: 0.1055375694093053
train: 0.898892	val: 0.799677	test: 0.773200

Epoch: 27
Loss: 0.10465297837972179
train: 0.906241	val: 0.807564	test: 0.779654

Epoch: 28
Loss: 0.10406708156495727
train: 0.909169	val: 0.820002	test: 0.764845

Epoch: 29
Loss: 0.10353921142654497
train: 0.907813	val: 0.786544	test: 0.769501

Epoch: 30
Loss: 0.10312676068172225
train: 0.910156	val: 0.821061	test: 0.764955

Epoch: 31
Loss: 0.10355266026610044
train: 0.886095	val: 0.769716	test: 0.739331

Epoch: 32
Loss: 0.10357590499850358
train: 0.909362	val: 0.801991	test: 0.781027

Epoch: 33
Loss: 0.10163350583287512
train: 0.915880	val: 0.811480	test: 0.786591

Epoch: 34
Loss: 0.10053851071928949
train: 0.911247	val: 0.764201	test: 0.776572

Epoch: 35
Loss: 0.1020415440338573
train: 0.917950	val: 0.783897	test: 0.780446

Epoch: 36
Loss: 0.10024736837990914
train: 0.917591	val: 0.805292	test: 0.765181

Epoch: 37
Loss: 0.09956695076983979
train: 0.919521	val: 0.787212	test: 0.766697

Epoch: 38
Loss: 0.09926354561130084
train: 0.923811	val: 0.817267	test: 0.768308

Epoch: 39
Loss: 0.09893270468815178
train: 0.925895	val: 0.802239	test: 0.756324

Epoch: 40
Loss: 0.09746316072658424
train: 0.928459	val: 0.819852	test: 0.775453

Epoch: 41
Loss: 0.09719179596953004
train: 0.929268	val: 0.777821	test: 0.770179

Epoch: 42
Loss: 0.09667430536418306
train: 0.928171	val: 0.794624	test: 0.770393

Epoch: 43
Loss: 0.09535325955742552
train: 0.933154	val: 0.784563	test: 0.765494

Epoch: 44
Loss: 0.09641552424534186
train: 0.927124	val: 0.785955	test: 0.770503

Epoch: 45
Loss: 0.09645978650872714
train: 0.936940	val: 0.773329	test: 0.750096

Epoch: 46
Loss: 0.09464013118963624
train: 0.933961	val: 0.796771	test: 0.768727

Epoch: 47
Loss: 0.09440945589298132
train: 0.940615	val: 0.795075	test: 0.770034

Epoch: 48
Loss: 0.09337678166033699
train: 0.938137	val: 0.803994	test: 0.778248

Epoch: 49
Loss: 0.09422978289006907
train: 0.939478	val: 0.803431	test: 0.762827

Epoch: 50
Loss: 0.0933297580054772
train: 0.942822	val: 0.801336	test: 0.776991

Epoch: 51
Loss: 0.09363084153952769
train: 0.940772	val: 0.784551	test: 0.770643

Epoch: 52
Loss: 0.09182017810181275
train: 0.944263	val: 0.800350	test: 0.786805

Epoch: 53
Loss: 0.09363180906794154
train: 0.944022	val: 0.798588	test: 0.789069

Epoch: 54
Loss: 0.09138478517240628
train: 0.943787	val: 0.773705	test: 0.767151

Epoch: 55
Loss: 0.09102831686269833
train: 0.945540	val: 0.792169	test: 0.760438

Epoch: 56
Loss: 0.09126992922835403
train: 0.948563	val: 0.790243	test: 0.777543

Epoch: 57
Loss: 0.09000395273172823
train: 0.951194	val: 0.787248	test: 0.763987

Epoch: 58
Loss: 0.09007435899361338
train: 0.946612	val: 0.808363	test: 0.774387

Epoch: 59
Loss: 0.08946968696346688
train: 0.949689	val: 0.805721	test: 0.779044

Epoch: 60
Loss: 0.08990252513071033
train: 0.944935	val: 0.784817	test: 0.777628

Epoch: 61
Loss: 0.08959855735256379
train: 0.947707	val: 0.782484	test: 0.770631

Epoch: 62
Loss: 0.08811869032070987
train: 0.955021	val: 0.802080	test: 0.788397

Epoch: 63
Loss: 0.08887529056161618
train: 0.953283	val: 0.780898	test: 0.772759

Epoch: 64
Loss: 0.08810938400330334
train: 0.955873	val: 0.807433	test: 0.774868

Epoch: 65
Loss: 0.08756202087729355
train: 0.955256	val: 0.807380	test: 0.764080

Epoch: 66
Loss: 0.08692738860089641
train: 0.960191	val: 0.794683	test: 0.761633

Epoch: 67
Loss: 0.08534974249640903
train: 0.957807	val: 0.809741	test: 0.773408

Epoch: 68
Loss: 0.0881913645086253
train: 0.955311	val: 0.797922	test: 0.768597

Epoch: 69
Loss: 0.08520266513518268
train: 0.957294	val: 0.803415	test: 0.742177

Epoch: 70
Loss: 0.08515415377387343
train: 0.957000	val: 0.818171	test: 0.772195

Epoch: 71
Loss: 0.08546283646054255
train: 0.960206	val: 0.827574	test: 0.775534

Epoch: 72
Loss: 0.08460791616651447
train: 0.963611	val: 0.810442	test: 0.774057

Epoch: 73
Loss: 0.08345267841805558
train: 0.960213	val: 0.796342	test: 0.743730

Epoch: 74
Loss: 0.08446815990638476
train: 0.962911	val: 0.799928	test: 0.769864

Epoch: 75
Loss: 0.08270739648234471
train: 0.962854	val: 0.795197	test: 0.780423

Epoch: 76
Loss: 0.08309304413291864
train: 0.967510	val: 0.795130	test: 0.758896

Epoch: 77
Loss: 0.08141894605261593
train: 0.968003	val: 0.798538	test: 0.786807

Epoch: 78
Loss: 0.0843220728082745
train: 0.967614	val: 0.814835	test: 0.789082

Epoch: 79
Loss: 0.08177407733708657
train: 0.967654	val: 0.780436	test: 0.765926

Epoch: 80
Loss: 0.0814660960282267
train: 0.967435	val: 0.789790	test: 0.777398

Epoch: 81
Loss: 0.08188455923346814
train: 0.971874	val: 0.805139	test: 0.773437

Epoch: 82
Loss: 0.08003276452173459
train: 0.969601	val: 0.789566	test: 0.764242

Epoch: 83
Loss: 0.0795574193988645
train: 0.965915	val: 0.773506	test: 0.766063

Epoch: 84
Loss: 0.08040390392043549
train: 0.968687	val: 0.773908	test: 0.779776

Epoch: 85
Loss: 0.08070132106264494
train: 0.972457	val: 0.792769	test: 0.771948

Epoch: 86
Loss: 0.07800267093146981
train: 0.971965	val: 0.804233	test: 0.758873

Epoch: 87
Loss: 0.0784456656869399
train: 0.972307	val: 0.773231	test: 0.771770

Epoch: 88
Loss: 0.07878902792120966
train: 0.969239	val: 0.788758	test: 0.778708

Epoch: 89
Loss: 0.07921723260540535
train: 0.975476	val: 0.773390	test: 0.746650

Epoch: 90
Loss: 0.07713936971391876
train: 0.972742	val: 0.802843	test: 0.772993

Epoch: 91
Loss: 0.07691397042076613
train: 0.971743	val: 0.792879	test: 0.760955

Epoch: 92
Loss: 0.07611482247115656
train: 0.974514	val: 0.799024	test: 0.765646

Epoch: 93
Loss: 0.07556333594080523
train: 0.975838	val: 0.815755	test: 0.754169

Epoch: 94
Loss: 0.07732158199332376
train: 0.972180	val: 0.778026	test: 0.756874

Epoch: 95
Loss: 0.07578255192144827
train: 0.976969	val: 0.787233	test: 0.765938

Epoch: 96
Loss: 0.07573051702142111
train: 0.975867	val: 0.767722	test: 0.778227

Epoch: 97
Loss: 0.076151256168314
train: 0.978411	val: 0.783810	test: 0.762075

Epoch: 98
Loss: 0.07623950379859991
train: 0.975859	val: 0.805703	test: 0.771871

Epoch: 99
Loss: 0.07307730281023707
train: 0.971168	val: 0.785785	test: 0.761264

Epoch: 100
Loss: 0.07670606645217797
train: 0.979600	val: 0.805099	test: 0.781498

best train: 0.845280	val: 0.840752	test: 0.742855
end
