9087242_2
--dataset=sider --runseed=2 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=True, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=2, schnet_lr_scale=1, seed=42, split='scaffold', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6859499226627294
train: 0.518910	val: 0.517232	test: 0.478209

Epoch: 2
Loss: 0.6487769881954195
train: 0.548258	val: 0.516424	test: 0.506691

Epoch: 3
Loss: 0.6142291619127347
train: 0.561683	val: 0.513251	test: 0.523300

Epoch: 4
Loss: 0.5875923495404483
train: 0.581318	val: 0.517450	test: 0.534830

Epoch: 5
Loss: 0.5607018009674305
train: 0.614889	val: 0.542710	test: 0.554564

Epoch: 6
Loss: 0.5486349126224603
train: 0.632844	val: 0.554248	test: 0.568576

Epoch: 7
Loss: 0.5362828702873431
train: 0.651888	val: 0.570307	test: 0.584630

Epoch: 8
Loss: 0.5267745382859339
train: 0.668113	val: 0.583839	test: 0.591184

Epoch: 9
Loss: 0.5208690642400471
train: 0.678472	val: 0.589412	test: 0.593908

Epoch: 10
Loss: 0.507863024584001
train: 0.682208	val: 0.590005	test: 0.594464

Epoch: 11
Loss: 0.5072892762512183
train: 0.690960	val: 0.592204	test: 0.596746

Epoch: 12
Loss: 0.49837816477335223
train: 0.700188	val: 0.605695	test: 0.595526

Epoch: 13
Loss: 0.4916707956819682
train: 0.706472	val: 0.603745	test: 0.603872

Epoch: 14
Loss: 0.4886719970871501
train: 0.713097	val: 0.603425	test: 0.605296

Epoch: 15
Loss: 0.48431643035922195
train: 0.716420	val: 0.609025	test: 0.608481

Epoch: 16
Loss: 0.4798835172196051
train: 0.722900	val: 0.603347	test: 0.621863

Epoch: 17
Loss: 0.480124333862879
train: 0.726044	val: 0.612339	test: 0.615379

Epoch: 18
Loss: 0.4790233411764424
train: 0.732199	val: 0.616971	test: 0.608798

Epoch: 19
Loss: 0.47205251078075944
train: 0.736581	val: 0.614855	test: 0.615270

Epoch: 20
Loss: 0.4716866667306633
train: 0.740753	val: 0.606354	test: 0.625897

Epoch: 21
Loss: 0.4703972239418318
train: 0.744904	val: 0.599915	test: 0.620709

Epoch: 22
Loss: 0.46823141762258536
train: 0.749528	val: 0.612489	test: 0.609448

Epoch: 23
Loss: 0.46074039043525145
train: 0.751183	val: 0.609567	test: 0.609895

Epoch: 24
Loss: 0.4608359470546704
train: 0.758078	val: 0.607926	test: 0.611497

Epoch: 25
Loss: 0.45927916834128535
train: 0.760435	val: 0.610406	test: 0.615950

Epoch: 26
Loss: 0.45859051825619607
train: 0.764165	val: 0.606601	test: 0.621738

Epoch: 27
Loss: 0.45695580540328795
train: 0.762785	val: 0.618105	test: 0.616159

Epoch: 28
Loss: 0.45367061240713236
train: 0.771124	val: 0.612156	test: 0.621022

Epoch: 29
Loss: 0.4499512355958538
train: 0.774228	val: 0.621442	test: 0.610601

Epoch: 30
Loss: 0.45326447921697505
train: 0.776710	val: 0.611292	test: 0.615154

Epoch: 31
Loss: 0.4517422620155405
train: 0.778243	val: 0.613218	test: 0.617793

Epoch: 32
Loss: 0.45430811315101716
train: 0.780727	val: 0.625151	test: 0.616425

Epoch: 33
Loss: 0.44755990892707054
train: 0.781471	val: 0.622191	test: 0.621033

Epoch: 34
Loss: 0.4478462301779328
train: 0.787529	val: 0.624518	test: 0.621083

Epoch: 35
Loss: 0.4438121874779607
train: 0.789214	val: 0.615937	test: 0.616435

Epoch: 36
Loss: 0.4458461666452246
train: 0.790122	val: 0.622957	test: 0.617501

Epoch: 37
Loss: 0.4419399605119301
train: 0.785938	val: 0.626907	test: 0.607039

Epoch: 38
Loss: 0.4433539543616972
train: 0.787041	val: 0.615745	test: 0.609923

Epoch: 39
Loss: 0.43962474611538094
train: 0.793831	val: 0.617827	test: 0.605200

Epoch: 40
Loss: 0.43976315598294774
train: 0.797405	val: 0.602441	test: 0.605249

Epoch: 41
Loss: 0.4360654457021978
train: 0.805974	val: 0.608246	test: 0.610853

Epoch: 42
Loss: 0.43759372196461077
train: 0.805290	val: 0.619477	test: 0.601564

Epoch: 43
Loss: 0.43461688743063115
train: 0.807302	val: 0.603038	test: 0.610550

Epoch: 44
Loss: 0.43774774816681516
train: 0.808542	val: 0.603739	test: 0.613023

Epoch: 45
Loss: 0.43340218330004754
train: 0.815047	val: 0.610119	test: 0.610644

Epoch: 46
Loss: 0.4307672088497153
train: 0.812422	val: 0.600725	test: 0.611926

Epoch: 47
Loss: 0.4257799119112439
train: 0.814674	val: 0.606327	test: 0.607828

Epoch: 48
Loss: 0.42837404520348776
train: 0.814613	val: 0.616915	test: 0.606427

Epoch: 49
Loss: 0.4280698077886592
train: 0.810921	val: 0.621411	test: 0.599076

Epoch: 50
Loss: 0.4259037120430305
train: 0.815967	val: 0.604196	test: 0.601336

Epoch: 51
Loss: 0.42316350711815093
train: 0.822225	val: 0.599182	test: 0.605055

Epoch: 52
Loss: 0.42852605537777694
train: 0.822659	val: 0.617184	test: 0.598271

Epoch: 53
Loss: 0.42786544650740543
train: 0.827223	val: 0.614903	test: 0.603682

Epoch: 54
Loss: 0.4278726108887577
train: 0.831505	val: 0.602125	test: 0.610790

Epoch: 55
Loss: 0.4197003612088189
train: 0.830578	val: 0.614260	test: 0.597656

Epoch: 56
Loss: 0.419013804731503
train: 0.835438	val: 0.604603	test: 0.605835

Epoch: 57
Loss: 0.42147139708174947
train: 0.828818	val: 0.601314	test: 0.588400

Epoch: 58
Loss: 0.4167247145319831
train: 0.833039	val: 0.609162	test: 0.575782

Epoch: 59
Loss: 0.41569749194377714
train: 0.838417	val: 0.618093	test: 0.600371

Epoch: 60
Loss: 0.41470566865848396
train: 0.829772	val: 0.617145	test: 0.618876

Epoch: 61
Loss: 0.41845840762263214
train: 0.835539	val: 0.608861	test: 0.618459

Epoch: 62
Loss: 0.4123394708082853
train: 0.840653	val: 0.603769	test: 0.603370

Epoch: 63
Loss: 0.409264476151136
train: 0.840807	val: 0.608460	test: 0.595320

Epoch: 64
Loss: 0.4093196122187341
train: 0.836694	val: 0.627104	test: 0.580872

Epoch: 65
Loss: 0.40730956076329294
train: 0.845058	val: 0.611049	test: 0.602256

Epoch: 66
Loss: 0.40709892009240856
train: 0.845557	val: 0.604833	test: 0.611219

Epoch: 67
Loss: 0.4085461576543052
train: 0.851366	val: 0.606069	test: 0.602213

Epoch: 68
Loss: 0.40506981514017604
train: 0.848508	val: 0.611605	test: 0.588870

Epoch: 69
Loss: 0.4057910625386419
train: 0.848747	val: 0.617467	test: 0.611934

Epoch: 70
Loss: 0.40577979574040324
train: 0.840778	val: 0.632420	test: 0.612442

Epoch: 71
Loss: 0.40442975735907966
train: 0.850159	val: 0.626452	test: 0.605383

Epoch: 72
Loss: 0.40348732410076316
train: 0.854665	val: 0.616902	test: 0.599751

Epoch: 73
Loss: 0.4069450259015886
train: 0.850956	val: 0.613237	test: 0.591109

Epoch: 74
Loss: 0.40234511663799183
train: 0.854892	val: 0.614072	test: 0.594233

Epoch: 75
Loss: 0.40358854451809306
train: 0.861999	val: 0.615226	test: 0.597687

Epoch: 76
Loss: 0.3951623768620995
train: 0.858592	val: 0.613560	test: 0.612508

Epoch: 77
Loss: 0.3942399572269807
train: 0.856751	val: 0.614682	test: 0.618181

Epoch: 78
Loss: 0.3950148252736974
train: 0.860982	val: 0.629834	test: 0.602461

Epoch: 79
Loss: 0.39444981240969246
train: 0.861219	val: 0.615174	test: 0.591562

Epoch: 80
Loss: 0.39304577054526535
train: 0.865698	val: 0.610811	test: 0.597504

Epoch: 81
Loss: 0.3985670674772632
train: 0.863714	val: 0.608031	test: 0.608158

Epoch: 82
Loss: 0.39340652934077636
train: 0.867495	val: 0.608590	test: 0.606902

Epoch: 83
Loss: 0.3864617346116931
train: 0.866757	val: 0.613609	test: 0.583006

Epoch: 84
Loss: 0.38952069128141387
train: 0.868010	val: 0.601979	test: 0.584169

Epoch: 85
Loss: 0.38836510496849597
train: 0.873272	val: 0.601878	test: 0.579957

Epoch: 86
Loss: 0.3904945779367221
train: 0.874582	val: 0.606784	test: 0.598043

Epoch: 87
Loss: 0.3849745776387078
train: 0.867393	val: 0.615144	test: 0.630112

Epoch: 88
Loss: 0.3881713290047212
train: 0.869710	val: 0.619098	test: 0.622245

Epoch: 89
Loss: 0.38510330636351975
train: 0.877211	val: 0.623141	test: 0.608799

Epoch: 90
Loss: 0.3838297952848837
train: 0.878617	val: 0.613640	test: 0.610347

Epoch: 91
Loss: 0.3850062623692224
train: 0.877415	val: 0.621157	test: 0.588965

Epoch: 92
Loss: 0.38368801240874856
train: 0.873111	val: 0.617141	test: 0.610944

Epoch: 93
Loss: 0.3795181293007277
train: 0.874923	val: 0.609719	test: 0.605076

Epoch: 94
Loss: 0.3850022065557782
train: 0.882064	val: 0.618394	test: 0.595629

Epoch: 95
Loss: 0.38776197851140115
train: 0.879793	val: 0.612954	test: 0.601076

Epoch: 96
Loss: 0.3816151782577927
train: 0.880255	val: 0.616368	test: 0.601789

Epoch: 97
Loss: 0.37222098006261284
train: 0.885626	val: 0.610967	test: 0.600294

Epoch: 98
Loss: 0.37574843356388216
train: 0.884403	val: 0.618917	test: 0.608229

Epoch: 99
Loss: 0.37282751043412266
train: 0.888516	val: 0.614841	test: 0.615703

Epoch: 100
Loss: 0.3742090039883899
train: 0.888734	val: 0.605993	test: 0.616498

best train: 0.840778	val: 0.632420	test: 0.612442
end
