9090750_0
--dataset=sider --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='sider', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=True, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, seed=42, split='scaffold', verbose=False)
Dataset: sider
Data: Data(edge_attr=[100912, 2], edge_index=[2, 100912], id=[1427], x=[48006, 2], y=[38529])
MoleculeDataset(1427)
split via scaffold
Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)
)
Epoch: 1
Loss: 0.6787215037474927
train: 0.530280	val: 0.527305	test: 0.482678

Epoch: 2
Loss: 0.6413348709149315
train: 0.541187	val: 0.526435	test: 0.492479

Epoch: 3
Loss: 0.6082167979719799
train: 0.559241	val: 0.505351	test: 0.519421

Epoch: 4
Loss: 0.5835296802853456
train: 0.582522	val: 0.507691	test: 0.539998

Epoch: 5
Loss: 0.5591167169108001
train: 0.614347	val: 0.524269	test: 0.562127

Epoch: 6
Loss: 0.546041317704877
train: 0.638148	val: 0.557540	test: 0.580938

Epoch: 7
Loss: 0.5314101518962839
train: 0.653377	val: 0.579515	test: 0.582654

Epoch: 8
Loss: 0.528829438368464
train: 0.665074	val: 0.580427	test: 0.589131

Epoch: 9
Loss: 0.5149235120291273
train: 0.674878	val: 0.595809	test: 0.597585

Epoch: 10
Loss: 0.510852363083176
train: 0.681760	val: 0.592887	test: 0.593316

Epoch: 11
Loss: 0.49679962037766084
train: 0.681641	val: 0.579964	test: 0.582276

Epoch: 12
Loss: 0.4977988388541423
train: 0.694064	val: 0.593466	test: 0.581359

Epoch: 13
Loss: 0.48877088226323606
train: 0.706963	val: 0.606231	test: 0.590940

Epoch: 14
Loss: 0.48659558974543077
train: 0.714028	val: 0.606166	test: 0.595946

Epoch: 15
Loss: 0.4841039201244192
train: 0.714409	val: 0.604077	test: 0.591535

Epoch: 16
Loss: 0.4799229974505838
train: 0.722538	val: 0.619462	test: 0.588310

Epoch: 17
Loss: 0.47712381871681203
train: 0.729348	val: 0.613787	test: 0.604421

Epoch: 18
Loss: 0.47255572709640487
train: 0.728144	val: 0.620098	test: 0.605042

Epoch: 19
Loss: 0.4748782281036588
train: 0.729747	val: 0.633673	test: 0.588176

Epoch: 20
Loss: 0.4713738000202161
train: 0.737063	val: 0.618004	test: 0.600998

Epoch: 21
Loss: 0.4682699468208035
train: 0.741809	val: 0.608618	test: 0.598121

Epoch: 22
Loss: 0.465238854218621
train: 0.744633	val: 0.610008	test: 0.602448

Epoch: 23
Loss: 0.46590634065223585
train: 0.744652	val: 0.623485	test: 0.588619

Epoch: 24
Loss: 0.46883029099844986
train: 0.752965	val: 0.615885	test: 0.592926

Epoch: 25
Loss: 0.45872655571421816
train: 0.753611	val: 0.616728	test: 0.604782

Epoch: 26
Loss: 0.4573952201848831
train: 0.755816	val: 0.626561	test: 0.608665

Epoch: 27
Loss: 0.4630107210407555
train: 0.761873	val: 0.623123	test: 0.596134

Epoch: 28
Loss: 0.4562110898846862
train: 0.763283	val: 0.612433	test: 0.586717

Epoch: 29
Loss: 0.45360842874259194
train: 0.769407	val: 0.614343	test: 0.606547

Epoch: 30
Loss: 0.454315691711924
train: 0.773082	val: 0.616859	test: 0.612574

Epoch: 31
Loss: 0.45142004578230155
train: 0.773875	val: 0.610906	test: 0.617598

Epoch: 32
Loss: 0.4514847548200165
train: 0.776279	val: 0.621431	test: 0.618578

Epoch: 33
Loss: 0.4430647149465474
train: 0.777638	val: 0.623410	test: 0.619193

Epoch: 34
Loss: 0.4480421931813333
train: 0.779524	val: 0.631441	test: 0.612473

Epoch: 35
Loss: 0.445058725894075
train: 0.780786	val: 0.609670	test: 0.616230

Epoch: 36
Loss: 0.4433815777413816
train: 0.776535	val: 0.615993	test: 0.602882

Epoch: 37
Loss: 0.4483302816587442
train: 0.787821	val: 0.633439	test: 0.605153

Epoch: 38
Loss: 0.4438610407630784
train: 0.790502	val: 0.614796	test: 0.614037

Epoch: 39
Loss: 0.4383517853104458
train: 0.789633	val: 0.599409	test: 0.606006

Epoch: 40
Loss: 0.43853527395516123
train: 0.797539	val: 0.611187	test: 0.618330

Epoch: 41
Loss: 0.4368686057103802
train: 0.801517	val: 0.616565	test: 0.617193

Epoch: 42
Loss: 0.43870957479528216
train: 0.799629	val: 0.619473	test: 0.611477

Epoch: 43
Loss: 0.438639068478324
train: 0.800620	val: 0.616480	test: 0.608971

Epoch: 44
Loss: 0.43872747317116423
train: 0.804327	val: 0.628239	test: 0.614499

Epoch: 45
Loss: 0.4372732365726099
train: 0.807359	val: 0.617294	test: 0.621736

Epoch: 46
Loss: 0.4345985713899011
train: 0.808748	val: 0.614154	test: 0.620582

Epoch: 47
Loss: 0.43251540369130925
train: 0.811426	val: 0.618993	test: 0.625139

Epoch: 48
Loss: 0.42981819269429894
train: 0.810415	val: 0.621237	test: 0.628275

Epoch: 49
Loss: 0.42986406736159893
train: 0.815507	val: 0.616715	test: 0.622557

Epoch: 50
Loss: 0.4230716021558626
train: 0.820215	val: 0.618732	test: 0.626547

Epoch: 51
Loss: 0.42891750067625195
train: 0.820611	val: 0.614826	test: 0.615312

Epoch: 52
Loss: 0.4239448707391366
train: 0.821146	val: 0.609561	test: 0.620561

Epoch: 53
Loss: 0.41827445611220854
train: 0.819072	val: 0.620979	test: 0.610801

Epoch: 54
Loss: 0.42518118423424
train: 0.822703	val: 0.626485	test: 0.614797

Epoch: 55
Loss: 0.4226311500291051
train: 0.822596	val: 0.613638	test: 0.613847

Epoch: 56
Loss: 0.4184171406580762
train: 0.820846	val: 0.611081	test: 0.626945

Epoch: 57
Loss: 0.41703801653211475
train: 0.833724	val: 0.624981	test: 0.643036

Epoch: 58
Loss: 0.42120751248821636
train: 0.830469	val: 0.618046	test: 0.639392

Epoch: 59
Loss: 0.4135578712987621
train: 0.824475	val: 0.611967	test: 0.634242

Epoch: 60
Loss: 0.4177312576663672
train: 0.832926	val: 0.620914	test: 0.623484

Epoch: 61
Loss: 0.4135474663361191
train: 0.837916	val: 0.618596	test: 0.629570

Epoch: 62
Loss: 0.4184546808870908
train: 0.835961	val: 0.618938	test: 0.618981

Epoch: 63
Loss: 0.4177230450374573
train: 0.824824	val: 0.617056	test: 0.603766

Epoch: 64
Loss: 0.4085780150634277
train: 0.840441	val: 0.616782	test: 0.622531

Epoch: 65
Loss: 0.41149737776348483
train: 0.837372	val: 0.614855	test: 0.632971

Epoch: 66
Loss: 0.410617436728925
train: 0.844545	val: 0.624662	test: 0.638426

Epoch: 67
Loss: 0.40795786741435436
train: 0.840471	val: 0.611310	test: 0.630764

Epoch: 68
Loss: 0.4073086355157418
train: 0.841830	val: 0.617148	test: 0.617906

Epoch: 69
Loss: 0.40774715106839643
train: 0.844752	val: 0.612666	test: 0.618752

Epoch: 70
Loss: 0.40817933143154017
train: 0.848347	val: 0.615838	test: 0.634731

Epoch: 71
Loss: 0.4090235876250847
train: 0.852261	val: 0.635675	test: 0.631752

Epoch: 72
Loss: 0.40027443966471266
train: 0.851843	val: 0.626490	test: 0.622310

Epoch: 73
Loss: 0.4041357644043183
train: 0.854794	val: 0.620591	test: 0.636713

Epoch: 74
Loss: 0.3947663757385994
train: 0.855486	val: 0.619845	test: 0.624351

Epoch: 75
Loss: 0.40097945012048947
train: 0.852188	val: 0.609970	test: 0.603231

Epoch: 76
Loss: 0.39631132657499907
train: 0.851985	val: 0.614078	test: 0.603194

Epoch: 77
Loss: 0.3896824660160817
train: 0.860940	val: 0.624172	test: 0.619764

Epoch: 78
Loss: 0.3955935344405216
train: 0.863268	val: 0.626064	test: 0.624888

Epoch: 79
Loss: 0.39850556149320654
train: 0.865724	val: 0.629121	test: 0.639003

Epoch: 80
Loss: 0.3929815762304899
train: 0.861892	val: 0.627396	test: 0.625649

Epoch: 81
Loss: 0.39639880343922296
train: 0.863840	val: 0.621944	test: 0.623945

Epoch: 82
Loss: 0.3919163522128882
train: 0.869869	val: 0.629159	test: 0.627709

Epoch: 83
Loss: 0.39173595369809283
train: 0.866043	val: 0.624668	test: 0.637677

Epoch: 84
Loss: 0.38593604891754746
train: 0.870479	val: 0.627840	test: 0.631161

Epoch: 85
Loss: 0.38577703880156283
train: 0.870683	val: 0.628100	test: 0.623223

Epoch: 86
Loss: 0.38698312482441616
train: 0.872210	val: 0.635391	test: 0.624400

Epoch: 87
Loss: 0.3879870787523748
train: 0.864874	val: 0.627344	test: 0.626569

Epoch: 88
Loss: 0.3820518947768069
train: 0.873032	val: 0.614827	test: 0.617910

Epoch: 89
Loss: 0.3886624469336043
train: 0.877388	val: 0.627636	test: 0.618782

Epoch: 90
Loss: 0.38066567507450844
train: 0.874565	val: 0.619588	test: 0.639367

Epoch: 91
Loss: 0.3804237169756005
train: 0.877583	val: 0.612609	test: 0.632499

Epoch: 92
Loss: 0.38466530923501197
train: 0.880858	val: 0.609057	test: 0.625415

Epoch: 93
Loss: 0.37814977604021754
train: 0.880311	val: 0.612982	test: 0.622419

Epoch: 94
Loss: 0.3813647335304315
train: 0.871528	val: 0.610585	test: 0.618370

Epoch: 95
Loss: 0.3848455001835288
train: 0.862168	val: 0.621645	test: 0.616374

Epoch: 96
Loss: 0.3824647667339811
train: 0.880376	val: 0.629504	test: 0.615657

Epoch: 97
Loss: 0.38101093751281956
train: 0.880282	val: 0.624528	test: 0.635300

Epoch: 98
Loss: 0.37579157377961403
train: 0.880310	val: 0.636747	test: 0.639494

Epoch: 99
Loss: 0.38299062366628306
train: 0.885984	val: 0.639118	test: 0.645007

Epoch: 100
Loss: 0.3694026009875893
train: 0.884605	val: 0.623844	test: 0.639111

best train: 0.885984	val: 0.639118	test: 0.645007
end
