13984931_0
--dataset=tox21 --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, data_dir_chirality='../datasets/chirality/d4_docking/d4_docking_rs.csv', dataset='tox21', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=False, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, se3_transformer_div=2, se3_transformer_n_heads=8, se3_transformer_num_channels=32, se3_transformer_num_degrees=4, se3_transformer_num_layers=7, se3_transformer_num_nlayers=1, seed=42, spherenet_basis_emb_size_angle=8, spherenet_basis_emb_size_dist=8, spherenet_basis_emb_size_torsion=8, spherenet_cutoff=3.0, spherenet_envelope_exponent=5, spherenet_int_emb_size=64, spherenet_num_after_skip=2, spherenet_num_before_skip=1, spherenet_num_layers=4, spherenet_num_output_layers=3, spherenet_num_radial=6, spherenet_num_spherical=3, spherenet_out_emb_channels=256, split='scaffold', split_path='../datasets/chirality/d4_docking/rs/split0.npy', verbose=False)
Dataset: tox21
Data: Data(edge_attr=[302190, 2], edge_index=[2, 302190], id=[7831], x=[145459, 2], y=[93972])
MoleculeDataset(7831)
split via scaffold
Data(edge_attr=[20, 2], edge_index=[2, 20], id=[1], x=[11, 2], y=[12])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=12, bias=True)
)
Epoch: 1
Loss: 0.5423816066146798
train: 0.724199	val: 0.664572	test: 0.610478
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 2
Loss: 0.3518108366969242
train: 0.756814	val: 0.681026	test: 0.635269
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 3
Loss: 0.2482183561002801
train: 0.793288	val: 0.739789	test: 0.696927
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 4
Loss: 0.21158712653347383
train: 0.809557	val: 0.747987	test: 0.712387
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 5
Loss: 0.19944027510926254
train: 0.821898	val: 0.740621	test: 0.703498
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 6
Loss: 0.19444753460457814
train: 0.824822	val: 0.736410	test: 0.700709
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 7
Loss: 0.1900730401993779
train: 0.838417	val: 0.752972	test: 0.714496
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 8
Loss: 0.18734324527512064
train: 0.845553	val: 0.760188	test: 0.729600
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 9
Loss: 0.18468008048396853
train: 0.852524	val: 0.748927	test: 0.720076
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 10
Loss: 0.18330875470346084
train: 0.854664	val: 0.771101	test: 0.733653
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 11
Loss: 0.1822557636832579
train: 0.857571	val: 0.742511	test: 0.714625
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 12
Loss: 0.17806236033758133
train: 0.861754	val: 0.769402	test: 0.726962
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 13
Loss: 0.1764921245128889
train: 0.864494	val: 0.765274	test: 0.718621
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 14
Loss: 0.1748534459759937
train: 0.872574	val: 0.757629	test: 0.722716
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 15
Loss: 0.17264156338372036
train: 0.875112	val: 0.763099	test: 0.736081
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 16
Loss: 0.1717168227008202
train: 0.878834	val: 0.762783	test: 0.722476
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 17
Loss: 0.1701864975852443
train: 0.879121	val: 0.759340	test: 0.720609
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 18
Loss: 0.17004654174476524
train: 0.885579	val: 0.771791	test: 0.737972
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 19
Loss: 0.16767201019985584
train: 0.872667	val: 0.760274	test: 0.714212
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 20
Loss: 0.1675572670604489
train: 0.890943	val: 0.765120	test: 0.731838
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 21
Loss: 0.16581344217862717
train: 0.889567	val: 0.781384	test: 0.741084
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 22
Loss: 0.16366582466931212
train: 0.893372	val: 0.779676	test: 0.729358
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 23
Loss: 0.16470667513235265
train: 0.893302	val: 0.763417	test: 0.733106
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 24
Loss: 0.1626852848401544
train: 0.897383	val: 0.774974	test: 0.739951
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 25
Loss: 0.1622717456706242
train: 0.899638	val: 0.775613	test: 0.740426
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 26
Loss: 0.1602724407122383
train: 0.902072	val: 0.774369	test: 0.740422
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 27
Loss: 0.1608427482745425
train: 0.900389	val: 0.785452	test: 0.739653
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 28
Loss: 0.15931096040833892
train: 0.903379	val: 0.773549	test: 0.735397
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 29
Loss: 0.15935170896811932
train: 0.906042	val: 0.779458	test: 0.741651
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 30
Loss: 0.15765005611414507
train: 0.908898	val: 0.770255	test: 0.741667
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 31
Loss: 0.1567596300301252
train: 0.909756	val: 0.777659	test: 0.741114
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 32
Loss: 0.1570723144920916
train: 0.913094	val: 0.780294	test: 0.742556
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 33
Loss: 0.15337442121193445
train: 0.913485	val: 0.771572	test: 0.734309
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 34
Loss: 0.15405477690223882
train: 0.914213	val: 0.769223	test: 0.741486
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 35
Loss: 0.154923525118324
train: 0.916430	val: 0.767903	test: 0.729436
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 36
Loss: 0.15305409245448362
train: 0.918860	val: 0.774898	test: 0.746945
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 37
Loss: 0.1511309819609425
train: 0.921490	val: 0.782872	test: 0.745390
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 38
Loss: 0.15161607003036692
train: 0.919387	val: 0.789908	test: 0.749034
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 39
Loss: 0.15067599107778562
train: 0.923340	val: 0.779957	test: 0.741526
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 40
Loss: 0.151641816734113
train: 0.923051	val: 0.777305	test: 0.740696
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 41
Loss: 0.1501829593285419
train: 0.919392	val: 0.783829	test: 0.738695
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 42
Loss: 0.14729676495436123
train: 0.918517	val: 0.782126	test: 0.728759
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 43
Loss: 0.14695042292644703
train: 0.923564	val: 0.782392	test: 0.742107
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 44
Loss: 0.1462816882911009
train: 0.928603	val: 0.782636	test: 0.746148
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 45
Loss: 0.144638370150283
train: 0.928586	val: 0.787326	test: 0.743774
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 46
Loss: 0.1461389309933211
train: 0.929369	val: 0.781861	test: 0.743748
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 47
Loss: 0.1462618020725848
train: 0.928351	val: 0.768641	test: 0.749977
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 48
Loss: 0.14466924938722026
train: 0.934902	val: 0.782702	test: 0.736225
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 49
Loss: 0.1414787794853714
train: 0.934289	val: 0.781022	test: 0.742038
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 50
Loss: 0.14162592564248477
train: 0.935222	val: 0.781971	test: 0.745761
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 51
Loss: 0.14116159552269003
train: 0.934780	val: 0.789948	test: 0.755665
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 52
Loss: 0.14134388334814432
train: 0.940371	val: 0.784677	test: 0.743534
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 53
Loss: 0.14041291256731156
train: 0.937908	val: 0.790921	test: 0.749622
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 54
Loss: 0.1407821887830651
train: 0.933276	val: 0.778510	test: 0.749410
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 55
Loss: 0.13892212253277822
train: 0.938705	val: 0.786539	test: 0.752386
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 56
Loss: 0.13780808480291926
train: 0.942634	val: 0.781115	test: 0.742572
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 57
Loss: 0.13718541049303706
train: 0.942270	val: 0.784391	test: 0.750435
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 58
Loss: 0.1387416949234912
train: 0.942793	val: 0.785015	test: 0.742507
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 59
Loss: 0.13650406118494948
train: 0.945373	val: 0.779434	test: 0.750344
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 60
Loss: 0.13689318210153564
train: 0.944765	val: 0.781134	test: 0.744019
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 61
Loss: 0.1356800195288853
train: 0.946038	val: 0.776269	test: 0.738092
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 62
Loss: 0.1347764697841854
train: 0.943536	val: 0.784881	test: 0.734727
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 63
Loss: 0.13595608755635577
train: 0.946852	val: 0.786142	test: 0.752707
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 64
Loss: 0.13329974328720703
train: 0.950186	val: 0.782904	test: 0.741676
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 65
Loss: 0.13215975086048448
train: 0.947879	val: 0.773377	test: 0.737596
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 66
Loss: 0.1334600612031895
train: 0.950616	val: 0.784351	test: 0.739846
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 67
Loss: 0.13215961217155384
train: 0.951918	val: 0.774956	test: 0.743890
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 68
Loss: 0.130734081699555
train: 0.952473	val: 0.774589	test: 0.731413
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 69
Loss: 0.13113197030331822
train: 0.953266	val: 0.780669	test: 0.743486
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 70
Loss: 0.12890811387217768
train: 0.955923	val: 0.785317	test: 0.744080
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 71
Loss: 0.13070025275612185
train: 0.952588	val: 0.787048	test: 0.748356
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 72
Loss: 0.12705928565783642
train: 0.955599	val: 0.774435	test: 0.739820
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 73
Loss: 0.12779763243864722
train: 0.956533	val: 0.782086	test: 0.750322
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 74
Loss: 0.12854040982572992
train: 0.956432	val: 0.773040	test: 0.739234
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 75
Loss: 0.12717132552443858
train: 0.956693	val: 0.777400	test: 0.733424
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 76
Loss: 0.12561491938871922
train: 0.959309	val: 0.769270	test: 0.745015
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 77
Loss: 0.1265135736106698
train: 0.960945	val: 0.770424	test: 0.735608
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 78
Loss: 0.1249972710267115
train: 0.956667	val: 0.778968	test: 0.734940
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 79
Loss: 0.12554113439858436
train: 0.961711	val: 0.778853	test: 0.740149
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 80
Loss: 0.1248810241975204
train: 0.960293	val: 0.785294	test: 0.739402
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 81
Loss: 0.12321763937101843
train: 0.963583	val: 0.786146	test: 0.742470
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 82
Loss: 0.12515784980898176
train: 0.962980	val: 0.769943	test: 0.733806
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 83
Loss: 0.1240507781851046
train: 0.962176	val: 0.774554	test: 0.740214
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 84
Loss: 0.1202524265804684
train: 0.964882	val: 0.781557	test: 0.746229
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 85
Loss: 0.12156741256852534
train: 0.964309	val: 0.773377	test: 0.735210
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 86
Loss: 0.12091650352129894
train: 0.963825	val: 0.781223	test: 0.734076
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 87
Loss: 0.11747880440774498
train: 0.966117	val: 0.782064	test: 0.745584
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 88
Loss: 0.11992327996752879
train: 0.964190	val: 0.778735	test: 0.733294
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 89
Loss: 0.11803986607234686
train: 0.966171	val: 0.780117	test: 0.736976
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 90
Loss: 0.11836019591629768
train: 0.967331	val: 0.774186	test: 0.730648
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 91
Loss: 0.1163983319517267
train: 0.964124	val: 0.773821	test: 0.745258
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 92
Loss: 0.11957039130235225
train: 0.967425	val: 0.773550	test: 0.744864
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 93
Loss: 0.11467039955157643
train: 0.971285	val: 0.771412	test: 0.736651
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 94
Loss: 0.11469052173531213
train: 0.969704	val: 0.773198	test: 0.733017
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 95
Loss: 0.11484047497794499
train: 0.970546	val: 0.780217	test: 0.739867
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 96
Loss: 0.11390521928313331
train: 0.968087	val: 0.767244	test: 0.742586
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 97
Loss: 0.1135218198409338
train: 0.971787	val: 0.779881	test: 0.740209
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 98
Loss: 0.11402574644118524
train: 0.972058	val: 0.784371	test: 0.754766
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 99
Loss: 0.11280296660864036
train: 0.973201	val: 0.772703	test: 0.736057
acc train: 0.000000	val: 0.000000	test: 0.000000

Epoch: 100
Loss: 0.11149862212618608
train: 0.972800	val: 0.781664	test: 0.746967
acc train: 0.000000	val: 0.000000	test: 0.000000

best train: 0.937908	val: 0.790921	test: 0.749622
best ACC train: 0.000000	val: 0.000000	test: 0.000000
end
