9090750_0
--dataset=hiv --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='hiv', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=True, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, seed=42, split='scaffold', verbose=False)
Dataset: hiv
Data: Data(edge_attr=[2259376, 2], edge_index=[2, 2259376], id=[41127], x=[1049163, 2], y=[41127])
MoleculeDataset(41127)
split via scaffold
Data(edge_attr=[32, 2], edge_index=[2, 32], id=[1], x=[16, 2], y=[1])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)
)
Epoch: 1
Loss: 0.2787016555666856
train: 0.777233	val: 0.723928	test: 0.699073

Epoch: 2
Loss: 0.1389806552740529
train: 0.787845	val: 0.782475	test: 0.718647

Epoch: 3
Loss: 0.13225227015611393
train: 0.812498	val: 0.796180	test: 0.737115

Epoch: 4
Loss: 0.12793089638595592
train: 0.812356	val: 0.768436	test: 0.754381

Epoch: 5
Loss: 0.1261324837580914
train: 0.839514	val: 0.781097	test: 0.772242

Epoch: 6
Loss: 0.12348435595114492
train: 0.817427	val: 0.777318	test: 0.753313

Epoch: 7
Loss: 0.12172372343516136
train: 0.838100	val: 0.782386	test: 0.768678

Epoch: 8
Loss: 0.1211753999957721
train: 0.840343	val: 0.798164	test: 0.761672

Epoch: 9
Loss: 0.11865005541867084
train: 0.861512	val: 0.812393	test: 0.730908

Epoch: 10
Loss: 0.11639569803439012
train: 0.867725	val: 0.802806	test: 0.767607

Epoch: 11
Loss: 0.11686855701883105
train: 0.864804	val: 0.820127	test: 0.790759

Epoch: 12
Loss: 0.11511263956903224
train: 0.873451	val: 0.801768	test: 0.771994

Epoch: 13
Loss: 0.11408182386486038
train: 0.871332	val: 0.795886	test: 0.746658

Epoch: 14
Loss: 0.1117466975495341
train: 0.876672	val: 0.792757	test: 0.762883

Epoch: 15
Loss: 0.1116789615948941
train: 0.880735	val: 0.786054	test: 0.761148

Epoch: 16
Loss: 0.11197436575204382
train: 0.880757	val: 0.802714	test: 0.743890

Epoch: 17
Loss: 0.10964409896104968
train: 0.879627	val: 0.808210	test: 0.748524

Epoch: 18
Loss: 0.10984226489623218
train: 0.892888	val: 0.791737	test: 0.755266

Epoch: 19
Loss: 0.11022668847728347
train: 0.892304	val: 0.797040	test: 0.756956

Epoch: 20
Loss: 0.10718145843263299
train: 0.891607	val: 0.828232	test: 0.749721

Epoch: 21
Loss: 0.10705476804108545
train: 0.896339	val: 0.812831	test: 0.758591

Epoch: 22
Loss: 0.10639375279864652
train: 0.888711	val: 0.803238	test: 0.736859

Epoch: 23
Loss: 0.10594778403554256
train: 0.890224	val: 0.794827	test: 0.733517

Epoch: 24
Loss: 0.10633214596386162
train: 0.905559	val: 0.804619	test: 0.758725

Epoch: 25
Loss: 0.1048158482658543
train: 0.906540	val: 0.814698	test: 0.747598

Epoch: 26
Loss: 0.10448617191674292
train: 0.907574	val: 0.815608	test: 0.754182

Epoch: 27
Loss: 0.10419787178896164
train: 0.919050	val: 0.817684	test: 0.757624

Epoch: 28
Loss: 0.10324506135506112
train: 0.900959	val: 0.773862	test: 0.737807

Epoch: 29
Loss: 0.10215677708426975
train: 0.910464	val: 0.795730	test: 0.739325

Epoch: 30
Loss: 0.10223505081953733
train: 0.920486	val: 0.814619	test: 0.762120

Epoch: 31
Loss: 0.10112824309861358
train: 0.919869	val: 0.800957	test: 0.770305

Epoch: 32
Loss: 0.0997343547127293
train: 0.918169	val: 0.791103	test: 0.748630

Epoch: 33
Loss: 0.10139054498393799
train: 0.924204	val: 0.816701	test: 0.755017

Epoch: 34
Loss: 0.1005825988267511
train: 0.926106	val: 0.802451	test: 0.765279

Epoch: 35
Loss: 0.09878786499274385
train: 0.916328	val: 0.818820	test: 0.753346

Epoch: 36
Loss: 0.10115513097011888
train: 0.923287	val: 0.794854	test: 0.768335

Epoch: 37
Loss: 0.09878610382971993
train: 0.926169	val: 0.809022	test: 0.763271

Epoch: 38
Loss: 0.09917701973156477
train: 0.926785	val: 0.791027	test: 0.738189

Epoch: 39
Loss: 0.09922290861589741
train: 0.930853	val: 0.804906	test: 0.762960

Epoch: 40
Loss: 0.09763183218709987
train: 0.934689	val: 0.794141	test: 0.765980

Epoch: 41
Loss: 0.09603440295612489
train: 0.936585	val: 0.804738	test: 0.753862

Epoch: 42
Loss: 0.09739793362028201
train: 0.936900	val: 0.813125	test: 0.757834

Epoch: 43
Loss: 0.09550356685171547
train: 0.928639	val: 0.801792	test: 0.769175

Epoch: 44
Loss: 0.09593204041694574
train: 0.937690	val: 0.777536	test: 0.749342

Epoch: 45
Loss: 0.09617458958683982
train: 0.936893	val: 0.807711	test: 0.753232

Epoch: 46
Loss: 0.0950532781109515
train: 0.935043	val: 0.800611	test: 0.754733

Epoch: 47
Loss: 0.09553890185357491
train: 0.933898	val: 0.793706	test: 0.755063

Epoch: 48
Loss: 0.0933979617015079
train: 0.944776	val: 0.812797	test: 0.768358

Epoch: 49
Loss: 0.09378539500549285
train: 0.941900	val: 0.788896	test: 0.755335

Epoch: 50
Loss: 0.0932566512913228
train: 0.943493	val: 0.805794	test: 0.765359

Epoch: 51
Loss: 0.09346534754785313
train: 0.948167	val: 0.828597	test: 0.762294

Epoch: 52
Loss: 0.09126937044443102
train: 0.949998	val: 0.795852	test: 0.779127

Epoch: 53
Loss: 0.09219140689688923
train: 0.949338	val: 0.790861	test: 0.778161

Epoch: 54
Loss: 0.0910137718508109
train: 0.946950	val: 0.796088	test: 0.757512

Epoch: 55
Loss: 0.09181078877587169
train: 0.949879	val: 0.787254	test: 0.764735

Epoch: 56
Loss: 0.09088162453476012
train: 0.953812	val: 0.787674	test: 0.767064

Epoch: 57
Loss: 0.08963274859845241
train: 0.945443	val: 0.803323	test: 0.750818

Epoch: 58
Loss: 0.08956076076387494
train: 0.953829	val: 0.790093	test: 0.751295

Epoch: 59
Loss: 0.08807879975562996
train: 0.951676	val: 0.803535	test: 0.746337

Epoch: 60
Loss: 0.08861892388762685
train: 0.954479	val: 0.796511	test: 0.743954

Epoch: 61
Loss: 0.08874796445276076
train: 0.952773	val: 0.800583	test: 0.758485

Epoch: 62
Loss: 0.08881391960085917
train: 0.957187	val: 0.794820	test: 0.754435

Epoch: 63
Loss: 0.08702309413100025
train: 0.958474	val: 0.807194	test: 0.754391

Epoch: 64
Loss: 0.08747246657506604
train: 0.959396	val: 0.818805	test: 0.778487

Epoch: 65
Loss: 0.08634982624462095
train: 0.957471	val: 0.790148	test: 0.755961

Epoch: 66
Loss: 0.0845138888794548
train: 0.963983	val: 0.799674	test: 0.769588

Epoch: 67
Loss: 0.0859877188613629
train: 0.955229	val: 0.783886	test: 0.758924

Epoch: 68
Loss: 0.08610097581439853
train: 0.960074	val: 0.800433	test: 0.761403

Epoch: 69
Loss: 0.08555977908653312
train: 0.964224	val: 0.801355	test: 0.747145

Epoch: 70
Loss: 0.0842614608318411
train: 0.966547	val: 0.797034	test: 0.759905

Epoch: 71
Loss: 0.08499630193388247
train: 0.963631	val: 0.797674	test: 0.774675

Epoch: 72
Loss: 0.08454132224646871
train: 0.963240	val: 0.801768	test: 0.773696

Epoch: 73
Loss: 0.08480452575775307
train: 0.962878	val: 0.785904	test: 0.749242

Epoch: 74
Loss: 0.08396869551074324
train: 0.961752	val: 0.792974	test: 0.750885

Epoch: 75
Loss: 0.08255005034945193
train: 0.968256	val: 0.820623	test: 0.758655

Epoch: 76
Loss: 0.0831858872597967
train: 0.965461	val: 0.812145	test: 0.760353

Epoch: 77
Loss: 0.0822193092535976
train: 0.970296	val: 0.803409	test: 0.776917

Epoch: 78
Loss: 0.08222764266573848
train: 0.968400	val: 0.831420	test: 0.767460

Epoch: 79
Loss: 0.0801699325316279
train: 0.969799	val: 0.799288	test: 0.751662

Epoch: 80
Loss: 0.08256177078463235
train: 0.970842	val: 0.793718	test: 0.751314

Epoch: 81
Loss: 0.08106750620103048
train: 0.967761	val: 0.808259	test: 0.750269

Epoch: 82
Loss: 0.07999879901985225
train: 0.972734	val: 0.817901	test: 0.739512

Epoch: 83
Loss: 0.08055948254561335
train: 0.969839	val: 0.818220	test: 0.752286

Epoch: 84
Loss: 0.0785548921003336
train: 0.971864	val: 0.799631	test: 0.756728

Epoch: 85
Loss: 0.07941854743537302
train: 0.973313	val: 0.814006	test: 0.758390

Epoch: 86
Loss: 0.07931563595043417
train: 0.975219	val: 0.798421	test: 0.769716

Epoch: 87
Loss: 0.07685101884029592
train: 0.972878	val: 0.806557	test: 0.755316

Epoch: 88
Loss: 0.07948425849776157
train: 0.973772	val: 0.793939	test: 0.754706

Epoch: 89
Loss: 0.07755184825233696
train: 0.976032	val: 0.804677	test: 0.755443

Epoch: 90
Loss: 0.07780375479633216
train: 0.975354	val: 0.809221	test: 0.762776

Epoch: 91
Loss: 0.07584318395562957
train: 0.978175	val: 0.798081	test: 0.750186

Epoch: 92
Loss: 0.07719164421133762
train: 0.976021	val: 0.809273	test: 0.764041

Epoch: 93
Loss: 0.0761607602281348
train: 0.975159	val: 0.798152	test: 0.770196

Epoch: 94
Loss: 0.07458448474633188
train: 0.979444	val: 0.805280	test: 0.758091

Epoch: 95
Loss: 0.0758511876675235
train: 0.977423	val: 0.786379	test: 0.751023

Epoch: 96
Loss: 0.07520818637191497
train: 0.978987	val: 0.803183	test: 0.751007

Epoch: 97
Loss: 0.07509229222814781
train: 0.976109	val: 0.784352	test: 0.745918

Epoch: 98
Loss: 0.07503308370646244
train: 0.980565	val: 0.798293	test: 0.765793

Epoch: 99
Loss: 0.07275273620685373
train: 0.979378	val: 0.789710	test: 0.759937

Epoch: 100
Loss: 0.07551019951560535
train: 0.980095	val: 0.793296	test: 0.737034

best train: 0.968400	val: 0.831420	test: 0.767460
end
