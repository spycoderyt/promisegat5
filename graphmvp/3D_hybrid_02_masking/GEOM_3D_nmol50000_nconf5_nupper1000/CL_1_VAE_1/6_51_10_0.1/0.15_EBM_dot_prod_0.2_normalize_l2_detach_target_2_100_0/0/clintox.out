9090750_0
--dataset=clintox --runseed=0 --eval_train --batch_size=256 --dropout_ratio=0.5 --input_model_file=../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth
start
arguments	 Namespace(AE_loss='l2', AE_model='AE', CL_neg_samples=1, CL_similarity_metric='InfoNCE_dot_prod', JK='last', SSL_2D_mode='AM', SSL_masking_ratio=0, T=0.1, alpha_1=1, alpha_2=1, alpha_3=0.1, batch_size=256, beta=1, contextpred_neg_samples=1, csize=3, cutoff=10, dataset='clintox', decay=0, detach_target=True, device=0, dropout_ratio=0.5, emb_dim=300, engg_n_layers=4, epochs=100, eval_train=True, flow_length=8, flow_model='planar', gamma_joao=0.1, gamma_joaov2=0.1, gnn_lr_scale=1, gnn_type='gin', graph_pooling='mean', input_data_dir='', input_model_file='../output/3D_hybrid_02_masking/GEOM_3D_nmol50000_nconf5_nupper1000/CL_1_VAE_1/6_51_10_0.1/0.15_EBM_dot_prod_0.2_normalize_l2_detach_target_2_100_0/pretraining_model.pth', iw_samples=5, lr=0.001, lr_scale=1, m_dim=50, mask_edge=0, mask_rate=0.15, model_3d='schnet', normalize=True, num_filters=128, num_gaussians=51, num_interactions=6, num_layer=5, num_workers=8, output_model_dir='', readout='mean', runseed=0, schnet_lr_scale=1, seed=42, split='scaffold', verbose=False)
Dataset: clintox
Data: Data(edge_attr=[82372, 2], edge_index=[2, 82372], id=[1477], x=[38637, 2], y=[2954])
MoleculeDataset(1477)
split via scaffold
Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[2])
GNN_graphpred(
(molecule_model): GNN(
(x_embedding1): Embedding(120, 300)
(x_embedding2): Embedding(3, 300)
(gnns): ModuleList(
(0): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(1): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(2): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(3): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
(4): GINConv(
(mlp): Sequential(
(0): Linear(in_features=300, out_features=600, bias=True)
(1): ReLU()
(2): Linear(in_features=600, out_features=300, bias=True)
)
(edge_embedding1): Embedding(6, 300)
(edge_embedding2): Embedding(3, 300)
)
)
(batch_norms): ModuleList(
(0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
)
(graph_pred_linear): Linear(in_features=300, out_features=2, bias=True)
)
Epoch: 1
Loss: 0.6194722851389888
train: 0.621644	val: 0.742730	test: 0.537082

Epoch: 2
Loss: 0.5450641865472827
train: 0.702383	val: 0.800822	test: 0.517079

Epoch: 3
Loss: 0.4916373235640778
train: 0.747094	val: 0.864405	test: 0.529850

Epoch: 4
Loss: 0.442340566481477
train: 0.785357	val: 0.876680	test: 0.567820

Epoch: 5
Loss: 0.40570816180551594
train: 0.787059	val: 0.828456	test: 0.530398

Epoch: 6
Loss: 0.3727960537943219
train: 0.803129	val: 0.818367	test: 0.543877

Epoch: 7
Loss: 0.3385206894955368
train: 0.830080	val: 0.873971	test: 0.605451

Epoch: 8
Loss: 0.3137631610164582
train: 0.840045	val: 0.873159	test: 0.593616

Epoch: 9
Loss: 0.29248906341167996
train: 0.847514	val: 0.859299	test: 0.604384

Epoch: 10
Loss: 0.2751438276532647
train: 0.863763	val: 0.852531	test: 0.626471

Epoch: 11
Loss: 0.2568195140199906
train: 0.873896	val: 0.864630	test: 0.632967

Epoch: 12
Loss: 0.25186170202922326
train: 0.880228	val: 0.837885	test: 0.641442

Epoch: 13
Loss: 0.23445996678220898
train: 0.889739	val: 0.857001	test: 0.654833

Epoch: 14
Loss: 0.22299906689574617
train: 0.890773	val: 0.862958	test: 0.652041

Epoch: 15
Loss: 0.2141243127380033
train: 0.904503	val: 0.883174	test: 0.670166

Epoch: 16
Loss: 0.20691847508706687
train: 0.901903	val: 0.867941	test: 0.676756

Epoch: 17
Loss: 0.19852552996993905
train: 0.917926	val: 0.895948	test: 0.678830

Epoch: 18
Loss: 0.1999081145495197
train: 0.923073	val: 0.890979	test: 0.688867

Epoch: 19
Loss: 0.19269831578045685
train: 0.918935	val: 0.825873	test: 0.681951

Epoch: 20
Loss: 0.18977496540803993
train: 0.927549	val: 0.854218	test: 0.694513

Epoch: 21
Loss: 0.18328948100712886
train: 0.925247	val: 0.823712	test: 0.694563

Epoch: 22
Loss: 0.18975007286342988
train: 0.926000	val: 0.830656	test: 0.697815

Epoch: 23
Loss: 0.17069405968690443
train: 0.934697	val: 0.880728	test: 0.722606

Epoch: 24
Loss: 0.18386014078051777
train: 0.935359	val: 0.882714	test: 0.732811

Epoch: 25
Loss: 0.1748814844682528
train: 0.935738	val: 0.863495	test: 0.704042

Epoch: 26
Loss: 0.1611169573789694
train: 0.946155	val: 0.878542	test: 0.697628

Epoch: 27
Loss: 0.1660461792198332
train: 0.950024	val: 0.889419	test: 0.715836

Epoch: 28
Loss: 0.17229559783507503
train: 0.943945	val: 0.839097	test: 0.700570

Epoch: 29
Loss: 0.17884102239018312
train: 0.952883	val: 0.849885	test: 0.720262

Epoch: 30
Loss: 0.17521298925993173
train: 0.949510	val: 0.881052	test: 0.738614

Epoch: 31
Loss: 0.16494556197607615
train: 0.957284	val: 0.885423	test: 0.729089

Epoch: 32
Loss: 0.1701126497777898
train: 0.950698	val: 0.884411	test: 0.738007

Epoch: 33
Loss: 0.16985403200395938
train: 0.952387	val: 0.879815	test: 0.770530

Epoch: 34
Loss: 0.15226444292642652
train: 0.961832	val: 0.849523	test: 0.752573

Epoch: 35
Loss: 0.15219276120680675
train: 0.960503	val: 0.859475	test: 0.726784

Epoch: 36
Loss: 0.15261938088685187
train: 0.958969	val: 0.861510	test: 0.712487

Epoch: 37
Loss: 0.15704245198401978
train: 0.963819	val: 0.911969	test: 0.764834

Epoch: 38
Loss: 0.1466930292360397
train: 0.957917	val: 0.884573	test: 0.778643

Epoch: 39
Loss: 0.1424075094199985
train: 0.966159	val: 0.872048	test: 0.788006

Epoch: 40
Loss: 0.1544887886104683
train: 0.967799	val: 0.885609	test: 0.768875

Epoch: 41
Loss: 0.1310883466793808
train: 0.966489	val: 0.896960	test: 0.752522

Epoch: 42
Loss: 0.15202039798207004
train: 0.967152	val: 0.878680	test: 0.744975

Epoch: 43
Loss: 0.15281715479428615
train: 0.964608	val: 0.853255	test: 0.788654

Epoch: 44
Loss: 0.14575579551001747
train: 0.972547	val: 0.856565	test: 0.775244

Epoch: 45
Loss: 0.14106220851007825
train: 0.973222	val: 0.864170	test: 0.762354

Epoch: 46
Loss: 0.14768034775562863
train: 0.969731	val: 0.865906	test: 0.744066

Epoch: 47
Loss: 0.13768320321702845
train: 0.970266	val: 0.832266	test: 0.726865

Epoch: 48
Loss: 0.130891671459531
train: 0.974459	val: 0.835200	test: 0.779592

Epoch: 49
Loss: 0.13615790936178046
train: 0.968443	val: 0.850634	test: 0.784250

Epoch: 50
Loss: 0.1305379032459036
train: 0.970093	val: 0.834950	test: 0.757068

Epoch: 51
Loss: 0.14123976965227764
train: 0.972492	val: 0.887033	test: 0.789567

Epoch: 52
Loss: 0.14788403209845669
train: 0.968148	val: 0.875819	test: 0.764328

Epoch: 53
Loss: 0.14241619285885398
train: 0.974943	val: 0.810650	test: 0.759268

Epoch: 54
Loss: 0.13316331944121812
train: 0.971068	val: 0.757981	test: 0.737256

Epoch: 55
Loss: 0.1394853241022358
train: 0.967794	val: 0.775562	test: 0.726189

Epoch: 56
Loss: 0.14875567396556982
train: 0.976053	val: 0.767933	test: 0.735146

Epoch: 57
Loss: 0.13028068955399036
train: 0.975410	val: 0.853455	test: 0.799049

Epoch: 58
Loss: 0.13173794006113962
train: 0.974025	val: 0.843454	test: 0.791672

Epoch: 59
Loss: 0.12465110383146613
train: 0.975984	val: 0.862385	test: 0.795349

Epoch: 60
Loss: 0.13103289454061323
train: 0.973411	val: 0.862184	test: 0.797929

Epoch: 61
Loss: 0.1336838411256543
train: 0.978218	val: 0.843679	test: 0.794300

Epoch: 62
Loss: 0.14248228809286143
train: 0.979465	val: 0.873422	test: 0.798915

Epoch: 63
Loss: 0.12832370789529002
train: 0.978023	val: 0.858713	test: 0.774874

Epoch: 64
Loss: 0.14524617182135646
train: 0.979961	val: 0.850159	test: 0.784943

Epoch: 65
Loss: 0.12261510259820389
train: 0.980415	val: 0.819766	test: 0.773694

Epoch: 66
Loss: 0.13288736704064855
train: 0.980823	val: 0.816856	test: 0.790208

Epoch: 67
Loss: 0.1232197870258952
train: 0.980431	val: 0.829106	test: 0.802799

Epoch: 68
Loss: 0.11719451366683031
train: 0.980148	val: 0.787225	test: 0.761416

Epoch: 69
Loss: 0.1231369754790859
train: 0.978182	val: 0.794380	test: 0.732972

Epoch: 70
Loss: 0.12438413700373463
train: 0.980780	val: 0.862908	test: 0.798515

Epoch: 71
Loss: 0.11519297728956499
train: 0.977679	val: 0.871437	test: 0.816553

Epoch: 72
Loss: 0.12517432202988218
train: 0.981247	val: 0.835512	test: 0.814054

Epoch: 73
Loss: 0.12582303617306362
train: 0.982265	val: 0.824187	test: 0.794481

Epoch: 74
Loss: 0.12204390162641869
train: 0.979348	val: 0.817306	test: 0.809004

Epoch: 75
Loss: 0.12016759726256165
train: 0.983230	val: 0.826622	test: 0.815918

Epoch: 76
Loss: 0.12237297793201389
train: 0.981357	val: 0.835400	test: 0.833974

Epoch: 77
Loss: 0.11545183534859742
train: 0.980727	val: 0.855641	test: 0.859150

Epoch: 78
Loss: 0.1268295302075479
train: 0.976972	val: 0.852682	test: 0.845203

Epoch: 79
Loss: 0.11812013152922266
train: 0.981424	val: 0.855617	test: 0.840574

Epoch: 80
Loss: 0.10884535488362909
train: 0.982818	val: 0.855553	test: 0.824317

Epoch: 81
Loss: 0.11218538185978873
train: 0.983242	val: 0.851783	test: 0.821127

Epoch: 82
Loss: 0.11659572856660586
train: 0.982533	val: 0.828407	test: 0.797006

Epoch: 83
Loss: 0.11643616103522017
train: 0.982446	val: 0.819179	test: 0.803289

Epoch: 84
Loss: 0.11182054205357277
train: 0.982886	val: 0.791871	test: 0.767318

Epoch: 85
Loss: 0.11414412050016483
train: 0.980555	val: 0.822001	test: 0.776312

Epoch: 86
Loss: 0.10481915303796355
train: 0.980743	val: 0.815233	test: 0.777461

Epoch: 87
Loss: 0.11043776921839363
train: 0.983168	val: 0.836936	test: 0.792807

Epoch: 88
Loss: 0.1152407795842155
train: 0.983205	val: 0.874009	test: 0.838139

Epoch: 89
Loss: 0.10712954363635027
train: 0.984539	val: 0.865481	test: 0.841667

Epoch: 90
Loss: 0.10666519522123905
train: 0.985004	val: 0.826646	test: 0.829387

Epoch: 91
Loss: 0.11974152745166851
train: 0.984317	val: 0.809789	test: 0.802174

Epoch: 92
Loss: 0.10478453104596117
train: 0.984322	val: 0.858013	test: 0.813381

Epoch: 93
Loss: 0.0935436948664362
train: 0.983830	val: 0.844702	test: 0.812526

Epoch: 94
Loss: 0.11825422278830171
train: 0.983737	val: 0.800224	test: 0.813417

Epoch: 95
Loss: 0.10806921864611022
train: 0.981804	val: 0.826084	test: 0.842384

Epoch: 96
Loss: 0.09846426024864385
train: 0.983673	val: 0.828632	test: 0.852983

Epoch: 97
Loss: 0.09686207198655096
train: 0.983284	val: 0.791446	test: 0.834609

Epoch: 98
Loss: 0.11292734951140038
train: 0.985563	val: 0.792258	test: 0.803918

Epoch: 99
Loss: 0.10006624183941755
train: 0.985010	val: 0.776124	test: 0.807883

Epoch: 100
Loss: 0.11466580445588115
train: 0.985888	val: 0.761727	test: 0.818488

best train: 0.963819	val: 0.911969	test: 0.764834
end
